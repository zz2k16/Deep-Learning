{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepDream\n",
    "## Inception V3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15043440082116597145\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15589952717\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1723042628417857993\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained Inception V3 model from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "# disable model training\n",
    "K.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Inception V3 network without its convolutional base. \n",
    "# Load model with pretrained ImageNet weights\n",
    "model = inception_v3.InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the DeepDream configuration\n",
    "\n",
    "Compute the loss to maximize during the *gradient-ascent* process\n",
    "\n",
    "in parallel, maximize the activation of all filters in a number of layers and the weighted sum of L2 norm of activations of a set of high-level layers\n",
    "\n",
    "Lower levels = Geometric patterns\n",
    "Higher levels = visuals containing class features from ImageNet (dog, cat etc)\n",
    "\n",
    "#### Initial config: 4 Layers\n",
    "Create a dictionary mapping of layer names and a coefficient quantifying the degree of the layer's activation contribution to the loss we seek to minimize. *Layer names are hard-coded in Inception, use model.summary() for all layer names to create varying configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_contributions = {\n",
    "    'mixed1': 3.0, 'mixed2': 3., 'mixed3': 2.0, 'mixed4': 3., 'mixed5': 1.5, 'mixed6': 3., 'mixed7': 2.0, \n",
    "    'mixed8': 2.2, 'mixed9': 2.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a tensor containing the loss: the weighted sum of the L2 norm of the activations of the layers\n",
    "\n",
    "##### Define the loss to be maximized\n",
    "\n",
    "Create a dictionary mapping of layer names to layer instances\n",
    "\n",
    "Define loss by adding layer contributions to the scalar variable\n",
    "\n",
    "Retrieve layer's output to activation variable\n",
    "\n",
    "Add the L2 norm of the features of a layer to the loss. Border artifacts are avoided by only involving nonporder pixels in the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict(\n",
    "    [layer.name, layer] for layer in model.layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = K.variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output\n",
    "    \n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Gradient-ascent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to hold the generated image\n",
    "dream = model.input\n",
    "\n",
    "# compute gradients of the dream with regard to the loss\n",
    "grads = K.gradients(loss, dream)[0]\n",
    "\n",
    "# normalise gradients\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "# setup a keras function to retrieve the value of the loss and gradients, given an input image\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grads_value = outs[1]\n",
    "    return loss_value, grads_value\n",
    "\n",
    "# gradient ascent function to run for a number of iterations\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_value = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('... Loss value at {}, step:{}'.format(loss_value, i))\n",
    "        x += step * grad_value\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepDream Algorithm\n",
    "\n",
    "1 - Define a list of *scales* (octaves) to process the images\n",
    "\n",
    "2 - Each successive scale is larger than the previous one by a factor of 1.4 (40%),\n",
    "    Start by processing a small image and increasingly scale up\n",
    "    \n",
    "3 - For each successive scale, from smallest to largets, run a gradient descent to maximize the loss previously defined     at that scale.\n",
    "\n",
    "4 - After each gradient ascent run, upscale the resulting image by 40%\n",
    "\n",
    "### Detail injection\n",
    "\n",
    "To avoid losing detail at each scale up, at each scale up, reinject the loss details back into the image using the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run gradient ascent over different successive scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# hyperparameters for varying effects\n",
    "step = 0.01 # gradient ascent step\n",
    "num_octave = 3  # number of scales to run gradient ascent\n",
    "octave_scale = 1.4  # size ratio between scales\n",
    "iterations = 100  # number of iteration steps to run at each scale\n",
    "\n",
    "# max loss to interrup the gradient ascent process to avoid image artifacts\n",
    "max_loss = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures\n",
    "    # into appropriate tensors.\n",
    "    img = image.load_img(image_path)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image.\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess image\n",
    "\n",
    "Loads base image into a numpy array\n",
    "\n",
    "Prepares a list of shape tuples defining the different scale to run gradient ascent\n",
    "\n",
    "Reverse the list of shapes so theyre in increasing order\n",
    "\n",
    "Resize numpy array of the image to the smallest scale\n",
    "\n",
    "Scale up smaller version of the orinigal image\n",
    "\n",
    "Compute high quality version of original image\n",
    "\n",
    "Compute difference in detail lost between images when scaling up\n",
    "\n",
    "Reinject lost detail into the dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base image path for sample input\n",
    "base_image_path = 'images/florence_1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/florence_1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(base_image_path[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_image(base_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.11372554,  0.24705887,  0.5764706 ],\n",
       "         [ 0.10588241,  0.23921573,  0.5686275 ],\n",
       "         [ 0.09019613,  0.22352946,  0.5529412 ],\n",
       "         ...,\n",
       "         [ 0.20000005,  0.05882359, -0.06666666],\n",
       "         [ 0.14509809,  0.00392163, -0.10588235],\n",
       "         [ 0.16078436,  0.03529418, -0.06666666]],\n",
       "\n",
       "        [[ 0.12156868,  0.254902  ,  0.58431375],\n",
       "         [ 0.13725495,  0.27058828,  0.6       ],\n",
       "         [ 0.11372554,  0.24705887,  0.5764706 ],\n",
       "         ...,\n",
       "         [ 0.18431377,  0.04313731, -0.08235294],\n",
       "         [ 0.19215691,  0.05882359, -0.05098039],\n",
       "         [ 0.14509809,  0.0196079 , -0.08235294]],\n",
       "\n",
       "        [[ 0.12156868,  0.254902  ,  0.58431375],\n",
       "         [ 0.11372554,  0.24705887,  0.5764706 ],\n",
       "         [ 0.11372554,  0.24705887,  0.5764706 ],\n",
       "         ...,\n",
       "         [ 0.20784318,  0.06666672, -0.05098039],\n",
       "         [ 0.20000005,  0.05882359, -0.04313725],\n",
       "         [ 0.1686275 ,  0.04313731, -0.06666666]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.8509804 , -0.8352941 , -0.7490196 ],\n",
       "         [-0.8039216 , -0.78039217, -0.7019608 ],\n",
       "         [-0.78039217, -0.75686276, -0.69411767],\n",
       "         ...,\n",
       "         [-0.17647058, -0.23921567, -0.27058822],\n",
       "         [-0.17647058, -0.23921567, -0.27058822],\n",
       "         [-0.14509803, -0.20784312, -0.23921567]],\n",
       "\n",
       "        [[-0.85882354, -0.81960785, -0.75686276],\n",
       "         [-0.81960785, -0.78039217, -0.7176471 ],\n",
       "         [-0.7882353 , -0.7490196 , -0.69411767],\n",
       "         ...,\n",
       "         [-0.17647058, -0.23921567, -0.27058822],\n",
       "         [-0.1607843 , -0.2235294 , -0.25490195],\n",
       "         [-0.1372549 , -0.19999999, -0.23137254]],\n",
       "\n",
       "        [[-0.85882354, -0.81960785, -0.75686276],\n",
       "         [-0.81960785, -0.78039217, -0.7254902 ],\n",
       "         [-0.79607844, -0.7490196 , -0.69411767],\n",
       "         ...,\n",
       "         [-0.19999999, -0.26274508, -0.29411763],\n",
       "         [-0.16862744, -0.23137254, -0.26274508],\n",
       "         [-0.16862744, -0.23137254, -0.26274508]]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = img.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list of shape tuples defining the different scales to run gradient ascent\n",
    "successive_shapes = [original_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, num_octave):\n",
    "    shape = tuple(\n",
    "        [int(dim / (octave_scale ** i)) for dim in original_shape]\n",
    "    )\n",
    "    successive_shapes.append(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1672, 2508), (1194, 1791), (853, 1279)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successive_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the list of shapes to increasing order\n",
    "successive_shapes = successive_shapes[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(853, 1279), (1194, 1791), (1672, 2508)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successive_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = np.copy(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "shrunk_original_img = resize_img(img, successive_shapes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image shape  (853, 1279)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Loss value at 6.50858211517334, step:0\n",
      "... Loss value at 7.582132339477539, step:1\n",
      "... Loss value at 8.713152885437012, step:2\n",
      "... Loss value at 9.84902572631836, step:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:15: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image shape  (1194, 1791)\n",
      "... Loss value at 7.045809268951416, step:0\n",
      "... Loss value at 8.602713584899902, step:1\n",
      "... Loss value at 9.95887565612793, step:2\n",
      "Processing image shape  (1672, 2508)\n",
      "... Loss value at 7.142994403839111, step:0\n",
      "... Loss value at 8.683769226074219, step:1\n",
      "---- Dreams Complete\n"
     ]
    }
   ],
   "source": [
    "# scale up dream image\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape ', shape)\n",
    "    # resize image and run gradient ascent\n",
    "    img = resize_img(img, shape)\n",
    "    img = gradient_ascent(img, iterations=iterations, step=step, max_loss=max_loss)\n",
    "    # scale up smaller version of the original image\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
    "    # compute high quality version of original version of original size\n",
    "    same_size_original = resize_img(original_img, shape)\n",
    "    # compute lost detail\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "    \n",
    "    # reinject lost detail into dream\n",
    "    img += lost_detail\n",
    "    \n",
    "    # resize original\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    # save each scale to file\n",
    "    save_img(img, fname=str(base_image_path[:-4]) + 'dream_at_scale_' + str(shape) + '.png')\n",
    "\n",
    "\n",
    "# save final image to file\n",
    "save_img(img, fname=str(base_image_path[:-4]) + 'final_dream.png')\n",
    "print('---- Dreams Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
