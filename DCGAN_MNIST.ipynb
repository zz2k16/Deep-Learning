{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN adapted from github:roatienza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer class\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCGAN Class\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator, without opt and loss\n",
    "        self.G = None   # generator, without opt and loss\n",
    "        self.AM = None  # adversarial model with opt and loss + DM\n",
    "        self.DM = None  # discriminator model with opt and loss\n",
    "\n",
    "    # Discriminator\n",
    "    # Takes in MNIST Images of 28x28x1 -> downsamples to 14x14x64 -> 7x7x128 -> 4x4x256 -> 4x4x512\n",
    "    # applies stride = 2 instead of MaxPooling and adds dropout of 0.4 for stabilised learning\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Outout through sigmoid activation to squash pixel values to 0,1\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.4\n",
    "        depth = 256\n",
    "        dim = 7\n",
    "        # Takes in vector of random noise of size 100\n",
    "        # applies Convolution Transpose and upscales repeatedly to match Discriminator convolutions but in reverse\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        # inputs: dim x dim x depth 7x7x256\n",
    "        # outputs: 2*dim x 2*dim x depth/2 for 14x14x128\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        # upsample repeat\n",
    "        # inputs: dim x dim x depth 14x14x128\n",
    "        # outputs: 2*dim x 2*dim x depth/2 for 28x28x64\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        # upsample repeat\n",
    "        # inputs: dim x dim x depth 28x28x64\n",
    "        # outputs: 2*dim x 2*dim x depth/2 for 28x28x32\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # final convolution layer outputs a 28 x 28 x 1 grayscale image [0.0,1.0] per pixel value\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "    \n",
    "    # D, Discriminator Model - app RMSprop optimizer and loss functions\n",
    "    # Trains on MNIST Data\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.DM\n",
    "    \n",
    "    # AM, Adversarial Model - Stack G and D and apply optimizer and loss function\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Stacked GAN and training and noise samples for D, G\n",
    "# Define plotting function and save at specific training intervals\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "        # load in MNIST images and reshape into 1D vectors\n",
    "        self.x_train = input_data.read_data_sets(\"mnist\", one_hot=True).train.images\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows, self.img_cols, 1).astype(np.float32)\n",
    "        # initialise DCGAN with D, G and AM models\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "    \n",
    "    # train D and G on real and noise data\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0], noise=noise_input, step=(i+1))\n",
    "            \n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialise DCGAN and Timer\n",
    "mnist_dcgan = MNIST_DCGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.703390, acc: 0.552500]  [A loss: 0.781994, acc: 0.370000]\n",
      "1: [D loss: 0.716392, acc: 0.497500]  [A loss: 0.775761, acc: 0.390000]\n",
      "2: [D loss: 0.695051, acc: 0.542500]  [A loss: 0.814009, acc: 0.340000]\n",
      "3: [D loss: 0.714973, acc: 0.517500]  [A loss: 0.791081, acc: 0.355000]\n",
      "4: [D loss: 0.712546, acc: 0.542500]  [A loss: 0.824238, acc: 0.310000]\n",
      "5: [D loss: 0.710426, acc: 0.537500]  [A loss: 0.820325, acc: 0.345000]\n",
      "6: [D loss: 0.720372, acc: 0.522500]  [A loss: 0.769186, acc: 0.360000]\n",
      "7: [D loss: 0.727890, acc: 0.490000]  [A loss: 0.867830, acc: 0.285000]\n",
      "8: [D loss: 0.687465, acc: 0.512500]  [A loss: 0.799581, acc: 0.385000]\n",
      "9: [D loss: 0.706860, acc: 0.522500]  [A loss: 0.826467, acc: 0.350000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise Noise \n",
    "mnist_dcgan.train(train_steps=10, batch_size=200, save_interval=1)\n",
    "mnist_dcgan.plot_images(fake=True, save2file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.676510, acc: 0.576172]  [A loss: 0.927176, acc: 0.070312]\n",
      "1: [D loss: 0.682454, acc: 0.560547]  [A loss: 0.847062, acc: 0.171875]\n",
      "2: [D loss: 0.694049, acc: 0.494141]  [A loss: 0.944294, acc: 0.070312]\n",
      "3: [D loss: 0.685476, acc: 0.535156]  [A loss: 0.892463, acc: 0.109375]\n",
      "4: [D loss: 0.679122, acc: 0.585938]  [A loss: 0.887934, acc: 0.101562]\n",
      "5: [D loss: 0.687118, acc: 0.552734]  [A loss: 0.915107, acc: 0.136719]\n",
      "6: [D loss: 0.701477, acc: 0.550781]  [A loss: 0.958487, acc: 0.042969]\n",
      "7: [D loss: 0.691788, acc: 0.525391]  [A loss: 0.945148, acc: 0.050781]\n",
      "8: [D loss: 0.679253, acc: 0.585938]  [A loss: 0.910327, acc: 0.109375]\n",
      "9: [D loss: 0.685358, acc: 0.558594]  [A loss: 1.043645, acc: 0.023438]\n",
      "10: [D loss: 0.675296, acc: 0.566406]  [A loss: 0.844653, acc: 0.187500]\n",
      "11: [D loss: 0.679972, acc: 0.550781]  [A loss: 1.119424, acc: 0.011719]\n",
      "12: [D loss: 0.693679, acc: 0.509766]  [A loss: 0.772725, acc: 0.269531]\n",
      "13: [D loss: 0.692237, acc: 0.535156]  [A loss: 1.178445, acc: 0.000000]\n",
      "14: [D loss: 0.677945, acc: 0.552734]  [A loss: 0.651820, acc: 0.628906]\n",
      "15: [D loss: 0.717001, acc: 0.509766]  [A loss: 1.328683, acc: 0.000000]\n",
      "16: [D loss: 0.696991, acc: 0.531250]  [A loss: 0.568274, acc: 0.863281]\n",
      "17: [D loss: 0.731333, acc: 0.503906]  [A loss: 1.041537, acc: 0.015625]\n",
      "18: [D loss: 0.675553, acc: 0.568359]  [A loss: 0.726760, acc: 0.394531]\n",
      "19: [D loss: 0.694315, acc: 0.535156]  [A loss: 0.907645, acc: 0.074219]\n",
      "20: [D loss: 0.671299, acc: 0.572266]  [A loss: 0.786770, acc: 0.261719]\n",
      "21: [D loss: 0.688506, acc: 0.566406]  [A loss: 0.945048, acc: 0.023438]\n",
      "22: [D loss: 0.677999, acc: 0.574219]  [A loss: 0.804142, acc: 0.156250]\n",
      "23: [D loss: 0.685219, acc: 0.558594]  [A loss: 0.967856, acc: 0.027344]\n",
      "24: [D loss: 0.681346, acc: 0.578125]  [A loss: 0.823270, acc: 0.187500]\n",
      "25: [D loss: 0.683989, acc: 0.572266]  [A loss: 0.946482, acc: 0.050781]\n",
      "26: [D loss: 0.662736, acc: 0.615234]  [A loss: 0.827993, acc: 0.164062]\n",
      "27: [D loss: 0.681952, acc: 0.548828]  [A loss: 1.067588, acc: 0.015625]\n",
      "28: [D loss: 0.665316, acc: 0.587891]  [A loss: 0.748333, acc: 0.363281]\n",
      "29: [D loss: 0.686026, acc: 0.535156]  [A loss: 1.222004, acc: 0.003906]\n",
      "30: [D loss: 0.683443, acc: 0.552734]  [A loss: 0.654277, acc: 0.621094]\n",
      "31: [D loss: 0.725321, acc: 0.503906]  [A loss: 1.149008, acc: 0.019531]\n",
      "32: [D loss: 0.680444, acc: 0.541016]  [A loss: 0.647167, acc: 0.660156]\n",
      "33: [D loss: 0.716966, acc: 0.505859]  [A loss: 1.082509, acc: 0.003906]\n",
      "34: [D loss: 0.671565, acc: 0.583984]  [A loss: 0.705040, acc: 0.441406]\n",
      "35: [D loss: 0.702055, acc: 0.507812]  [A loss: 0.995425, acc: 0.027344]\n",
      "36: [D loss: 0.676349, acc: 0.576172]  [A loss: 0.770786, acc: 0.289062]\n",
      "37: [D loss: 0.693201, acc: 0.519531]  [A loss: 0.923833, acc: 0.050781]\n",
      "38: [D loss: 0.671454, acc: 0.568359]  [A loss: 0.783190, acc: 0.296875]\n",
      "39: [D loss: 0.699562, acc: 0.521484]  [A loss: 1.021320, acc: 0.015625]\n",
      "40: [D loss: 0.678877, acc: 0.582031]  [A loss: 0.727514, acc: 0.425781]\n",
      "41: [D loss: 0.695585, acc: 0.533203]  [A loss: 1.013674, acc: 0.019531]\n",
      "42: [D loss: 0.674217, acc: 0.552734]  [A loss: 0.695960, acc: 0.519531]\n",
      "43: [D loss: 0.699518, acc: 0.527344]  [A loss: 1.043515, acc: 0.007812]\n",
      "44: [D loss: 0.671187, acc: 0.587891]  [A loss: 0.711070, acc: 0.503906]\n",
      "45: [D loss: 0.711365, acc: 0.511719]  [A loss: 1.070947, acc: 0.007812]\n",
      "46: [D loss: 0.670234, acc: 0.583984]  [A loss: 0.688704, acc: 0.519531]\n",
      "47: [D loss: 0.700451, acc: 0.519531]  [A loss: 1.132032, acc: 0.003906]\n",
      "48: [D loss: 0.678697, acc: 0.585938]  [A loss: 0.669758, acc: 0.621094]\n",
      "49: [D loss: 0.706518, acc: 0.509766]  [A loss: 1.030589, acc: 0.003906]\n",
      "50: [D loss: 0.679931, acc: 0.548828]  [A loss: 0.708407, acc: 0.464844]\n",
      "51: [D loss: 0.698018, acc: 0.501953]  [A loss: 0.988893, acc: 0.015625]\n",
      "52: [D loss: 0.661207, acc: 0.607422]  [A loss: 0.793472, acc: 0.238281]\n",
      "53: [D loss: 0.693511, acc: 0.537109]  [A loss: 0.975230, acc: 0.015625]\n",
      "54: [D loss: 0.675324, acc: 0.587891]  [A loss: 0.783108, acc: 0.246094]\n",
      "55: [D loss: 0.678678, acc: 0.550781]  [A loss: 0.985249, acc: 0.046875]\n",
      "56: [D loss: 0.677669, acc: 0.576172]  [A loss: 0.801856, acc: 0.269531]\n",
      "57: [D loss: 0.693805, acc: 0.525391]  [A loss: 1.006982, acc: 0.027344]\n",
      "58: [D loss: 0.667194, acc: 0.611328]  [A loss: 0.776608, acc: 0.289062]\n",
      "59: [D loss: 0.689670, acc: 0.529297]  [A loss: 1.046472, acc: 0.007812]\n",
      "60: [D loss: 0.665697, acc: 0.619141]  [A loss: 0.742303, acc: 0.375000]\n",
      "61: [D loss: 0.696244, acc: 0.521484]  [A loss: 1.160643, acc: 0.000000]\n",
      "62: [D loss: 0.674566, acc: 0.570312]  [A loss: 0.686630, acc: 0.554688]\n",
      "63: [D loss: 0.732288, acc: 0.511719]  [A loss: 1.023984, acc: 0.039062]\n",
      "64: [D loss: 0.664669, acc: 0.607422]  [A loss: 0.733031, acc: 0.425781]\n",
      "65: [D loss: 0.694341, acc: 0.537109]  [A loss: 1.067921, acc: 0.003906]\n",
      "66: [D loss: 0.676913, acc: 0.572266]  [A loss: 0.684556, acc: 0.511719]\n",
      "67: [D loss: 0.698965, acc: 0.521484]  [A loss: 1.028625, acc: 0.015625]\n",
      "68: [D loss: 0.671981, acc: 0.589844]  [A loss: 0.690259, acc: 0.527344]\n",
      "69: [D loss: 0.697134, acc: 0.527344]  [A loss: 0.988599, acc: 0.058594]\n",
      "70: [D loss: 0.689747, acc: 0.537109]  [A loss: 0.790327, acc: 0.230469]\n",
      "71: [D loss: 0.678416, acc: 0.556641]  [A loss: 0.928751, acc: 0.062500]\n",
      "72: [D loss: 0.669030, acc: 0.607422]  [A loss: 0.798177, acc: 0.210938]\n",
      "73: [D loss: 0.691108, acc: 0.531250]  [A loss: 0.956001, acc: 0.035156]\n",
      "74: [D loss: 0.666944, acc: 0.615234]  [A loss: 0.772014, acc: 0.257812]\n",
      "75: [D loss: 0.689797, acc: 0.546875]  [A loss: 1.014684, acc: 0.019531]\n",
      "76: [D loss: 0.676934, acc: 0.558594]  [A loss: 0.752453, acc: 0.320312]\n",
      "77: [D loss: 0.695696, acc: 0.529297]  [A loss: 1.104654, acc: 0.011719]\n",
      "78: [D loss: 0.671821, acc: 0.605469]  [A loss: 0.665572, acc: 0.628906]\n",
      "79: [D loss: 0.710411, acc: 0.519531]  [A loss: 1.056381, acc: 0.015625]\n",
      "80: [D loss: 0.675455, acc: 0.585938]  [A loss: 0.726337, acc: 0.457031]\n",
      "81: [D loss: 0.693267, acc: 0.533203]  [A loss: 0.989893, acc: 0.023438]\n",
      "82: [D loss: 0.672197, acc: 0.599609]  [A loss: 0.791627, acc: 0.253906]\n",
      "83: [D loss: 0.684844, acc: 0.566406]  [A loss: 0.965120, acc: 0.039062]\n",
      "84: [D loss: 0.674282, acc: 0.603516]  [A loss: 0.808592, acc: 0.207031]\n",
      "85: [D loss: 0.679955, acc: 0.560547]  [A loss: 0.939947, acc: 0.039062]\n",
      "86: [D loss: 0.677528, acc: 0.572266]  [A loss: 0.777066, acc: 0.300781]\n",
      "87: [D loss: 0.692517, acc: 0.521484]  [A loss: 1.061059, acc: 0.011719]\n",
      "88: [D loss: 0.672410, acc: 0.585938]  [A loss: 0.717145, acc: 0.460938]\n",
      "89: [D loss: 0.695643, acc: 0.523438]  [A loss: 1.081658, acc: 0.027344]\n",
      "90: [D loss: 0.674679, acc: 0.564453]  [A loss: 0.734557, acc: 0.363281]\n",
      "91: [D loss: 0.693694, acc: 0.511719]  [A loss: 1.053340, acc: 0.003906]\n",
      "92: [D loss: 0.663992, acc: 0.603516]  [A loss: 0.693588, acc: 0.492188]\n",
      "93: [D loss: 0.706784, acc: 0.509766]  [A loss: 1.058760, acc: 0.000000]\n",
      "94: [D loss: 0.670517, acc: 0.595703]  [A loss: 0.713795, acc: 0.468750]\n",
      "95: [D loss: 0.693939, acc: 0.548828]  [A loss: 0.981631, acc: 0.019531]\n",
      "96: [D loss: 0.678099, acc: 0.566406]  [A loss: 0.769368, acc: 0.304688]\n",
      "97: [D loss: 0.683339, acc: 0.544922]  [A loss: 0.909087, acc: 0.089844]\n",
      "98: [D loss: 0.669461, acc: 0.601562]  [A loss: 0.829719, acc: 0.183594]\n",
      "99: [D loss: 0.677795, acc: 0.560547]  [A loss: 0.883082, acc: 0.105469]\n",
      "100: [D loss: 0.668429, acc: 0.580078]  [A loss: 0.883815, acc: 0.085938]\n",
      "101: [D loss: 0.676409, acc: 0.564453]  [A loss: 0.946300, acc: 0.035156]\n",
      "102: [D loss: 0.676118, acc: 0.570312]  [A loss: 0.775261, acc: 0.296875]\n",
      "103: [D loss: 0.680850, acc: 0.544922]  [A loss: 1.115843, acc: 0.015625]\n",
      "104: [D loss: 0.664359, acc: 0.625000]  [A loss: 0.686346, acc: 0.515625]\n",
      "105: [D loss: 0.708009, acc: 0.523438]  [A loss: 1.112566, acc: 0.000000]\n",
      "106: [D loss: 0.674455, acc: 0.558594]  [A loss: 0.693274, acc: 0.523438]\n",
      "107: [D loss: 0.702681, acc: 0.509766]  [A loss: 1.039471, acc: 0.031250]\n",
      "108: [D loss: 0.666280, acc: 0.619141]  [A loss: 0.698071, acc: 0.500000]\n",
      "109: [D loss: 0.699214, acc: 0.541016]  [A loss: 1.003879, acc: 0.015625]\n",
      "110: [D loss: 0.666175, acc: 0.609375]  [A loss: 0.750666, acc: 0.347656]\n",
      "111: [D loss: 0.688417, acc: 0.541016]  [A loss: 0.980345, acc: 0.046875]\n",
      "112: [D loss: 0.663041, acc: 0.607422]  [A loss: 0.747656, acc: 0.375000]\n",
      "113: [D loss: 0.696022, acc: 0.537109]  [A loss: 0.976923, acc: 0.019531]\n",
      "114: [D loss: 0.672255, acc: 0.589844]  [A loss: 0.779788, acc: 0.289062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115: [D loss: 0.683729, acc: 0.554688]  [A loss: 0.867077, acc: 0.148438]\n",
      "116: [D loss: 0.664465, acc: 0.601562]  [A loss: 0.835776, acc: 0.171875]\n",
      "117: [D loss: 0.670764, acc: 0.564453]  [A loss: 0.925818, acc: 0.074219]\n",
      "118: [D loss: 0.673695, acc: 0.585938]  [A loss: 0.853518, acc: 0.144531]\n",
      "119: [D loss: 0.687208, acc: 0.544922]  [A loss: 0.938443, acc: 0.074219]\n",
      "120: [D loss: 0.661379, acc: 0.601562]  [A loss: 0.791736, acc: 0.273438]\n",
      "121: [D loss: 0.699408, acc: 0.531250]  [A loss: 1.124660, acc: 0.011719]\n",
      "122: [D loss: 0.682032, acc: 0.572266]  [A loss: 0.647615, acc: 0.660156]\n",
      "123: [D loss: 0.723821, acc: 0.507812]  [A loss: 1.097392, acc: 0.000000]\n",
      "124: [D loss: 0.689093, acc: 0.548828]  [A loss: 0.687876, acc: 0.500000]\n",
      "125: [D loss: 0.702351, acc: 0.531250]  [A loss: 1.012321, acc: 0.019531]\n",
      "126: [D loss: 0.672300, acc: 0.582031]  [A loss: 0.726868, acc: 0.402344]\n",
      "127: [D loss: 0.683525, acc: 0.537109]  [A loss: 0.971354, acc: 0.039062]\n",
      "128: [D loss: 0.669593, acc: 0.578125]  [A loss: 0.744214, acc: 0.328125]\n",
      "129: [D loss: 0.698482, acc: 0.529297]  [A loss: 0.990636, acc: 0.035156]\n",
      "130: [D loss: 0.674255, acc: 0.554688]  [A loss: 0.753950, acc: 0.382812]\n",
      "131: [D loss: 0.680145, acc: 0.562500]  [A loss: 0.955084, acc: 0.042969]\n",
      "132: [D loss: 0.667396, acc: 0.601562]  [A loss: 0.808308, acc: 0.210938]\n",
      "133: [D loss: 0.680122, acc: 0.562500]  [A loss: 0.964180, acc: 0.050781]\n",
      "134: [D loss: 0.666209, acc: 0.597656]  [A loss: 0.825918, acc: 0.179688]\n",
      "135: [D loss: 0.688286, acc: 0.531250]  [A loss: 0.960165, acc: 0.042969]\n",
      "136: [D loss: 0.668777, acc: 0.603516]  [A loss: 0.757073, acc: 0.347656]\n",
      "137: [D loss: 0.690890, acc: 0.537109]  [A loss: 1.004154, acc: 0.031250]\n",
      "138: [D loss: 0.667915, acc: 0.603516]  [A loss: 0.718697, acc: 0.449219]\n",
      "139: [D loss: 0.701865, acc: 0.519531]  [A loss: 1.037642, acc: 0.027344]\n",
      "140: [D loss: 0.671291, acc: 0.580078]  [A loss: 0.697881, acc: 0.515625]\n",
      "141: [D loss: 0.705703, acc: 0.523438]  [A loss: 1.074039, acc: 0.003906]\n",
      "142: [D loss: 0.683387, acc: 0.564453]  [A loss: 0.713351, acc: 0.445312]\n",
      "143: [D loss: 0.685456, acc: 0.566406]  [A loss: 0.957241, acc: 0.039062]\n",
      "144: [D loss: 0.671045, acc: 0.589844]  [A loss: 0.776203, acc: 0.312500]\n",
      "145: [D loss: 0.698663, acc: 0.539062]  [A loss: 0.957758, acc: 0.023438]\n",
      "146: [D loss: 0.669937, acc: 0.580078]  [A loss: 0.740299, acc: 0.375000]\n",
      "147: [D loss: 0.687529, acc: 0.535156]  [A loss: 0.974838, acc: 0.035156]\n",
      "148: [D loss: 0.668168, acc: 0.617188]  [A loss: 0.777955, acc: 0.292969]\n",
      "149: [D loss: 0.685493, acc: 0.554688]  [A loss: 0.903496, acc: 0.078125]\n",
      "150: [D loss: 0.670805, acc: 0.570312]  [A loss: 0.773203, acc: 0.316406]\n",
      "151: [D loss: 0.691691, acc: 0.570312]  [A loss: 0.972274, acc: 0.035156]\n",
      "152: [D loss: 0.675639, acc: 0.582031]  [A loss: 0.770667, acc: 0.281250]\n",
      "153: [D loss: 0.689021, acc: 0.556641]  [A loss: 0.960473, acc: 0.046875]\n",
      "154: [D loss: 0.665685, acc: 0.585938]  [A loss: 0.791596, acc: 0.261719]\n",
      "155: [D loss: 0.684390, acc: 0.533203]  [A loss: 0.964078, acc: 0.046875]\n",
      "156: [D loss: 0.676194, acc: 0.585938]  [A loss: 0.757932, acc: 0.324219]\n",
      "157: [D loss: 0.688178, acc: 0.542969]  [A loss: 1.047815, acc: 0.003906]\n",
      "158: [D loss: 0.679520, acc: 0.560547]  [A loss: 0.727968, acc: 0.433594]\n",
      "159: [D loss: 0.701179, acc: 0.517578]  [A loss: 1.029503, acc: 0.003906]\n",
      "160: [D loss: 0.664454, acc: 0.617188]  [A loss: 0.749576, acc: 0.332031]\n",
      "161: [D loss: 0.691017, acc: 0.531250]  [A loss: 0.969613, acc: 0.050781]\n",
      "162: [D loss: 0.670196, acc: 0.597656]  [A loss: 0.756254, acc: 0.332031]\n",
      "163: [D loss: 0.682832, acc: 0.527344]  [A loss: 0.954665, acc: 0.085938]\n",
      "164: [D loss: 0.677947, acc: 0.548828]  [A loss: 0.845393, acc: 0.144531]\n",
      "165: [D loss: 0.669739, acc: 0.583984]  [A loss: 0.921386, acc: 0.082031]\n",
      "166: [D loss: 0.663362, acc: 0.603516]  [A loss: 0.887702, acc: 0.128906]\n",
      "167: [D loss: 0.697410, acc: 0.554688]  [A loss: 0.940518, acc: 0.062500]\n",
      "168: [D loss: 0.678458, acc: 0.558594]  [A loss: 0.836029, acc: 0.179688]\n",
      "169: [D loss: 0.693981, acc: 0.541016]  [A loss: 0.988337, acc: 0.035156]\n",
      "170: [D loss: 0.672216, acc: 0.570312]  [A loss: 0.744307, acc: 0.371094]\n",
      "171: [D loss: 0.688899, acc: 0.558594]  [A loss: 1.038723, acc: 0.031250]\n",
      "172: [D loss: 0.668836, acc: 0.587891]  [A loss: 0.709732, acc: 0.503906]\n",
      "173: [D loss: 0.697593, acc: 0.537109]  [A loss: 1.104489, acc: 0.011719]\n",
      "174: [D loss: 0.669964, acc: 0.580078]  [A loss: 0.677403, acc: 0.593750]\n",
      "175: [D loss: 0.700863, acc: 0.501953]  [A loss: 1.015170, acc: 0.039062]\n",
      "176: [D loss: 0.668097, acc: 0.580078]  [A loss: 0.710398, acc: 0.507812]\n",
      "177: [D loss: 0.682658, acc: 0.533203]  [A loss: 1.014918, acc: 0.003906]\n",
      "178: [D loss: 0.679601, acc: 0.558594]  [A loss: 0.726662, acc: 0.425781]\n",
      "179: [D loss: 0.699938, acc: 0.525391]  [A loss: 1.005959, acc: 0.031250]\n",
      "180: [D loss: 0.673543, acc: 0.572266]  [A loss: 0.723473, acc: 0.417969]\n",
      "181: [D loss: 0.685772, acc: 0.523438]  [A loss: 0.961370, acc: 0.039062]\n",
      "182: [D loss: 0.677023, acc: 0.587891]  [A loss: 0.743395, acc: 0.386719]\n",
      "183: [D loss: 0.690797, acc: 0.533203]  [A loss: 0.933378, acc: 0.074219]\n",
      "184: [D loss: 0.665467, acc: 0.605469]  [A loss: 0.757339, acc: 0.296875]\n",
      "185: [D loss: 0.685281, acc: 0.564453]  [A loss: 0.956061, acc: 0.042969]\n",
      "186: [D loss: 0.664283, acc: 0.597656]  [A loss: 0.768184, acc: 0.343750]\n",
      "187: [D loss: 0.690661, acc: 0.544922]  [A loss: 0.985219, acc: 0.042969]\n",
      "188: [D loss: 0.660024, acc: 0.613281]  [A loss: 0.755545, acc: 0.304688]\n",
      "189: [D loss: 0.675920, acc: 0.550781]  [A loss: 0.969378, acc: 0.066406]\n",
      "190: [D loss: 0.683315, acc: 0.572266]  [A loss: 0.755904, acc: 0.367188]\n",
      "191: [D loss: 0.705444, acc: 0.503906]  [A loss: 0.964391, acc: 0.031250]\n",
      "192: [D loss: 0.681207, acc: 0.558594]  [A loss: 0.757108, acc: 0.332031]\n",
      "193: [D loss: 0.681264, acc: 0.552734]  [A loss: 0.910587, acc: 0.070312]\n",
      "194: [D loss: 0.656934, acc: 0.630859]  [A loss: 0.762506, acc: 0.367188]\n",
      "195: [D loss: 0.696587, acc: 0.537109]  [A loss: 1.127454, acc: 0.000000]\n",
      "196: [D loss: 0.669491, acc: 0.599609]  [A loss: 0.688418, acc: 0.496094]\n",
      "197: [D loss: 0.727195, acc: 0.519531]  [A loss: 0.999071, acc: 0.011719]\n",
      "198: [D loss: 0.675560, acc: 0.562500]  [A loss: 0.738013, acc: 0.339844]\n",
      "199: [D loss: 0.693481, acc: 0.517578]  [A loss: 0.865759, acc: 0.082031]\n",
      "200: [D loss: 0.668992, acc: 0.595703]  [A loss: 0.796417, acc: 0.210938]\n",
      "201: [D loss: 0.694497, acc: 0.533203]  [A loss: 0.958569, acc: 0.035156]\n",
      "202: [D loss: 0.665764, acc: 0.603516]  [A loss: 0.780828, acc: 0.246094]\n",
      "203: [D loss: 0.692283, acc: 0.513672]  [A loss: 0.997313, acc: 0.031250]\n",
      "204: [D loss: 0.659263, acc: 0.607422]  [A loss: 0.796656, acc: 0.226562]\n",
      "205: [D loss: 0.676786, acc: 0.564453]  [A loss: 0.929571, acc: 0.066406]\n",
      "206: [D loss: 0.669801, acc: 0.595703]  [A loss: 0.838747, acc: 0.167969]\n",
      "207: [D loss: 0.686908, acc: 0.550781]  [A loss: 0.949114, acc: 0.019531]\n",
      "208: [D loss: 0.680383, acc: 0.533203]  [A loss: 0.816157, acc: 0.242188]\n",
      "209: [D loss: 0.685233, acc: 0.544922]  [A loss: 0.941201, acc: 0.074219]\n",
      "210: [D loss: 0.667203, acc: 0.597656]  [A loss: 0.803993, acc: 0.234375]\n",
      "211: [D loss: 0.668693, acc: 0.609375]  [A loss: 0.978556, acc: 0.039062]\n",
      "212: [D loss: 0.678930, acc: 0.556641]  [A loss: 0.825048, acc: 0.167969]\n",
      "213: [D loss: 0.681989, acc: 0.554688]  [A loss: 1.018895, acc: 0.019531]\n",
      "214: [D loss: 0.661344, acc: 0.603516]  [A loss: 0.731071, acc: 0.417969]\n",
      "215: [D loss: 0.688109, acc: 0.525391]  [A loss: 1.116182, acc: 0.007812]\n",
      "216: [D loss: 0.669680, acc: 0.583984]  [A loss: 0.694117, acc: 0.511719]\n",
      "217: [D loss: 0.717647, acc: 0.509766]  [A loss: 1.113941, acc: 0.000000]\n",
      "218: [D loss: 0.680659, acc: 0.580078]  [A loss: 0.731271, acc: 0.402344]\n",
      "219: [D loss: 0.693915, acc: 0.542969]  [A loss: 0.937777, acc: 0.066406]\n",
      "220: [D loss: 0.670448, acc: 0.570312]  [A loss: 0.793336, acc: 0.257812]\n",
      "221: [D loss: 0.669890, acc: 0.558594]  [A loss: 0.892688, acc: 0.105469]\n",
      "222: [D loss: 0.670573, acc: 0.560547]  [A loss: 0.833113, acc: 0.175781]\n",
      "223: [D loss: 0.680030, acc: 0.546875]  [A loss: 0.903511, acc: 0.113281]\n",
      "224: [D loss: 0.688892, acc: 0.556641]  [A loss: 0.891920, acc: 0.089844]\n",
      "225: [D loss: 0.667258, acc: 0.597656]  [A loss: 0.892372, acc: 0.144531]\n",
      "226: [D loss: 0.679713, acc: 0.546875]  [A loss: 0.912086, acc: 0.085938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227: [D loss: 0.671175, acc: 0.587891]  [A loss: 0.895107, acc: 0.121094]\n",
      "228: [D loss: 0.668758, acc: 0.576172]  [A loss: 0.911885, acc: 0.093750]\n",
      "229: [D loss: 0.672330, acc: 0.587891]  [A loss: 0.950281, acc: 0.062500]\n",
      "230: [D loss: 0.675493, acc: 0.564453]  [A loss: 0.850724, acc: 0.132812]\n",
      "231: [D loss: 0.687334, acc: 0.548828]  [A loss: 1.021948, acc: 0.023438]\n",
      "232: [D loss: 0.663577, acc: 0.609375]  [A loss: 0.748131, acc: 0.390625]\n",
      "233: [D loss: 0.695643, acc: 0.556641]  [A loss: 1.142660, acc: 0.003906]\n",
      "234: [D loss: 0.679413, acc: 0.570312]  [A loss: 0.645078, acc: 0.667969]\n",
      "235: [D loss: 0.728519, acc: 0.503906]  [A loss: 1.053333, acc: 0.035156]\n",
      "236: [D loss: 0.671571, acc: 0.597656]  [A loss: 0.727091, acc: 0.398438]\n",
      "237: [D loss: 0.698055, acc: 0.527344]  [A loss: 0.987099, acc: 0.035156]\n",
      "238: [D loss: 0.664788, acc: 0.611328]  [A loss: 0.766818, acc: 0.312500]\n",
      "239: [D loss: 0.694522, acc: 0.546875]  [A loss: 0.940079, acc: 0.070312]\n",
      "240: [D loss: 0.669929, acc: 0.558594]  [A loss: 0.779249, acc: 0.296875]\n",
      "241: [D loss: 0.699426, acc: 0.529297]  [A loss: 0.967570, acc: 0.058594]\n",
      "242: [D loss: 0.677303, acc: 0.572266]  [A loss: 0.797625, acc: 0.250000]\n",
      "243: [D loss: 0.678931, acc: 0.552734]  [A loss: 0.990438, acc: 0.019531]\n",
      "244: [D loss: 0.674401, acc: 0.585938]  [A loss: 0.795190, acc: 0.261719]\n",
      "245: [D loss: 0.685370, acc: 0.556641]  [A loss: 0.923049, acc: 0.046875]\n",
      "246: [D loss: 0.672515, acc: 0.585938]  [A loss: 0.816231, acc: 0.191406]\n",
      "247: [D loss: 0.678573, acc: 0.580078]  [A loss: 0.968541, acc: 0.046875]\n",
      "248: [D loss: 0.664986, acc: 0.599609]  [A loss: 0.793317, acc: 0.261719]\n",
      "249: [D loss: 0.675045, acc: 0.552734]  [A loss: 1.027275, acc: 0.042969]\n",
      "250: [D loss: 0.658252, acc: 0.589844]  [A loss: 0.829821, acc: 0.226562]\n",
      "251: [D loss: 0.707528, acc: 0.505859]  [A loss: 1.079586, acc: 0.011719]\n",
      "252: [D loss: 0.694272, acc: 0.521484]  [A loss: 0.731369, acc: 0.402344]\n",
      "253: [D loss: 0.699816, acc: 0.533203]  [A loss: 1.037538, acc: 0.011719]\n",
      "254: [D loss: 0.666238, acc: 0.585938]  [A loss: 0.719478, acc: 0.449219]\n",
      "255: [D loss: 0.691510, acc: 0.527344]  [A loss: 0.996919, acc: 0.042969]\n",
      "256: [D loss: 0.658965, acc: 0.625000]  [A loss: 0.765578, acc: 0.316406]\n",
      "257: [D loss: 0.685321, acc: 0.572266]  [A loss: 1.063289, acc: 0.023438]\n",
      "258: [D loss: 0.678792, acc: 0.583984]  [A loss: 0.708510, acc: 0.453125]\n",
      "259: [D loss: 0.699625, acc: 0.542969]  [A loss: 1.021731, acc: 0.003906]\n",
      "260: [D loss: 0.670681, acc: 0.570312]  [A loss: 0.738074, acc: 0.371094]\n",
      "261: [D loss: 0.684586, acc: 0.552734]  [A loss: 0.957417, acc: 0.050781]\n",
      "262: [D loss: 0.678615, acc: 0.574219]  [A loss: 0.782542, acc: 0.269531]\n",
      "263: [D loss: 0.683259, acc: 0.556641]  [A loss: 0.919706, acc: 0.078125]\n",
      "264: [D loss: 0.672120, acc: 0.585938]  [A loss: 0.863903, acc: 0.136719]\n",
      "265: [D loss: 0.666658, acc: 0.599609]  [A loss: 0.891706, acc: 0.113281]\n",
      "266: [D loss: 0.671188, acc: 0.576172]  [A loss: 0.920648, acc: 0.093750]\n",
      "267: [D loss: 0.685737, acc: 0.531250]  [A loss: 0.975894, acc: 0.046875]\n",
      "268: [D loss: 0.670746, acc: 0.591797]  [A loss: 0.815625, acc: 0.234375]\n",
      "269: [D loss: 0.688090, acc: 0.576172]  [A loss: 0.966010, acc: 0.050781]\n",
      "270: [D loss: 0.677231, acc: 0.558594]  [A loss: 0.781810, acc: 0.339844]\n",
      "271: [D loss: 0.692677, acc: 0.535156]  [A loss: 1.054652, acc: 0.027344]\n",
      "272: [D loss: 0.672978, acc: 0.587891]  [A loss: 0.714078, acc: 0.457031]\n",
      "273: [D loss: 0.701960, acc: 0.533203]  [A loss: 1.122122, acc: 0.003906]\n",
      "274: [D loss: 0.668263, acc: 0.578125]  [A loss: 0.682527, acc: 0.570312]\n",
      "275: [D loss: 0.713204, acc: 0.515625]  [A loss: 1.113728, acc: 0.011719]\n",
      "276: [D loss: 0.664884, acc: 0.589844]  [A loss: 0.695489, acc: 0.515625]\n",
      "277: [D loss: 0.744974, acc: 0.500000]  [A loss: 0.993840, acc: 0.042969]\n",
      "278: [D loss: 0.672412, acc: 0.591797]  [A loss: 0.774408, acc: 0.285156]\n",
      "279: [D loss: 0.685624, acc: 0.544922]  [A loss: 0.947279, acc: 0.058594]\n",
      "280: [D loss: 0.657050, acc: 0.609375]  [A loss: 0.793939, acc: 0.277344]\n",
      "281: [D loss: 0.679979, acc: 0.556641]  [A loss: 0.906731, acc: 0.105469]\n",
      "282: [D loss: 0.668565, acc: 0.589844]  [A loss: 0.871692, acc: 0.152344]\n",
      "283: [D loss: 0.678386, acc: 0.562500]  [A loss: 0.864831, acc: 0.164062]\n",
      "284: [D loss: 0.675506, acc: 0.560547]  [A loss: 0.883717, acc: 0.128906]\n",
      "285: [D loss: 0.659553, acc: 0.634766]  [A loss: 0.807850, acc: 0.222656]\n",
      "286: [D loss: 0.682221, acc: 0.597656]  [A loss: 0.993797, acc: 0.054688]\n",
      "287: [D loss: 0.665328, acc: 0.611328]  [A loss: 0.769367, acc: 0.312500]\n",
      "288: [D loss: 0.674884, acc: 0.566406]  [A loss: 1.068907, acc: 0.023438]\n",
      "289: [D loss: 0.664529, acc: 0.609375]  [A loss: 0.761369, acc: 0.378906]\n",
      "290: [D loss: 0.763777, acc: 0.500000]  [A loss: 1.136776, acc: 0.007812]\n",
      "291: [D loss: 0.677109, acc: 0.556641]  [A loss: 0.698839, acc: 0.500000]\n",
      "292: [D loss: 0.715662, acc: 0.521484]  [A loss: 0.988750, acc: 0.046875]\n",
      "293: [D loss: 0.678865, acc: 0.583984]  [A loss: 0.737016, acc: 0.390625]\n",
      "294: [D loss: 0.704461, acc: 0.527344]  [A loss: 0.959997, acc: 0.050781]\n",
      "295: [D loss: 0.662854, acc: 0.593750]  [A loss: 0.774866, acc: 0.339844]\n",
      "296: [D loss: 0.662150, acc: 0.591797]  [A loss: 0.917024, acc: 0.140625]\n",
      "297: [D loss: 0.669409, acc: 0.585938]  [A loss: 0.852036, acc: 0.171875]\n",
      "298: [D loss: 0.699882, acc: 0.531250]  [A loss: 0.978696, acc: 0.031250]\n",
      "299: [D loss: 0.654323, acc: 0.634766]  [A loss: 0.793088, acc: 0.304688]\n",
      "300: [D loss: 0.705565, acc: 0.523438]  [A loss: 1.024567, acc: 0.031250]\n",
      "301: [D loss: 0.664386, acc: 0.607422]  [A loss: 0.758072, acc: 0.363281]\n",
      "302: [D loss: 0.677580, acc: 0.570312]  [A loss: 0.980978, acc: 0.046875]\n",
      "303: [D loss: 0.659488, acc: 0.601562]  [A loss: 0.800449, acc: 0.281250]\n",
      "304: [D loss: 0.666424, acc: 0.589844]  [A loss: 1.054255, acc: 0.023438]\n",
      "305: [D loss: 0.660501, acc: 0.605469]  [A loss: 0.781995, acc: 0.281250]\n",
      "306: [D loss: 0.680051, acc: 0.550781]  [A loss: 1.072107, acc: 0.027344]\n",
      "307: [D loss: 0.671927, acc: 0.607422]  [A loss: 0.706867, acc: 0.492188]\n",
      "308: [D loss: 0.699750, acc: 0.531250]  [A loss: 1.090839, acc: 0.011719]\n",
      "309: [D loss: 0.670211, acc: 0.564453]  [A loss: 0.709145, acc: 0.472656]\n",
      "310: [D loss: 0.734928, acc: 0.507812]  [A loss: 1.038807, acc: 0.011719]\n",
      "311: [D loss: 0.689226, acc: 0.542969]  [A loss: 0.796265, acc: 0.253906]\n",
      "312: [D loss: 0.685135, acc: 0.552734]  [A loss: 1.008458, acc: 0.039062]\n",
      "313: [D loss: 0.652574, acc: 0.634766]  [A loss: 0.828472, acc: 0.195312]\n",
      "314: [D loss: 0.724914, acc: 0.507812]  [A loss: 1.083564, acc: 0.019531]\n",
      "315: [D loss: 0.653496, acc: 0.658203]  [A loss: 0.727393, acc: 0.425781]\n",
      "316: [D loss: 0.693016, acc: 0.539062]  [A loss: 1.085272, acc: 0.042969]\n",
      "317: [D loss: 0.673440, acc: 0.607422]  [A loss: 0.759718, acc: 0.355469]\n",
      "318: [D loss: 0.694810, acc: 0.539062]  [A loss: 0.974559, acc: 0.054688]\n",
      "319: [D loss: 0.676210, acc: 0.562500]  [A loss: 0.786643, acc: 0.300781]\n",
      "320: [D loss: 0.688396, acc: 0.552734]  [A loss: 1.007902, acc: 0.042969]\n",
      "321: [D loss: 0.655827, acc: 0.623047]  [A loss: 0.818142, acc: 0.226562]\n",
      "322: [D loss: 0.687390, acc: 0.537109]  [A loss: 0.995090, acc: 0.046875]\n",
      "323: [D loss: 0.684912, acc: 0.550781]  [A loss: 0.838935, acc: 0.187500]\n",
      "324: [D loss: 0.674751, acc: 0.572266]  [A loss: 0.911989, acc: 0.093750]\n",
      "325: [D loss: 0.672225, acc: 0.583984]  [A loss: 0.870750, acc: 0.148438]\n",
      "326: [D loss: 0.676164, acc: 0.552734]  [A loss: 0.951486, acc: 0.097656]\n",
      "327: [D loss: 0.673859, acc: 0.574219]  [A loss: 0.929144, acc: 0.101562]\n",
      "328: [D loss: 0.673161, acc: 0.578125]  [A loss: 0.941646, acc: 0.109375]\n",
      "329: [D loss: 0.666105, acc: 0.593750]  [A loss: 0.888383, acc: 0.191406]\n",
      "330: [D loss: 0.683882, acc: 0.550781]  [A loss: 0.989277, acc: 0.070312]\n",
      "331: [D loss: 0.678144, acc: 0.566406]  [A loss: 0.826288, acc: 0.230469]\n",
      "332: [D loss: 0.691644, acc: 0.556641]  [A loss: 1.140282, acc: 0.039062]\n",
      "333: [D loss: 0.670028, acc: 0.580078]  [A loss: 0.696954, acc: 0.527344]\n",
      "334: [D loss: 0.684565, acc: 0.560547]  [A loss: 1.114933, acc: 0.023438]\n",
      "335: [D loss: 0.695033, acc: 0.554688]  [A loss: 0.847933, acc: 0.210938]\n",
      "336: [D loss: 0.686224, acc: 0.554688]  [A loss: 1.018774, acc: 0.066406]\n",
      "337: [D loss: 0.684154, acc: 0.556641]  [A loss: 0.893320, acc: 0.160156]\n",
      "338: [D loss: 0.677268, acc: 0.542969]  [A loss: 0.960450, acc: 0.085938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339: [D loss: 0.673230, acc: 0.550781]  [A loss: 0.905511, acc: 0.144531]\n",
      "340: [D loss: 0.680131, acc: 0.570312]  [A loss: 1.059040, acc: 0.046875]\n",
      "341: [D loss: 0.672907, acc: 0.578125]  [A loss: 0.835643, acc: 0.199219]\n",
      "342: [D loss: 0.682021, acc: 0.548828]  [A loss: 1.095680, acc: 0.015625]\n",
      "343: [D loss: 0.658944, acc: 0.599609]  [A loss: 0.724135, acc: 0.406250]\n",
      "344: [D loss: 0.714796, acc: 0.529297]  [A loss: 1.243167, acc: 0.003906]\n",
      "345: [D loss: 0.682279, acc: 0.550781]  [A loss: 0.627196, acc: 0.667969]\n",
      "346: [D loss: 0.725108, acc: 0.523438]  [A loss: 1.112629, acc: 0.011719]\n",
      "347: [D loss: 0.670821, acc: 0.583984]  [A loss: 0.723738, acc: 0.449219]\n",
      "348: [D loss: 0.696242, acc: 0.523438]  [A loss: 1.012912, acc: 0.058594]\n",
      "349: [D loss: 0.678258, acc: 0.562500]  [A loss: 0.831850, acc: 0.210938]\n",
      "350: [D loss: 0.689558, acc: 0.546875]  [A loss: 0.893038, acc: 0.136719]\n",
      "351: [D loss: 0.682163, acc: 0.574219]  [A loss: 0.897860, acc: 0.132812]\n",
      "352: [D loss: 0.680549, acc: 0.574219]  [A loss: 0.864159, acc: 0.187500]\n",
      "353: [D loss: 0.669122, acc: 0.595703]  [A loss: 0.918412, acc: 0.117188]\n",
      "354: [D loss: 0.686793, acc: 0.580078]  [A loss: 0.911520, acc: 0.105469]\n",
      "355: [D loss: 0.676247, acc: 0.578125]  [A loss: 0.866846, acc: 0.175781]\n",
      "356: [D loss: 0.675120, acc: 0.554688]  [A loss: 0.957979, acc: 0.089844]\n",
      "357: [D loss: 0.665763, acc: 0.605469]  [A loss: 0.854414, acc: 0.222656]\n",
      "358: [D loss: 0.669529, acc: 0.585938]  [A loss: 1.034940, acc: 0.042969]\n",
      "359: [D loss: 0.660169, acc: 0.597656]  [A loss: 0.777885, acc: 0.312500]\n",
      "360: [D loss: 0.744980, acc: 0.529297]  [A loss: 1.196119, acc: 0.015625]\n",
      "361: [D loss: 0.678602, acc: 0.554688]  [A loss: 0.628717, acc: 0.683594]\n",
      "362: [D loss: 0.730574, acc: 0.525391]  [A loss: 1.125357, acc: 0.003906]\n",
      "363: [D loss: 0.679900, acc: 0.550781]  [A loss: 0.724970, acc: 0.421875]\n",
      "364: [D loss: 0.707301, acc: 0.511719]  [A loss: 0.995888, acc: 0.074219]\n",
      "365: [D loss: 0.659163, acc: 0.605469]  [A loss: 0.812171, acc: 0.273438]\n",
      "366: [D loss: 0.671625, acc: 0.558594]  [A loss: 0.940693, acc: 0.101562]\n",
      "367: [D loss: 0.672774, acc: 0.589844]  [A loss: 0.885840, acc: 0.140625]\n",
      "368: [D loss: 0.682092, acc: 0.548828]  [A loss: 0.930200, acc: 0.089844]\n",
      "369: [D loss: 0.664047, acc: 0.591797]  [A loss: 0.821087, acc: 0.269531]\n",
      "370: [D loss: 0.664904, acc: 0.572266]  [A loss: 0.954833, acc: 0.105469]\n",
      "371: [D loss: 0.671078, acc: 0.583984]  [A loss: 0.797535, acc: 0.320312]\n",
      "372: [D loss: 0.711319, acc: 0.523438]  [A loss: 1.064973, acc: 0.035156]\n",
      "373: [D loss: 0.659129, acc: 0.609375]  [A loss: 0.759151, acc: 0.375000]\n",
      "374: [D loss: 0.694670, acc: 0.521484]  [A loss: 1.114741, acc: 0.015625]\n",
      "375: [D loss: 0.677030, acc: 0.562500]  [A loss: 0.746729, acc: 0.386719]\n",
      "376: [D loss: 0.727800, acc: 0.509766]  [A loss: 1.102515, acc: 0.019531]\n",
      "377: [D loss: 0.671179, acc: 0.578125]  [A loss: 0.706147, acc: 0.496094]\n",
      "378: [D loss: 0.701145, acc: 0.513672]  [A loss: 1.026361, acc: 0.039062]\n",
      "379: [D loss: 0.671604, acc: 0.580078]  [A loss: 0.756176, acc: 0.339844]\n",
      "380: [D loss: 0.697356, acc: 0.544922]  [A loss: 0.986910, acc: 0.066406]\n",
      "381: [D loss: 0.675460, acc: 0.583984]  [A loss: 0.785234, acc: 0.296875]\n",
      "382: [D loss: 0.679963, acc: 0.552734]  [A loss: 0.988792, acc: 0.042969]\n",
      "383: [D loss: 0.676952, acc: 0.580078]  [A loss: 0.782079, acc: 0.320312]\n",
      "384: [D loss: 0.685722, acc: 0.552734]  [A loss: 1.015418, acc: 0.058594]\n",
      "385: [D loss: 0.667119, acc: 0.599609]  [A loss: 0.803190, acc: 0.261719]\n",
      "386: [D loss: 0.685378, acc: 0.568359]  [A loss: 1.009875, acc: 0.050781]\n",
      "387: [D loss: 0.670082, acc: 0.597656]  [A loss: 0.801776, acc: 0.304688]\n",
      "388: [D loss: 0.686873, acc: 0.550781]  [A loss: 1.029241, acc: 0.046875]\n",
      "389: [D loss: 0.692292, acc: 0.531250]  [A loss: 0.927682, acc: 0.109375]\n",
      "390: [D loss: 0.673898, acc: 0.560547]  [A loss: 0.895497, acc: 0.128906]\n",
      "391: [D loss: 0.662682, acc: 0.597656]  [A loss: 0.879813, acc: 0.167969]\n",
      "392: [D loss: 0.681938, acc: 0.576172]  [A loss: 0.976477, acc: 0.089844]\n",
      "393: [D loss: 0.675700, acc: 0.542969]  [A loss: 0.885774, acc: 0.148438]\n",
      "394: [D loss: 0.698841, acc: 0.523438]  [A loss: 1.040513, acc: 0.039062]\n",
      "395: [D loss: 0.664260, acc: 0.621094]  [A loss: 0.760920, acc: 0.339844]\n",
      "396: [D loss: 0.697602, acc: 0.533203]  [A loss: 1.066950, acc: 0.039062]\n",
      "397: [D loss: 0.676302, acc: 0.554688]  [A loss: 0.727199, acc: 0.417969]\n",
      "398: [D loss: 0.699334, acc: 0.507812]  [A loss: 1.070725, acc: 0.023438]\n",
      "399: [D loss: 0.665972, acc: 0.589844]  [A loss: 0.742283, acc: 0.425781]\n",
      "400: [D loss: 0.700967, acc: 0.503906]  [A loss: 1.026474, acc: 0.062500]\n",
      "401: [D loss: 0.668218, acc: 0.574219]  [A loss: 0.694737, acc: 0.527344]\n",
      "402: [D loss: 0.714711, acc: 0.531250]  [A loss: 1.130940, acc: 0.007812]\n",
      "403: [D loss: 0.670350, acc: 0.582031]  [A loss: 0.715300, acc: 0.429688]\n",
      "404: [D loss: 0.750010, acc: 0.492188]  [A loss: 0.998183, acc: 0.035156]\n",
      "405: [D loss: 0.678838, acc: 0.570312]  [A loss: 0.749461, acc: 0.355469]\n",
      "406: [D loss: 0.700028, acc: 0.523438]  [A loss: 0.983755, acc: 0.066406]\n",
      "407: [D loss: 0.681417, acc: 0.544922]  [A loss: 0.797988, acc: 0.238281]\n",
      "408: [D loss: 0.664763, acc: 0.599609]  [A loss: 0.946370, acc: 0.078125]\n",
      "409: [D loss: 0.684672, acc: 0.548828]  [A loss: 0.878085, acc: 0.148438]\n",
      "410: [D loss: 0.667547, acc: 0.572266]  [A loss: 0.916381, acc: 0.105469]\n",
      "411: [D loss: 0.683415, acc: 0.580078]  [A loss: 0.897856, acc: 0.148438]\n",
      "412: [D loss: 0.682817, acc: 0.560547]  [A loss: 0.894666, acc: 0.140625]\n",
      "413: [D loss: 0.678370, acc: 0.570312]  [A loss: 0.904922, acc: 0.101562]\n",
      "414: [D loss: 0.673537, acc: 0.597656]  [A loss: 0.963636, acc: 0.093750]\n",
      "415: [D loss: 0.671337, acc: 0.585938]  [A loss: 0.910267, acc: 0.140625]\n",
      "416: [D loss: 0.674495, acc: 0.574219]  [A loss: 0.980335, acc: 0.082031]\n",
      "417: [D loss: 0.670076, acc: 0.599609]  [A loss: 0.784086, acc: 0.316406]\n",
      "418: [D loss: 0.682946, acc: 0.552734]  [A loss: 1.053698, acc: 0.035156]\n",
      "419: [D loss: 0.672900, acc: 0.556641]  [A loss: 0.759881, acc: 0.363281]\n",
      "420: [D loss: 0.705569, acc: 0.533203]  [A loss: 1.070830, acc: 0.019531]\n",
      "421: [D loss: 0.676876, acc: 0.603516]  [A loss: 0.735257, acc: 0.453125]\n",
      "422: [D loss: 0.702076, acc: 0.535156]  [A loss: 1.150060, acc: 0.035156]\n",
      "423: [D loss: 0.672174, acc: 0.576172]  [A loss: 0.708041, acc: 0.476562]\n",
      "424: [D loss: 0.760084, acc: 0.505859]  [A loss: 1.068352, acc: 0.019531]\n",
      "425: [D loss: 0.686521, acc: 0.539062]  [A loss: 0.801884, acc: 0.300781]\n",
      "426: [D loss: 0.702479, acc: 0.533203]  [A loss: 1.020603, acc: 0.027344]\n",
      "427: [D loss: 0.677270, acc: 0.560547]  [A loss: 0.764218, acc: 0.324219]\n",
      "428: [D loss: 0.699438, acc: 0.542969]  [A loss: 0.972864, acc: 0.066406]\n",
      "429: [D loss: 0.666086, acc: 0.589844]  [A loss: 0.847267, acc: 0.210938]\n",
      "430: [D loss: 0.686663, acc: 0.562500]  [A loss: 0.991777, acc: 0.105469]\n",
      "431: [D loss: 0.691832, acc: 0.544922]  [A loss: 0.863723, acc: 0.160156]\n",
      "432: [D loss: 0.690059, acc: 0.542969]  [A loss: 0.953869, acc: 0.093750]\n",
      "433: [D loss: 0.656024, acc: 0.609375]  [A loss: 0.819632, acc: 0.253906]\n",
      "434: [D loss: 0.695525, acc: 0.541016]  [A loss: 1.005853, acc: 0.058594]\n",
      "435: [D loss: 0.669819, acc: 0.585938]  [A loss: 0.773750, acc: 0.316406]\n",
      "436: [D loss: 0.691494, acc: 0.544922]  [A loss: 1.074764, acc: 0.058594]\n",
      "437: [D loss: 0.673650, acc: 0.574219]  [A loss: 0.677406, acc: 0.570312]\n",
      "438: [D loss: 0.699492, acc: 0.509766]  [A loss: 1.090811, acc: 0.031250]\n",
      "439: [D loss: 0.662596, acc: 0.566406]  [A loss: 0.729143, acc: 0.468750]\n",
      "440: [D loss: 0.705486, acc: 0.535156]  [A loss: 1.110934, acc: 0.007812]\n",
      "441: [D loss: 0.692451, acc: 0.531250]  [A loss: 0.719993, acc: 0.457031]\n",
      "442: [D loss: 0.683993, acc: 0.539062]  [A loss: 0.989937, acc: 0.054688]\n",
      "443: [D loss: 0.659704, acc: 0.609375]  [A loss: 0.763246, acc: 0.355469]\n",
      "444: [D loss: 0.686158, acc: 0.574219]  [A loss: 0.946756, acc: 0.117188]\n",
      "445: [D loss: 0.681632, acc: 0.554688]  [A loss: 0.909771, acc: 0.148438]\n",
      "446: [D loss: 0.675450, acc: 0.564453]  [A loss: 0.875585, acc: 0.179688]\n",
      "447: [D loss: 0.683212, acc: 0.556641]  [A loss: 0.934224, acc: 0.082031]\n",
      "448: [D loss: 0.677475, acc: 0.558594]  [A loss: 0.843611, acc: 0.257812]\n",
      "449: [D loss: 0.679753, acc: 0.554688]  [A loss: 0.949301, acc: 0.089844]\n",
      "450: [D loss: 0.673429, acc: 0.589844]  [A loss: 0.844127, acc: 0.191406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451: [D loss: 0.681854, acc: 0.558594]  [A loss: 0.959718, acc: 0.109375]\n",
      "452: [D loss: 0.682165, acc: 0.576172]  [A loss: 0.837291, acc: 0.242188]\n",
      "453: [D loss: 0.676448, acc: 0.597656]  [A loss: 0.939486, acc: 0.113281]\n",
      "454: [D loss: 0.669848, acc: 0.603516]  [A loss: 0.888075, acc: 0.164062]\n",
      "455: [D loss: 0.675745, acc: 0.576172]  [A loss: 0.909545, acc: 0.140625]\n",
      "456: [D loss: 0.691901, acc: 0.544922]  [A loss: 1.074784, acc: 0.039062]\n",
      "457: [D loss: 0.670375, acc: 0.574219]  [A loss: 0.725515, acc: 0.437500]\n",
      "458: [D loss: 0.704263, acc: 0.558594]  [A loss: 1.219538, acc: 0.000000]\n",
      "459: [D loss: 0.685008, acc: 0.570312]  [A loss: 0.612334, acc: 0.718750]\n",
      "460: [D loss: 0.745733, acc: 0.505859]  [A loss: 1.114857, acc: 0.035156]\n",
      "461: [D loss: 0.677808, acc: 0.566406]  [A loss: 0.721331, acc: 0.441406]\n",
      "462: [D loss: 0.709776, acc: 0.535156]  [A loss: 0.978803, acc: 0.035156]\n",
      "463: [D loss: 0.680977, acc: 0.556641]  [A loss: 0.836479, acc: 0.214844]\n",
      "464: [D loss: 0.676078, acc: 0.582031]  [A loss: 0.879158, acc: 0.152344]\n",
      "465: [D loss: 0.667168, acc: 0.568359]  [A loss: 0.829416, acc: 0.199219]\n",
      "466: [D loss: 0.691766, acc: 0.562500]  [A loss: 0.971862, acc: 0.078125]\n",
      "467: [D loss: 0.655679, acc: 0.621094]  [A loss: 0.841475, acc: 0.195312]\n",
      "468: [D loss: 0.669930, acc: 0.589844]  [A loss: 0.935963, acc: 0.117188]\n",
      "469: [D loss: 0.659695, acc: 0.615234]  [A loss: 0.831763, acc: 0.230469]\n",
      "470: [D loss: 0.673818, acc: 0.574219]  [A loss: 1.029893, acc: 0.042969]\n",
      "471: [D loss: 0.670784, acc: 0.572266]  [A loss: 0.778243, acc: 0.312500]\n",
      "472: [D loss: 0.690145, acc: 0.539062]  [A loss: 1.043319, acc: 0.042969]\n",
      "473: [D loss: 0.673384, acc: 0.572266]  [A loss: 0.785655, acc: 0.308594]\n",
      "474: [D loss: 0.687065, acc: 0.546875]  [A loss: 1.048038, acc: 0.046875]\n",
      "475: [D loss: 0.672396, acc: 0.566406]  [A loss: 0.796157, acc: 0.316406]\n",
      "476: [D loss: 0.688660, acc: 0.548828]  [A loss: 1.090816, acc: 0.046875]\n",
      "477: [D loss: 0.668356, acc: 0.601562]  [A loss: 0.704730, acc: 0.503906]\n",
      "478: [D loss: 0.694255, acc: 0.542969]  [A loss: 1.076998, acc: 0.035156]\n",
      "479: [D loss: 0.665858, acc: 0.582031]  [A loss: 0.778488, acc: 0.371094]\n",
      "480: [D loss: 0.693191, acc: 0.537109]  [A loss: 1.057630, acc: 0.070312]\n",
      "481: [D loss: 0.670164, acc: 0.595703]  [A loss: 0.751560, acc: 0.414062]\n",
      "482: [D loss: 0.742854, acc: 0.511719]  [A loss: 1.015358, acc: 0.042969]\n",
      "483: [D loss: 0.724384, acc: 0.453125]  [A loss: 0.866921, acc: 0.179688]\n",
      "484: [D loss: 0.693188, acc: 0.533203]  [A loss: 0.909422, acc: 0.101562]\n",
      "485: [D loss: 0.675843, acc: 0.574219]  [A loss: 0.831548, acc: 0.238281]\n",
      "486: [D loss: 0.680483, acc: 0.570312]  [A loss: 0.951413, acc: 0.105469]\n",
      "487: [D loss: 0.684704, acc: 0.556641]  [A loss: 0.878336, acc: 0.144531]\n",
      "488: [D loss: 0.689727, acc: 0.570312]  [A loss: 0.923924, acc: 0.101562]\n",
      "489: [D loss: 0.676227, acc: 0.578125]  [A loss: 0.881459, acc: 0.187500]\n",
      "490: [D loss: 0.671911, acc: 0.580078]  [A loss: 0.920722, acc: 0.132812]\n",
      "491: [D loss: 0.677139, acc: 0.544922]  [A loss: 0.851186, acc: 0.222656]\n",
      "492: [D loss: 0.675112, acc: 0.576172]  [A loss: 1.002342, acc: 0.050781]\n",
      "493: [D loss: 0.685444, acc: 0.554688]  [A loss: 0.806190, acc: 0.273438]\n",
      "494: [D loss: 0.696644, acc: 0.517578]  [A loss: 0.990030, acc: 0.082031]\n",
      "495: [D loss: 0.676969, acc: 0.570312]  [A loss: 0.818935, acc: 0.253906]\n",
      "496: [D loss: 0.694929, acc: 0.539062]  [A loss: 1.082901, acc: 0.031250]\n",
      "497: [D loss: 0.677885, acc: 0.576172]  [A loss: 0.688045, acc: 0.578125]\n",
      "498: [D loss: 0.709407, acc: 0.544922]  [A loss: 1.205537, acc: 0.023438]\n",
      "499: [D loss: 0.661747, acc: 0.591797]  [A loss: 0.767989, acc: 0.320312]\n",
      "500: [D loss: 0.753217, acc: 0.511719]  [A loss: 0.995212, acc: 0.074219]\n",
      "501: [D loss: 0.691452, acc: 0.560547]  [A loss: 0.870220, acc: 0.164062]\n",
      "502: [D loss: 0.704530, acc: 0.511719]  [A loss: 0.930125, acc: 0.105469]\n",
      "503: [D loss: 0.684261, acc: 0.544922]  [A loss: 0.872971, acc: 0.140625]\n",
      "504: [D loss: 0.684272, acc: 0.548828]  [A loss: 1.002257, acc: 0.070312]\n",
      "505: [D loss: 0.665815, acc: 0.603516]  [A loss: 0.839446, acc: 0.222656]\n",
      "506: [D loss: 0.685291, acc: 0.542969]  [A loss: 1.078466, acc: 0.054688]\n",
      "507: [D loss: 0.676218, acc: 0.580078]  [A loss: 0.778101, acc: 0.292969]\n",
      "508: [D loss: 0.681385, acc: 0.560547]  [A loss: 1.032394, acc: 0.039062]\n",
      "509: [D loss: 0.682708, acc: 0.564453]  [A loss: 0.774053, acc: 0.332031]\n",
      "510: [D loss: 0.701510, acc: 0.529297]  [A loss: 1.072870, acc: 0.027344]\n",
      "511: [D loss: 0.670319, acc: 0.568359]  [A loss: 0.736300, acc: 0.417969]\n",
      "512: [D loss: 0.698944, acc: 0.531250]  [A loss: 1.103682, acc: 0.019531]\n",
      "513: [D loss: 0.674109, acc: 0.574219]  [A loss: 0.715471, acc: 0.468750]\n",
      "514: [D loss: 0.702220, acc: 0.541016]  [A loss: 1.053868, acc: 0.031250]\n",
      "515: [D loss: 0.672246, acc: 0.611328]  [A loss: 0.704037, acc: 0.484375]\n",
      "516: [D loss: 0.690151, acc: 0.519531]  [A loss: 0.979638, acc: 0.074219]\n",
      "517: [D loss: 0.680547, acc: 0.578125]  [A loss: 0.787145, acc: 0.269531]\n",
      "518: [D loss: 0.686219, acc: 0.558594]  [A loss: 0.949883, acc: 0.070312]\n",
      "519: [D loss: 0.668357, acc: 0.595703]  [A loss: 0.838521, acc: 0.195312]\n",
      "520: [D loss: 0.666618, acc: 0.591797]  [A loss: 0.921313, acc: 0.125000]\n",
      "521: [D loss: 0.672912, acc: 0.574219]  [A loss: 0.837929, acc: 0.234375]\n",
      "522: [D loss: 0.678444, acc: 0.535156]  [A loss: 0.944284, acc: 0.105469]\n",
      "523: [D loss: 0.684413, acc: 0.566406]  [A loss: 0.834671, acc: 0.210938]\n",
      "524: [D loss: 0.683383, acc: 0.527344]  [A loss: 0.955522, acc: 0.113281]\n",
      "525: [D loss: 0.676722, acc: 0.578125]  [A loss: 0.842567, acc: 0.234375]\n",
      "526: [D loss: 0.689578, acc: 0.556641]  [A loss: 1.098848, acc: 0.046875]\n",
      "527: [D loss: 0.666013, acc: 0.607422]  [A loss: 0.749748, acc: 0.386719]\n",
      "528: [D loss: 0.692580, acc: 0.556641]  [A loss: 1.018057, acc: 0.070312]\n",
      "529: [D loss: 0.684897, acc: 0.525391]  [A loss: 0.822892, acc: 0.273438]\n",
      "530: [D loss: 0.694781, acc: 0.546875]  [A loss: 1.016749, acc: 0.074219]\n",
      "531: [D loss: 0.673751, acc: 0.554688]  [A loss: 0.757879, acc: 0.359375]\n",
      "532: [D loss: 0.702614, acc: 0.537109]  [A loss: 1.106528, acc: 0.011719]\n",
      "533: [D loss: 0.674978, acc: 0.552734]  [A loss: 0.662117, acc: 0.570312]\n",
      "534: [D loss: 0.704694, acc: 0.498047]  [A loss: 1.120594, acc: 0.027344]\n",
      "535: [D loss: 0.677167, acc: 0.568359]  [A loss: 0.722011, acc: 0.460938]\n",
      "536: [D loss: 0.704202, acc: 0.525391]  [A loss: 0.934800, acc: 0.085938]\n",
      "537: [D loss: 0.698354, acc: 0.533203]  [A loss: 0.917987, acc: 0.097656]\n",
      "538: [D loss: 0.684868, acc: 0.566406]  [A loss: 0.880120, acc: 0.144531]\n",
      "539: [D loss: 0.707333, acc: 0.527344]  [A loss: 0.846723, acc: 0.238281]\n",
      "540: [D loss: 0.693082, acc: 0.550781]  [A loss: 0.981464, acc: 0.074219]\n",
      "541: [D loss: 0.681639, acc: 0.562500]  [A loss: 0.874164, acc: 0.128906]\n",
      "542: [D loss: 0.676884, acc: 0.572266]  [A loss: 0.921085, acc: 0.097656]\n",
      "543: [D loss: 0.666861, acc: 0.580078]  [A loss: 0.913452, acc: 0.144531]\n",
      "544: [D loss: 0.684217, acc: 0.558594]  [A loss: 0.893302, acc: 0.128906]\n",
      "545: [D loss: 0.670649, acc: 0.587891]  [A loss: 0.959636, acc: 0.093750]\n",
      "546: [D loss: 0.694622, acc: 0.546875]  [A loss: 0.888686, acc: 0.148438]\n",
      "547: [D loss: 0.668224, acc: 0.580078]  [A loss: 0.977927, acc: 0.085938]\n",
      "548: [D loss: 0.682238, acc: 0.564453]  [A loss: 0.805277, acc: 0.304688]\n",
      "549: [D loss: 0.697226, acc: 0.548828]  [A loss: 1.079413, acc: 0.054688]\n",
      "550: [D loss: 0.676110, acc: 0.548828]  [A loss: 0.720262, acc: 0.484375]\n",
      "551: [D loss: 0.724316, acc: 0.541016]  [A loss: 1.234871, acc: 0.003906]\n",
      "552: [D loss: 0.669380, acc: 0.558594]  [A loss: 0.634610, acc: 0.640625]\n",
      "553: [D loss: 0.730510, acc: 0.515625]  [A loss: 1.092682, acc: 0.023438]\n",
      "554: [D loss: 0.670315, acc: 0.583984]  [A loss: 0.668846, acc: 0.574219]\n",
      "555: [D loss: 0.713345, acc: 0.537109]  [A loss: 0.934239, acc: 0.113281]\n",
      "556: [D loss: 0.666755, acc: 0.580078]  [A loss: 0.840184, acc: 0.210938]\n",
      "557: [D loss: 0.676036, acc: 0.582031]  [A loss: 0.806066, acc: 0.285156]\n",
      "558: [D loss: 0.698916, acc: 0.519531]  [A loss: 0.959878, acc: 0.082031]\n",
      "559: [D loss: 0.673034, acc: 0.601562]  [A loss: 0.830852, acc: 0.187500]\n",
      "560: [D loss: 0.679875, acc: 0.568359]  [A loss: 0.977378, acc: 0.105469]\n",
      "561: [D loss: 0.690213, acc: 0.535156]  [A loss: 0.756254, acc: 0.355469]\n",
      "562: [D loss: 0.688217, acc: 0.578125]  [A loss: 1.024629, acc: 0.050781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563: [D loss: 0.672625, acc: 0.572266]  [A loss: 0.822405, acc: 0.234375]\n",
      "564: [D loss: 0.715343, acc: 0.544922]  [A loss: 0.980920, acc: 0.062500]\n",
      "565: [D loss: 0.686872, acc: 0.535156]  [A loss: 0.905757, acc: 0.125000]\n",
      "566: [D loss: 0.701862, acc: 0.546875]  [A loss: 0.953240, acc: 0.089844]\n",
      "567: [D loss: 0.657082, acc: 0.617188]  [A loss: 0.916529, acc: 0.140625]\n",
      "568: [D loss: 0.688524, acc: 0.539062]  [A loss: 0.919976, acc: 0.101562]\n",
      "569: [D loss: 0.673147, acc: 0.583984]  [A loss: 0.915764, acc: 0.105469]\n",
      "570: [D loss: 0.682421, acc: 0.552734]  [A loss: 0.864994, acc: 0.199219]\n",
      "571: [D loss: 0.700429, acc: 0.533203]  [A loss: 0.977358, acc: 0.058594]\n",
      "572: [D loss: 0.680078, acc: 0.554688]  [A loss: 0.853998, acc: 0.199219]\n",
      "573: [D loss: 0.679827, acc: 0.541016]  [A loss: 1.008901, acc: 0.062500]\n",
      "574: [D loss: 0.689250, acc: 0.542969]  [A loss: 0.793558, acc: 0.320312]\n",
      "575: [D loss: 0.710128, acc: 0.511719]  [A loss: 1.133380, acc: 0.035156]\n",
      "576: [D loss: 0.663768, acc: 0.599609]  [A loss: 0.651796, acc: 0.621094]\n",
      "577: [D loss: 0.729957, acc: 0.503906]  [A loss: 1.119092, acc: 0.015625]\n",
      "578: [D loss: 0.680737, acc: 0.560547]  [A loss: 0.719225, acc: 0.484375]\n",
      "579: [D loss: 0.712925, acc: 0.525391]  [A loss: 1.136269, acc: 0.035156]\n",
      "580: [D loss: 0.682163, acc: 0.535156]  [A loss: 0.781486, acc: 0.375000]\n",
      "581: [D loss: 0.707399, acc: 0.539062]  [A loss: 0.952190, acc: 0.066406]\n",
      "582: [D loss: 0.676954, acc: 0.583984]  [A loss: 0.819471, acc: 0.257812]\n",
      "583: [D loss: 0.670666, acc: 0.591797]  [A loss: 0.969798, acc: 0.078125]\n",
      "584: [D loss: 0.670507, acc: 0.597656]  [A loss: 0.829717, acc: 0.218750]\n",
      "585: [D loss: 0.669655, acc: 0.599609]  [A loss: 0.900120, acc: 0.164062]\n",
      "586: [D loss: 0.684437, acc: 0.568359]  [A loss: 0.829226, acc: 0.261719]\n",
      "587: [D loss: 0.676793, acc: 0.566406]  [A loss: 0.961990, acc: 0.113281]\n",
      "588: [D loss: 0.669868, acc: 0.583984]  [A loss: 0.867770, acc: 0.207031]\n",
      "589: [D loss: 0.687590, acc: 0.556641]  [A loss: 0.999089, acc: 0.074219]\n",
      "590: [D loss: 0.668884, acc: 0.593750]  [A loss: 0.822875, acc: 0.253906]\n",
      "591: [D loss: 0.689162, acc: 0.568359]  [A loss: 1.075332, acc: 0.042969]\n",
      "592: [D loss: 0.686996, acc: 0.556641]  [A loss: 0.699047, acc: 0.527344]\n",
      "593: [D loss: 0.708153, acc: 0.523438]  [A loss: 1.136236, acc: 0.031250]\n",
      "594: [D loss: 0.673114, acc: 0.552734]  [A loss: 0.631959, acc: 0.675781]\n",
      "595: [D loss: 0.727173, acc: 0.513672]  [A loss: 1.063283, acc: 0.035156]\n",
      "596: [D loss: 0.663685, acc: 0.605469]  [A loss: 0.743729, acc: 0.394531]\n",
      "597: [D loss: 0.702372, acc: 0.521484]  [A loss: 1.005355, acc: 0.093750]\n",
      "598: [D loss: 0.679511, acc: 0.587891]  [A loss: 0.787599, acc: 0.316406]\n",
      "599: [D loss: 0.679593, acc: 0.578125]  [A loss: 0.906723, acc: 0.125000]\n",
      "600: [D loss: 0.675207, acc: 0.560547]  [A loss: 0.784648, acc: 0.324219]\n",
      "601: [D loss: 0.718024, acc: 0.505859]  [A loss: 0.966242, acc: 0.062500]\n",
      "602: [D loss: 0.675050, acc: 0.597656]  [A loss: 0.772672, acc: 0.363281]\n",
      "603: [D loss: 0.707132, acc: 0.529297]  [A loss: 0.983798, acc: 0.085938]\n",
      "604: [D loss: 0.690294, acc: 0.533203]  [A loss: 0.752311, acc: 0.398438]\n",
      "605: [D loss: 0.697261, acc: 0.537109]  [A loss: 1.070446, acc: 0.054688]\n",
      "606: [D loss: 0.688893, acc: 0.550781]  [A loss: 0.707706, acc: 0.496094]\n",
      "607: [D loss: 0.720051, acc: 0.521484]  [A loss: 1.081195, acc: 0.031250]\n",
      "608: [D loss: 0.677655, acc: 0.572266]  [A loss: 0.714946, acc: 0.480469]\n",
      "609: [D loss: 0.709147, acc: 0.511719]  [A loss: 0.984531, acc: 0.062500]\n",
      "610: [D loss: 0.667401, acc: 0.570312]  [A loss: 0.772244, acc: 0.355469]\n",
      "611: [D loss: 0.693604, acc: 0.544922]  [A loss: 0.965624, acc: 0.058594]\n",
      "612: [D loss: 0.685725, acc: 0.552734]  [A loss: 0.760444, acc: 0.367188]\n",
      "613: [D loss: 0.703560, acc: 0.523438]  [A loss: 0.982456, acc: 0.089844]\n",
      "614: [D loss: 0.680323, acc: 0.539062]  [A loss: 0.754317, acc: 0.343750]\n",
      "615: [D loss: 0.669732, acc: 0.548828]  [A loss: 0.966358, acc: 0.097656]\n",
      "616: [D loss: 0.668389, acc: 0.603516]  [A loss: 0.787431, acc: 0.324219]\n",
      "617: [D loss: 0.688222, acc: 0.552734]  [A loss: 1.025951, acc: 0.070312]\n",
      "618: [D loss: 0.679118, acc: 0.564453]  [A loss: 0.725125, acc: 0.453125]\n",
      "619: [D loss: 0.695620, acc: 0.541016]  [A loss: 1.093471, acc: 0.039062]\n",
      "620: [D loss: 0.667311, acc: 0.574219]  [A loss: 0.726059, acc: 0.433594]\n",
      "621: [D loss: 0.710541, acc: 0.541016]  [A loss: 0.993573, acc: 0.070312]\n",
      "622: [D loss: 0.689352, acc: 0.552734]  [A loss: 0.867642, acc: 0.183594]\n",
      "623: [D loss: 0.680032, acc: 0.578125]  [A loss: 0.923462, acc: 0.125000]\n",
      "624: [D loss: 0.672593, acc: 0.576172]  [A loss: 0.862810, acc: 0.195312]\n",
      "625: [D loss: 0.690715, acc: 0.558594]  [A loss: 0.994453, acc: 0.070312]\n",
      "626: [D loss: 0.682026, acc: 0.564453]  [A loss: 0.817201, acc: 0.300781]\n",
      "627: [D loss: 0.682020, acc: 0.576172]  [A loss: 0.974349, acc: 0.082031]\n",
      "628: [D loss: 0.688995, acc: 0.531250]  [A loss: 0.878772, acc: 0.167969]\n",
      "629: [D loss: 0.676594, acc: 0.564453]  [A loss: 0.882273, acc: 0.175781]\n",
      "630: [D loss: 0.673077, acc: 0.583984]  [A loss: 0.960868, acc: 0.144531]\n",
      "631: [D loss: 0.690514, acc: 0.546875]  [A loss: 0.781803, acc: 0.343750]\n",
      "632: [D loss: 0.696942, acc: 0.507812]  [A loss: 1.080870, acc: 0.027344]\n",
      "633: [D loss: 0.681518, acc: 0.539062]  [A loss: 0.731331, acc: 0.468750]\n",
      "634: [D loss: 0.722317, acc: 0.519531]  [A loss: 1.144141, acc: 0.039062]\n",
      "635: [D loss: 0.677491, acc: 0.574219]  [A loss: 0.663631, acc: 0.546875]\n",
      "636: [D loss: 0.713238, acc: 0.533203]  [A loss: 1.070957, acc: 0.050781]\n",
      "637: [D loss: 0.701958, acc: 0.507812]  [A loss: 0.755803, acc: 0.359375]\n",
      "638: [D loss: 0.700981, acc: 0.546875]  [A loss: 0.970957, acc: 0.089844]\n",
      "639: [D loss: 0.675456, acc: 0.601562]  [A loss: 0.830036, acc: 0.273438]\n",
      "640: [D loss: 0.669571, acc: 0.587891]  [A loss: 0.877524, acc: 0.160156]\n",
      "641: [D loss: 0.684092, acc: 0.544922]  [A loss: 0.852811, acc: 0.179688]\n",
      "642: [D loss: 0.682424, acc: 0.556641]  [A loss: 0.900758, acc: 0.121094]\n",
      "643: [D loss: 0.675580, acc: 0.585938]  [A loss: 0.792528, acc: 0.316406]\n",
      "644: [D loss: 0.679475, acc: 0.556641]  [A loss: 1.020078, acc: 0.070312]\n",
      "645: [D loss: 0.672488, acc: 0.572266]  [A loss: 0.808629, acc: 0.273438]\n",
      "646: [D loss: 0.693054, acc: 0.542969]  [A loss: 1.026307, acc: 0.035156]\n",
      "647: [D loss: 0.671267, acc: 0.589844]  [A loss: 0.749632, acc: 0.402344]\n",
      "648: [D loss: 0.699355, acc: 0.523438]  [A loss: 1.101278, acc: 0.066406]\n",
      "649: [D loss: 0.678104, acc: 0.583984]  [A loss: 0.779169, acc: 0.308594]\n",
      "650: [D loss: 0.697348, acc: 0.515625]  [A loss: 1.027971, acc: 0.066406]\n",
      "651: [D loss: 0.672835, acc: 0.550781]  [A loss: 0.691454, acc: 0.535156]\n",
      "652: [D loss: 0.721646, acc: 0.505859]  [A loss: 1.006281, acc: 0.062500]\n",
      "653: [D loss: 0.670358, acc: 0.572266]  [A loss: 0.793321, acc: 0.312500]\n",
      "654: [D loss: 0.699176, acc: 0.531250]  [A loss: 1.063154, acc: 0.035156]\n",
      "655: [D loss: 0.680455, acc: 0.550781]  [A loss: 0.775352, acc: 0.312500]\n",
      "656: [D loss: 0.682090, acc: 0.562500]  [A loss: 0.958631, acc: 0.078125]\n",
      "657: [D loss: 0.680776, acc: 0.572266]  [A loss: 0.787680, acc: 0.339844]\n",
      "658: [D loss: 0.687323, acc: 0.562500]  [A loss: 1.036153, acc: 0.058594]\n",
      "659: [D loss: 0.656576, acc: 0.625000]  [A loss: 0.722270, acc: 0.472656]\n",
      "660: [D loss: 0.691587, acc: 0.533203]  [A loss: 1.070924, acc: 0.042969]\n",
      "661: [D loss: 0.651022, acc: 0.625000]  [A loss: 0.726388, acc: 0.457031]\n",
      "662: [D loss: 0.709125, acc: 0.535156]  [A loss: 1.036006, acc: 0.074219]\n",
      "663: [D loss: 0.674299, acc: 0.587891]  [A loss: 0.846281, acc: 0.238281]\n",
      "664: [D loss: 0.679552, acc: 0.568359]  [A loss: 0.918335, acc: 0.148438]\n",
      "665: [D loss: 0.677775, acc: 0.570312]  [A loss: 0.854278, acc: 0.203125]\n",
      "666: [D loss: 0.662448, acc: 0.605469]  [A loss: 0.943919, acc: 0.132812]\n",
      "667: [D loss: 0.693949, acc: 0.533203]  [A loss: 0.949140, acc: 0.117188]\n",
      "668: [D loss: 0.668687, acc: 0.582031]  [A loss: 0.841960, acc: 0.250000]\n",
      "669: [D loss: 0.693615, acc: 0.542969]  [A loss: 1.054756, acc: 0.082031]\n",
      "670: [D loss: 0.676409, acc: 0.574219]  [A loss: 0.836312, acc: 0.277344]\n",
      "671: [D loss: 0.704719, acc: 0.529297]  [A loss: 1.103140, acc: 0.050781]\n",
      "672: [D loss: 0.663258, acc: 0.609375]  [A loss: 0.703416, acc: 0.527344]\n",
      "673: [D loss: 0.701133, acc: 0.539062]  [A loss: 1.162669, acc: 0.058594]\n",
      "674: [D loss: 0.681784, acc: 0.533203]  [A loss: 0.685733, acc: 0.535156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675: [D loss: 0.699358, acc: 0.535156]  [A loss: 1.058161, acc: 0.058594]\n",
      "676: [D loss: 0.685066, acc: 0.537109]  [A loss: 0.824678, acc: 0.261719]\n",
      "677: [D loss: 0.683753, acc: 0.583984]  [A loss: 0.938215, acc: 0.148438]\n",
      "678: [D loss: 0.700943, acc: 0.523438]  [A loss: 0.855880, acc: 0.218750]\n",
      "679: [D loss: 0.693369, acc: 0.537109]  [A loss: 0.975666, acc: 0.105469]\n",
      "680: [D loss: 0.675957, acc: 0.570312]  [A loss: 0.861660, acc: 0.257812]\n",
      "681: [D loss: 0.688109, acc: 0.554688]  [A loss: 1.013181, acc: 0.070312]\n",
      "682: [D loss: 0.690238, acc: 0.541016]  [A loss: 0.773725, acc: 0.367188]\n",
      "683: [D loss: 0.688653, acc: 0.542969]  [A loss: 1.071283, acc: 0.062500]\n",
      "684: [D loss: 0.678367, acc: 0.560547]  [A loss: 0.714793, acc: 0.476562]\n",
      "685: [D loss: 0.695390, acc: 0.544922]  [A loss: 1.142198, acc: 0.050781]\n",
      "686: [D loss: 0.669247, acc: 0.585938]  [A loss: 0.659173, acc: 0.582031]\n",
      "687: [D loss: 0.725690, acc: 0.515625]  [A loss: 1.049375, acc: 0.085938]\n",
      "688: [D loss: 0.676901, acc: 0.583984]  [A loss: 0.699427, acc: 0.527344]\n",
      "689: [D loss: 0.697396, acc: 0.562500]  [A loss: 1.020285, acc: 0.039062]\n",
      "690: [D loss: 0.679336, acc: 0.554688]  [A loss: 0.803863, acc: 0.296875]\n",
      "691: [D loss: 0.695024, acc: 0.541016]  [A loss: 0.956092, acc: 0.109375]\n",
      "692: [D loss: 0.652934, acc: 0.628906]  [A loss: 0.842009, acc: 0.273438]\n",
      "693: [D loss: 0.691431, acc: 0.558594]  [A loss: 0.956631, acc: 0.085938]\n",
      "694: [D loss: 0.681917, acc: 0.582031]  [A loss: 0.819996, acc: 0.269531]\n",
      "695: [D loss: 0.681888, acc: 0.548828]  [A loss: 0.949355, acc: 0.128906]\n",
      "696: [D loss: 0.668458, acc: 0.593750]  [A loss: 0.908523, acc: 0.179688]\n",
      "697: [D loss: 0.663870, acc: 0.583984]  [A loss: 0.903094, acc: 0.156250]\n",
      "698: [D loss: 0.703631, acc: 0.533203]  [A loss: 0.915280, acc: 0.140625]\n",
      "699: [D loss: 0.686061, acc: 0.550781]  [A loss: 0.877007, acc: 0.203125]\n",
      "700: [D loss: 0.691951, acc: 0.560547]  [A loss: 0.979733, acc: 0.070312]\n",
      "701: [D loss: 0.678699, acc: 0.544922]  [A loss: 0.836537, acc: 0.246094]\n",
      "702: [D loss: 0.679258, acc: 0.578125]  [A loss: 1.031578, acc: 0.046875]\n",
      "703: [D loss: 0.667666, acc: 0.587891]  [A loss: 0.804890, acc: 0.335938]\n",
      "704: [D loss: 0.696474, acc: 0.548828]  [A loss: 1.153300, acc: 0.015625]\n",
      "705: [D loss: 0.700286, acc: 0.548828]  [A loss: 0.609625, acc: 0.734375]\n",
      "706: [D loss: 0.738080, acc: 0.519531]  [A loss: 1.228179, acc: 0.031250]\n",
      "707: [D loss: 0.715921, acc: 0.515625]  [A loss: 0.704170, acc: 0.484375]\n",
      "708: [D loss: 0.708758, acc: 0.527344]  [A loss: 0.992939, acc: 0.066406]\n",
      "709: [D loss: 0.677393, acc: 0.580078]  [A loss: 0.732624, acc: 0.453125]\n",
      "710: [D loss: 0.685986, acc: 0.572266]  [A loss: 0.966885, acc: 0.082031]\n",
      "711: [D loss: 0.677036, acc: 0.615234]  [A loss: 0.781319, acc: 0.367188]\n",
      "712: [D loss: 0.693614, acc: 0.544922]  [A loss: 1.039966, acc: 0.066406]\n",
      "713: [D loss: 0.678923, acc: 0.570312]  [A loss: 0.766880, acc: 0.378906]\n",
      "714: [D loss: 0.698066, acc: 0.533203]  [A loss: 0.957883, acc: 0.113281]\n",
      "715: [D loss: 0.685039, acc: 0.539062]  [A loss: 0.880609, acc: 0.195312]\n",
      "716: [D loss: 0.685865, acc: 0.560547]  [A loss: 0.920998, acc: 0.183594]\n",
      "717: [D loss: 0.680740, acc: 0.556641]  [A loss: 0.868968, acc: 0.167969]\n",
      "718: [D loss: 0.684134, acc: 0.560547]  [A loss: 0.974329, acc: 0.085938]\n",
      "719: [D loss: 0.683825, acc: 0.562500]  [A loss: 0.861462, acc: 0.218750]\n",
      "720: [D loss: 0.679179, acc: 0.554688]  [A loss: 0.970523, acc: 0.113281]\n",
      "721: [D loss: 0.679697, acc: 0.583984]  [A loss: 0.818167, acc: 0.257812]\n",
      "722: [D loss: 0.685054, acc: 0.548828]  [A loss: 0.998987, acc: 0.101562]\n",
      "723: [D loss: 0.691526, acc: 0.529297]  [A loss: 0.785029, acc: 0.351562]\n",
      "724: [D loss: 0.675611, acc: 0.585938]  [A loss: 1.045706, acc: 0.066406]\n",
      "725: [D loss: 0.673937, acc: 0.550781]  [A loss: 0.672297, acc: 0.574219]\n",
      "726: [D loss: 0.728955, acc: 0.509766]  [A loss: 1.225844, acc: 0.031250]\n",
      "727: [D loss: 0.689136, acc: 0.552734]  [A loss: 0.632669, acc: 0.671875]\n",
      "728: [D loss: 0.726622, acc: 0.515625]  [A loss: 1.068211, acc: 0.035156]\n",
      "729: [D loss: 0.670655, acc: 0.591797]  [A loss: 0.726163, acc: 0.445312]\n",
      "730: [D loss: 0.702516, acc: 0.527344]  [A loss: 0.949968, acc: 0.117188]\n",
      "731: [D loss: 0.685173, acc: 0.552734]  [A loss: 0.835114, acc: 0.246094]\n",
      "732: [D loss: 0.694082, acc: 0.544922]  [A loss: 0.878327, acc: 0.207031]\n",
      "733: [D loss: 0.679107, acc: 0.556641]  [A loss: 0.845880, acc: 0.269531]\n",
      "734: [D loss: 0.687619, acc: 0.541016]  [A loss: 0.933841, acc: 0.089844]\n",
      "735: [D loss: 0.671529, acc: 0.582031]  [A loss: 0.830743, acc: 0.250000]\n",
      "736: [D loss: 0.696399, acc: 0.546875]  [A loss: 0.981906, acc: 0.121094]\n",
      "737: [D loss: 0.667739, acc: 0.603516]  [A loss: 0.778533, acc: 0.343750]\n",
      "738: [D loss: 0.691330, acc: 0.546875]  [A loss: 0.982548, acc: 0.074219]\n",
      "739: [D loss: 0.679408, acc: 0.566406]  [A loss: 0.834187, acc: 0.257812]\n",
      "740: [D loss: 0.696541, acc: 0.523438]  [A loss: 0.981530, acc: 0.093750]\n",
      "741: [D loss: 0.677536, acc: 0.560547]  [A loss: 0.761247, acc: 0.375000]\n",
      "742: [D loss: 0.703405, acc: 0.525391]  [A loss: 1.009644, acc: 0.101562]\n",
      "743: [D loss: 0.680737, acc: 0.570312]  [A loss: 0.749407, acc: 0.363281]\n",
      "744: [D loss: 0.696451, acc: 0.527344]  [A loss: 1.008840, acc: 0.066406]\n",
      "745: [D loss: 0.671074, acc: 0.580078]  [A loss: 0.781460, acc: 0.339844]\n",
      "746: [D loss: 0.706718, acc: 0.546875]  [A loss: 1.022933, acc: 0.070312]\n",
      "747: [D loss: 0.680899, acc: 0.548828]  [A loss: 0.825900, acc: 0.253906]\n",
      "748: [D loss: 0.669212, acc: 0.568359]  [A loss: 0.912799, acc: 0.136719]\n",
      "749: [D loss: 0.683682, acc: 0.570312]  [A loss: 0.961169, acc: 0.105469]\n",
      "750: [D loss: 0.669194, acc: 0.599609]  [A loss: 0.829268, acc: 0.320312]\n",
      "751: [D loss: 0.700960, acc: 0.531250]  [A loss: 1.110516, acc: 0.035156]\n",
      "752: [D loss: 0.687502, acc: 0.550781]  [A loss: 0.714122, acc: 0.503906]\n",
      "753: [D loss: 0.714260, acc: 0.537109]  [A loss: 1.123011, acc: 0.039062]\n",
      "754: [D loss: 0.682797, acc: 0.541016]  [A loss: 0.718430, acc: 0.492188]\n",
      "755: [D loss: 0.685736, acc: 0.562500]  [A loss: 1.013199, acc: 0.058594]\n",
      "756: [D loss: 0.674785, acc: 0.572266]  [A loss: 0.757991, acc: 0.394531]\n",
      "757: [D loss: 0.705244, acc: 0.535156]  [A loss: 0.967231, acc: 0.097656]\n",
      "758: [D loss: 0.675525, acc: 0.582031]  [A loss: 0.817966, acc: 0.269531]\n",
      "759: [D loss: 0.674268, acc: 0.568359]  [A loss: 0.977538, acc: 0.093750]\n",
      "760: [D loss: 0.691180, acc: 0.531250]  [A loss: 0.823993, acc: 0.289062]\n",
      "761: [D loss: 0.688804, acc: 0.533203]  [A loss: 1.002983, acc: 0.058594]\n",
      "762: [D loss: 0.664578, acc: 0.619141]  [A loss: 0.799155, acc: 0.324219]\n",
      "763: [D loss: 0.684796, acc: 0.568359]  [A loss: 0.977922, acc: 0.085938]\n",
      "764: [D loss: 0.683296, acc: 0.552734]  [A loss: 0.859143, acc: 0.203125]\n",
      "765: [D loss: 0.684889, acc: 0.554688]  [A loss: 0.942573, acc: 0.121094]\n",
      "766: [D loss: 0.673698, acc: 0.572266]  [A loss: 0.773011, acc: 0.347656]\n",
      "767: [D loss: 0.696678, acc: 0.560547]  [A loss: 1.063169, acc: 0.054688]\n",
      "768: [D loss: 0.677097, acc: 0.564453]  [A loss: 0.786726, acc: 0.339844]\n",
      "769: [D loss: 0.713161, acc: 0.517578]  [A loss: 1.017080, acc: 0.062500]\n",
      "770: [D loss: 0.675430, acc: 0.583984]  [A loss: 0.797281, acc: 0.316406]\n",
      "771: [D loss: 0.699227, acc: 0.548828]  [A loss: 1.144958, acc: 0.035156]\n",
      "772: [D loss: 0.683955, acc: 0.550781]  [A loss: 0.663792, acc: 0.597656]\n",
      "773: [D loss: 0.725762, acc: 0.513672]  [A loss: 1.131139, acc: 0.054688]\n",
      "774: [D loss: 0.673627, acc: 0.585938]  [A loss: 0.685039, acc: 0.558594]\n",
      "775: [D loss: 0.719174, acc: 0.515625]  [A loss: 1.023846, acc: 0.082031]\n",
      "776: [D loss: 0.681449, acc: 0.558594]  [A loss: 0.754164, acc: 0.410156]\n",
      "777: [D loss: 0.701151, acc: 0.527344]  [A loss: 1.018394, acc: 0.070312]\n",
      "778: [D loss: 0.677702, acc: 0.574219]  [A loss: 0.765209, acc: 0.394531]\n",
      "779: [D loss: 0.685682, acc: 0.544922]  [A loss: 1.001055, acc: 0.074219]\n",
      "780: [D loss: 0.668409, acc: 0.580078]  [A loss: 0.761943, acc: 0.394531]\n",
      "781: [D loss: 0.697013, acc: 0.539062]  [A loss: 1.026093, acc: 0.050781]\n",
      "782: [D loss: 0.668153, acc: 0.603516]  [A loss: 0.733247, acc: 0.464844]\n",
      "783: [D loss: 0.703453, acc: 0.535156]  [A loss: 0.958366, acc: 0.101562]\n",
      "784: [D loss: 0.683645, acc: 0.550781]  [A loss: 0.796902, acc: 0.316406]\n",
      "785: [D loss: 0.685727, acc: 0.578125]  [A loss: 0.936583, acc: 0.156250]\n",
      "786: [D loss: 0.687019, acc: 0.552734]  [A loss: 0.911208, acc: 0.183594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787: [D loss: 0.674306, acc: 0.572266]  [A loss: 0.817871, acc: 0.250000]\n",
      "788: [D loss: 0.688847, acc: 0.554688]  [A loss: 1.017486, acc: 0.050781]\n",
      "789: [D loss: 0.675714, acc: 0.544922]  [A loss: 0.751211, acc: 0.363281]\n",
      "790: [D loss: 0.691755, acc: 0.546875]  [A loss: 1.088260, acc: 0.046875]\n",
      "791: [D loss: 0.679352, acc: 0.566406]  [A loss: 0.699444, acc: 0.500000]\n",
      "792: [D loss: 0.718333, acc: 0.509766]  [A loss: 1.118343, acc: 0.035156]\n",
      "793: [D loss: 0.671796, acc: 0.585938]  [A loss: 0.747328, acc: 0.441406]\n",
      "794: [D loss: 0.693291, acc: 0.564453]  [A loss: 0.951890, acc: 0.093750]\n",
      "795: [D loss: 0.690979, acc: 0.550781]  [A loss: 0.803024, acc: 0.312500]\n",
      "796: [D loss: 0.700542, acc: 0.542969]  [A loss: 0.953758, acc: 0.101562]\n",
      "797: [D loss: 0.695354, acc: 0.539062]  [A loss: 0.776056, acc: 0.355469]\n",
      "798: [D loss: 0.700055, acc: 0.560547]  [A loss: 1.095256, acc: 0.062500]\n",
      "799: [D loss: 0.692566, acc: 0.554688]  [A loss: 0.718816, acc: 0.460938]\n",
      "800: [D loss: 0.696261, acc: 0.556641]  [A loss: 0.990740, acc: 0.089844]\n",
      "801: [D loss: 0.682501, acc: 0.550781]  [A loss: 0.784834, acc: 0.351562]\n",
      "802: [D loss: 0.687260, acc: 0.544922]  [A loss: 0.919953, acc: 0.140625]\n",
      "803: [D loss: 0.683376, acc: 0.537109]  [A loss: 0.785142, acc: 0.347656]\n",
      "804: [D loss: 0.676372, acc: 0.570312]  [A loss: 0.932590, acc: 0.148438]\n",
      "805: [D loss: 0.668118, acc: 0.568359]  [A loss: 0.812759, acc: 0.343750]\n",
      "806: [D loss: 0.670014, acc: 0.593750]  [A loss: 0.935002, acc: 0.140625]\n",
      "807: [D loss: 0.678614, acc: 0.562500]  [A loss: 0.826980, acc: 0.296875]\n",
      "808: [D loss: 0.689972, acc: 0.539062]  [A loss: 1.046360, acc: 0.070312]\n",
      "809: [D loss: 0.659471, acc: 0.607422]  [A loss: 0.794594, acc: 0.328125]\n",
      "810: [D loss: 0.682012, acc: 0.566406]  [A loss: 1.043991, acc: 0.082031]\n",
      "811: [D loss: 0.704979, acc: 0.517578]  [A loss: 0.759602, acc: 0.402344]\n",
      "812: [D loss: 0.707989, acc: 0.542969]  [A loss: 1.046400, acc: 0.066406]\n",
      "813: [D loss: 0.665705, acc: 0.576172]  [A loss: 0.753631, acc: 0.398438]\n",
      "814: [D loss: 0.701296, acc: 0.523438]  [A loss: 1.093160, acc: 0.054688]\n",
      "815: [D loss: 0.693192, acc: 0.542969]  [A loss: 0.731335, acc: 0.484375]\n",
      "816: [D loss: 0.695527, acc: 0.582031]  [A loss: 1.074724, acc: 0.054688]\n",
      "817: [D loss: 0.680382, acc: 0.570312]  [A loss: 0.697626, acc: 0.546875]\n",
      "818: [D loss: 0.701027, acc: 0.525391]  [A loss: 0.960440, acc: 0.101562]\n",
      "819: [D loss: 0.692863, acc: 0.548828]  [A loss: 0.800362, acc: 0.292969]\n",
      "820: [D loss: 0.678184, acc: 0.568359]  [A loss: 0.977233, acc: 0.089844]\n",
      "821: [D loss: 0.674983, acc: 0.576172]  [A loss: 0.801248, acc: 0.324219]\n",
      "822: [D loss: 0.687801, acc: 0.560547]  [A loss: 0.945322, acc: 0.144531]\n",
      "823: [D loss: 0.682236, acc: 0.574219]  [A loss: 0.754870, acc: 0.417969]\n",
      "824: [D loss: 0.699247, acc: 0.560547]  [A loss: 1.043254, acc: 0.113281]\n",
      "825: [D loss: 0.686610, acc: 0.554688]  [A loss: 0.756721, acc: 0.386719]\n",
      "826: [D loss: 0.678680, acc: 0.576172]  [A loss: 1.002714, acc: 0.093750]\n",
      "827: [D loss: 0.667015, acc: 0.582031]  [A loss: 0.712091, acc: 0.480469]\n",
      "828: [D loss: 0.694195, acc: 0.535156]  [A loss: 1.060217, acc: 0.058594]\n",
      "829: [D loss: 0.672807, acc: 0.572266]  [A loss: 0.747913, acc: 0.429688]\n",
      "830: [D loss: 0.709093, acc: 0.537109]  [A loss: 0.935986, acc: 0.117188]\n",
      "831: [D loss: 0.676836, acc: 0.560547]  [A loss: 0.915563, acc: 0.128906]\n",
      "832: [D loss: 0.678139, acc: 0.582031]  [A loss: 0.910217, acc: 0.164062]\n",
      "833: [D loss: 0.682969, acc: 0.564453]  [A loss: 0.832410, acc: 0.218750]\n",
      "834: [D loss: 0.681257, acc: 0.568359]  [A loss: 0.983680, acc: 0.109375]\n",
      "835: [D loss: 0.690517, acc: 0.527344]  [A loss: 0.821418, acc: 0.281250]\n",
      "836: [D loss: 0.689835, acc: 0.564453]  [A loss: 0.971332, acc: 0.121094]\n",
      "837: [D loss: 0.676737, acc: 0.578125]  [A loss: 0.804948, acc: 0.320312]\n",
      "838: [D loss: 0.695888, acc: 0.537109]  [A loss: 0.998080, acc: 0.105469]\n",
      "839: [D loss: 0.679286, acc: 0.552734]  [A loss: 0.759622, acc: 0.421875]\n",
      "840: [D loss: 0.685321, acc: 0.574219]  [A loss: 0.998674, acc: 0.082031]\n",
      "841: [D loss: 0.676065, acc: 0.562500]  [A loss: 0.817586, acc: 0.304688]\n",
      "842: [D loss: 0.700155, acc: 0.550781]  [A loss: 0.939295, acc: 0.125000]\n",
      "843: [D loss: 0.676146, acc: 0.546875]  [A loss: 0.840813, acc: 0.234375]\n",
      "844: [D loss: 0.682720, acc: 0.566406]  [A loss: 0.938607, acc: 0.144531]\n",
      "845: [D loss: 0.674021, acc: 0.558594]  [A loss: 0.842194, acc: 0.253906]\n",
      "846: [D loss: 0.690373, acc: 0.558594]  [A loss: 1.006710, acc: 0.093750]\n",
      "847: [D loss: 0.705207, acc: 0.492188]  [A loss: 0.850549, acc: 0.199219]\n",
      "848: [D loss: 0.670247, acc: 0.568359]  [A loss: 0.965814, acc: 0.113281]\n",
      "849: [D loss: 0.672911, acc: 0.578125]  [A loss: 0.816934, acc: 0.277344]\n",
      "850: [D loss: 0.692299, acc: 0.570312]  [A loss: 1.064407, acc: 0.058594]\n",
      "851: [D loss: 0.684276, acc: 0.554688]  [A loss: 0.821591, acc: 0.265625]\n",
      "852: [D loss: 0.680799, acc: 0.576172]  [A loss: 0.978007, acc: 0.105469]\n",
      "853: [D loss: 0.682344, acc: 0.570312]  [A loss: 0.857827, acc: 0.250000]\n",
      "854: [D loss: 0.685817, acc: 0.541016]  [A loss: 1.026559, acc: 0.082031]\n",
      "855: [D loss: 0.675917, acc: 0.572266]  [A loss: 0.727021, acc: 0.457031]\n",
      "856: [D loss: 0.701121, acc: 0.529297]  [A loss: 1.184248, acc: 0.031250]\n",
      "857: [D loss: 0.686776, acc: 0.554688]  [A loss: 0.626591, acc: 0.632812]\n",
      "858: [D loss: 0.744833, acc: 0.529297]  [A loss: 1.147557, acc: 0.039062]\n",
      "859: [D loss: 0.690946, acc: 0.554688]  [A loss: 0.722887, acc: 0.425781]\n",
      "860: [D loss: 0.688533, acc: 0.541016]  [A loss: 0.959100, acc: 0.144531]\n",
      "861: [D loss: 0.693713, acc: 0.554688]  [A loss: 0.823497, acc: 0.296875]\n",
      "862: [D loss: 0.695680, acc: 0.535156]  [A loss: 0.917583, acc: 0.191406]\n",
      "863: [D loss: 0.685999, acc: 0.578125]  [A loss: 0.782560, acc: 0.339844]\n",
      "864: [D loss: 0.685274, acc: 0.558594]  [A loss: 0.945041, acc: 0.125000]\n",
      "865: [D loss: 0.665434, acc: 0.595703]  [A loss: 0.813986, acc: 0.285156]\n",
      "866: [D loss: 0.689742, acc: 0.541016]  [A loss: 0.943347, acc: 0.140625]\n",
      "867: [D loss: 0.676113, acc: 0.591797]  [A loss: 0.843331, acc: 0.277344]\n",
      "868: [D loss: 0.698496, acc: 0.546875]  [A loss: 1.055883, acc: 0.078125]\n",
      "869: [D loss: 0.698228, acc: 0.541016]  [A loss: 0.764492, acc: 0.394531]\n",
      "870: [D loss: 0.684062, acc: 0.572266]  [A loss: 0.985216, acc: 0.089844]\n",
      "871: [D loss: 0.682805, acc: 0.566406]  [A loss: 0.852132, acc: 0.234375]\n",
      "872: [D loss: 0.684610, acc: 0.564453]  [A loss: 0.945323, acc: 0.121094]\n",
      "873: [D loss: 0.690049, acc: 0.572266]  [A loss: 0.945838, acc: 0.113281]\n",
      "874: [D loss: 0.681120, acc: 0.558594]  [A loss: 0.928383, acc: 0.148438]\n",
      "875: [D loss: 0.678451, acc: 0.603516]  [A loss: 0.838483, acc: 0.238281]\n",
      "876: [D loss: 0.691244, acc: 0.546875]  [A loss: 1.088081, acc: 0.066406]\n",
      "877: [D loss: 0.693928, acc: 0.531250]  [A loss: 0.661985, acc: 0.597656]\n",
      "878: [D loss: 0.710935, acc: 0.539062]  [A loss: 1.144307, acc: 0.039062]\n",
      "879: [D loss: 0.693932, acc: 0.544922]  [A loss: 0.677869, acc: 0.562500]\n",
      "880: [D loss: 0.711788, acc: 0.521484]  [A loss: 1.054950, acc: 0.066406]\n",
      "881: [D loss: 0.676690, acc: 0.568359]  [A loss: 0.675869, acc: 0.546875]\n",
      "882: [D loss: 0.721774, acc: 0.507812]  [A loss: 0.980681, acc: 0.105469]\n",
      "883: [D loss: 0.675759, acc: 0.578125]  [A loss: 0.717600, acc: 0.472656]\n",
      "884: [D loss: 0.708528, acc: 0.537109]  [A loss: 0.977276, acc: 0.101562]\n",
      "885: [D loss: 0.671196, acc: 0.611328]  [A loss: 0.765615, acc: 0.382812]\n",
      "886: [D loss: 0.698026, acc: 0.521484]  [A loss: 0.928135, acc: 0.125000]\n",
      "887: [D loss: 0.685978, acc: 0.539062]  [A loss: 0.774064, acc: 0.382812]\n",
      "888: [D loss: 0.689408, acc: 0.548828]  [A loss: 0.955864, acc: 0.125000]\n",
      "889: [D loss: 0.684670, acc: 0.570312]  [A loss: 0.831326, acc: 0.257812]\n",
      "890: [D loss: 0.684447, acc: 0.570312]  [A loss: 0.894278, acc: 0.160156]\n",
      "891: [D loss: 0.688137, acc: 0.554688]  [A loss: 0.892855, acc: 0.171875]\n",
      "892: [D loss: 0.693452, acc: 0.544922]  [A loss: 0.881856, acc: 0.171875]\n",
      "893: [D loss: 0.678679, acc: 0.576172]  [A loss: 0.855379, acc: 0.242188]\n",
      "894: [D loss: 0.695090, acc: 0.554688]  [A loss: 0.886429, acc: 0.171875]\n",
      "895: [D loss: 0.662700, acc: 0.583984]  [A loss: 0.911687, acc: 0.199219]\n",
      "896: [D loss: 0.704885, acc: 0.509766]  [A loss: 0.878692, acc: 0.156250]\n",
      "897: [D loss: 0.681736, acc: 0.558594]  [A loss: 0.995757, acc: 0.089844]\n",
      "898: [D loss: 0.679636, acc: 0.566406]  [A loss: 0.812718, acc: 0.316406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899: [D loss: 0.674580, acc: 0.578125]  [A loss: 0.997738, acc: 0.109375]\n",
      "900: [D loss: 0.674187, acc: 0.572266]  [A loss: 0.726540, acc: 0.457031]\n",
      "901: [D loss: 0.698216, acc: 0.529297]  [A loss: 1.202807, acc: 0.031250]\n",
      "902: [D loss: 0.701540, acc: 0.552734]  [A loss: 0.652590, acc: 0.601562]\n",
      "903: [D loss: 0.742270, acc: 0.509766]  [A loss: 1.069615, acc: 0.066406]\n",
      "904: [D loss: 0.665969, acc: 0.597656]  [A loss: 0.748553, acc: 0.386719]\n",
      "905: [D loss: 0.699185, acc: 0.531250]  [A loss: 0.928162, acc: 0.132812]\n",
      "906: [D loss: 0.681982, acc: 0.558594]  [A loss: 0.833494, acc: 0.281250]\n",
      "907: [D loss: 0.686236, acc: 0.558594]  [A loss: 0.833634, acc: 0.250000]\n",
      "908: [D loss: 0.699792, acc: 0.525391]  [A loss: 0.902892, acc: 0.164062]\n",
      "909: [D loss: 0.685425, acc: 0.554688]  [A loss: 0.884381, acc: 0.199219]\n",
      "910: [D loss: 0.693701, acc: 0.554688]  [A loss: 0.922529, acc: 0.171875]\n",
      "911: [D loss: 0.680678, acc: 0.542969]  [A loss: 0.874934, acc: 0.222656]\n",
      "912: [D loss: 0.690836, acc: 0.556641]  [A loss: 0.852940, acc: 0.242188]\n",
      "913: [D loss: 0.678726, acc: 0.552734]  [A loss: 1.003641, acc: 0.070312]\n",
      "914: [D loss: 0.675183, acc: 0.562500]  [A loss: 0.799312, acc: 0.308594]\n",
      "915: [D loss: 0.691708, acc: 0.568359]  [A loss: 0.995981, acc: 0.101562]\n",
      "916: [D loss: 0.674851, acc: 0.564453]  [A loss: 0.762997, acc: 0.378906]\n",
      "917: [D loss: 0.692060, acc: 0.556641]  [A loss: 1.103612, acc: 0.054688]\n",
      "918: [D loss: 0.682514, acc: 0.566406]  [A loss: 0.745584, acc: 0.460938]\n",
      "919: [D loss: 0.726224, acc: 0.519531]  [A loss: 1.112557, acc: 0.054688]\n",
      "920: [D loss: 0.682718, acc: 0.574219]  [A loss: 0.671475, acc: 0.550781]\n",
      "921: [D loss: 0.713672, acc: 0.533203]  [A loss: 1.074458, acc: 0.035156]\n",
      "922: [D loss: 0.667841, acc: 0.593750]  [A loss: 0.727073, acc: 0.472656]\n",
      "923: [D loss: 0.708048, acc: 0.535156]  [A loss: 0.992858, acc: 0.097656]\n",
      "924: [D loss: 0.684679, acc: 0.568359]  [A loss: 0.806739, acc: 0.335938]\n",
      "925: [D loss: 0.694551, acc: 0.556641]  [A loss: 1.024706, acc: 0.078125]\n",
      "926: [D loss: 0.669183, acc: 0.587891]  [A loss: 0.812766, acc: 0.308594]\n",
      "927: [D loss: 0.684264, acc: 0.548828]  [A loss: 0.922168, acc: 0.160156]\n",
      "928: [D loss: 0.671787, acc: 0.582031]  [A loss: 0.830920, acc: 0.296875]\n",
      "929: [D loss: 0.672657, acc: 0.589844]  [A loss: 0.950758, acc: 0.128906]\n",
      "930: [D loss: 0.685605, acc: 0.560547]  [A loss: 0.838360, acc: 0.265625]\n",
      "931: [D loss: 0.714306, acc: 0.500000]  [A loss: 1.056606, acc: 0.078125]\n",
      "932: [D loss: 0.676232, acc: 0.582031]  [A loss: 0.726414, acc: 0.460938]\n",
      "933: [D loss: 0.705189, acc: 0.541016]  [A loss: 1.036604, acc: 0.085938]\n",
      "934: [D loss: 0.694882, acc: 0.550781]  [A loss: 0.714416, acc: 0.503906]\n",
      "935: [D loss: 0.695312, acc: 0.537109]  [A loss: 1.064419, acc: 0.070312]\n",
      "936: [D loss: 0.706730, acc: 0.505859]  [A loss: 0.725914, acc: 0.460938]\n",
      "937: [D loss: 0.707743, acc: 0.519531]  [A loss: 1.004080, acc: 0.078125]\n",
      "938: [D loss: 0.679203, acc: 0.583984]  [A loss: 0.739624, acc: 0.445312]\n",
      "939: [D loss: 0.694488, acc: 0.531250]  [A loss: 0.998108, acc: 0.066406]\n",
      "940: [D loss: 0.679406, acc: 0.562500]  [A loss: 0.834152, acc: 0.304688]\n",
      "941: [D loss: 0.677802, acc: 0.591797]  [A loss: 0.891467, acc: 0.203125]\n",
      "942: [D loss: 0.678704, acc: 0.550781]  [A loss: 0.821553, acc: 0.296875]\n",
      "943: [D loss: 0.689436, acc: 0.556641]  [A loss: 0.924293, acc: 0.187500]\n",
      "944: [D loss: 0.695233, acc: 0.535156]  [A loss: 0.822295, acc: 0.230469]\n",
      "945: [D loss: 0.680907, acc: 0.576172]  [A loss: 0.983449, acc: 0.105469]\n",
      "946: [D loss: 0.665541, acc: 0.607422]  [A loss: 0.767098, acc: 0.386719]\n",
      "947: [D loss: 0.692314, acc: 0.550781]  [A loss: 1.050041, acc: 0.089844]\n",
      "948: [D loss: 0.670900, acc: 0.583984]  [A loss: 0.751178, acc: 0.429688]\n",
      "949: [D loss: 0.709889, acc: 0.535156]  [A loss: 1.088559, acc: 0.062500]\n",
      "950: [D loss: 0.681562, acc: 0.566406]  [A loss: 0.686582, acc: 0.535156]\n",
      "951: [D loss: 0.741989, acc: 0.509766]  [A loss: 1.079795, acc: 0.054688]\n",
      "952: [D loss: 0.682224, acc: 0.576172]  [A loss: 0.752644, acc: 0.398438]\n",
      "953: [D loss: 0.741063, acc: 0.496094]  [A loss: 0.911756, acc: 0.156250]\n",
      "954: [D loss: 0.681021, acc: 0.580078]  [A loss: 0.838581, acc: 0.222656]\n",
      "955: [D loss: 0.706268, acc: 0.548828]  [A loss: 0.984288, acc: 0.097656]\n",
      "956: [D loss: 0.686632, acc: 0.531250]  [A loss: 0.802071, acc: 0.300781]\n",
      "957: [D loss: 0.692869, acc: 0.531250]  [A loss: 0.957634, acc: 0.105469]\n",
      "958: [D loss: 0.675591, acc: 0.572266]  [A loss: 0.808720, acc: 0.324219]\n",
      "959: [D loss: 0.705300, acc: 0.509766]  [A loss: 0.947236, acc: 0.132812]\n",
      "960: [D loss: 0.678925, acc: 0.572266]  [A loss: 0.803117, acc: 0.304688]\n",
      "961: [D loss: 0.693439, acc: 0.544922]  [A loss: 0.985343, acc: 0.078125]\n",
      "962: [D loss: 0.681022, acc: 0.548828]  [A loss: 0.812974, acc: 0.328125]\n",
      "963: [D loss: 0.687293, acc: 0.554688]  [A loss: 0.937199, acc: 0.128906]\n",
      "964: [D loss: 0.680193, acc: 0.591797]  [A loss: 0.808994, acc: 0.308594]\n",
      "965: [D loss: 0.688702, acc: 0.519531]  [A loss: 0.970670, acc: 0.097656]\n",
      "966: [D loss: 0.704644, acc: 0.511719]  [A loss: 0.812584, acc: 0.269531]\n",
      "967: [D loss: 0.685003, acc: 0.564453]  [A loss: 0.944795, acc: 0.171875]\n",
      "968: [D loss: 0.689653, acc: 0.558594]  [A loss: 0.763947, acc: 0.363281]\n",
      "969: [D loss: 0.676837, acc: 0.587891]  [A loss: 0.944426, acc: 0.117188]\n",
      "970: [D loss: 0.669618, acc: 0.585938]  [A loss: 0.830271, acc: 0.316406]\n",
      "971: [D loss: 0.712827, acc: 0.525391]  [A loss: 1.087541, acc: 0.046875]\n",
      "972: [D loss: 0.679308, acc: 0.580078]  [A loss: 0.720490, acc: 0.480469]\n",
      "973: [D loss: 0.716731, acc: 0.505859]  [A loss: 1.125791, acc: 0.042969]\n",
      "974: [D loss: 0.680976, acc: 0.564453]  [A loss: 0.739645, acc: 0.433594]\n",
      "975: [D loss: 0.701890, acc: 0.537109]  [A loss: 1.004226, acc: 0.074219]\n",
      "976: [D loss: 0.693128, acc: 0.544922]  [A loss: 0.847744, acc: 0.253906]\n",
      "977: [D loss: 0.685753, acc: 0.537109]  [A loss: 0.910235, acc: 0.179688]\n",
      "978: [D loss: 0.674172, acc: 0.576172]  [A loss: 0.791549, acc: 0.324219]\n",
      "979: [D loss: 0.688086, acc: 0.560547]  [A loss: 0.967977, acc: 0.117188]\n",
      "980: [D loss: 0.682291, acc: 0.544922]  [A loss: 0.771948, acc: 0.367188]\n",
      "981: [D loss: 0.678520, acc: 0.550781]  [A loss: 0.987493, acc: 0.089844]\n",
      "982: [D loss: 0.667119, acc: 0.591797]  [A loss: 0.763621, acc: 0.390625]\n",
      "983: [D loss: 0.696948, acc: 0.570312]  [A loss: 1.053806, acc: 0.066406]\n",
      "984: [D loss: 0.684978, acc: 0.574219]  [A loss: 0.763844, acc: 0.375000]\n",
      "985: [D loss: 0.688218, acc: 0.531250]  [A loss: 1.074596, acc: 0.058594]\n",
      "986: [D loss: 0.692759, acc: 0.525391]  [A loss: 0.727654, acc: 0.449219]\n",
      "987: [D loss: 0.681467, acc: 0.552734]  [A loss: 1.013363, acc: 0.074219]\n",
      "988: [D loss: 0.671781, acc: 0.587891]  [A loss: 0.785108, acc: 0.308594]\n",
      "989: [D loss: 0.700021, acc: 0.537109]  [A loss: 0.986535, acc: 0.101562]\n",
      "990: [D loss: 0.677392, acc: 0.564453]  [A loss: 0.753958, acc: 0.335938]\n",
      "991: [D loss: 0.684091, acc: 0.544922]  [A loss: 0.961601, acc: 0.140625]\n",
      "992: [D loss: 0.672882, acc: 0.595703]  [A loss: 0.777425, acc: 0.367188]\n",
      "993: [D loss: 0.712628, acc: 0.525391]  [A loss: 1.063928, acc: 0.074219]\n",
      "994: [D loss: 0.675101, acc: 0.560547]  [A loss: 0.717621, acc: 0.460938]\n",
      "995: [D loss: 0.691283, acc: 0.539062]  [A loss: 1.070500, acc: 0.058594]\n",
      "996: [D loss: 0.679984, acc: 0.576172]  [A loss: 0.712538, acc: 0.515625]\n",
      "997: [D loss: 0.710966, acc: 0.542969]  [A loss: 1.039237, acc: 0.093750]\n",
      "998: [D loss: 0.668027, acc: 0.597656]  [A loss: 0.684660, acc: 0.562500]\n",
      "999: [D loss: 0.716568, acc: 0.498047]  [A loss: 1.043258, acc: 0.066406]\n",
      "Elapsed: 8.5093967517217 min \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train for 1000 epochs and save at 100 intervals\n",
    "# print G images\n",
    "# save images\n",
    "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=100)\n",
    "mnist_dcgan.plot_images(fake=True)\n",
    "mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.668090, acc: 0.580078]  [A loss: 0.728395, acc: 0.464844]\n",
      "1: [D loss: 0.694327, acc: 0.558594]  [A loss: 0.957397, acc: 0.109375]\n",
      "2: [D loss: 0.689644, acc: 0.560547]  [A loss: 0.776072, acc: 0.359375]\n",
      "3: [D loss: 0.687651, acc: 0.527344]  [A loss: 1.032217, acc: 0.082031]\n",
      "4: [D loss: 0.676154, acc: 0.574219]  [A loss: 0.760045, acc: 0.410156]\n",
      "5: [D loss: 0.707637, acc: 0.535156]  [A loss: 0.953330, acc: 0.121094]\n",
      "6: [D loss: 0.660298, acc: 0.621094]  [A loss: 0.819997, acc: 0.292969]\n",
      "7: [D loss: 0.678575, acc: 0.548828]  [A loss: 0.939528, acc: 0.156250]\n",
      "8: [D loss: 0.676732, acc: 0.595703]  [A loss: 0.839849, acc: 0.203125]\n",
      "9: [D loss: 0.688143, acc: 0.537109]  [A loss: 0.904341, acc: 0.179688]\n",
      "10: [D loss: 0.692000, acc: 0.556641]  [A loss: 0.944440, acc: 0.148438]\n",
      "11: [D loss: 0.682532, acc: 0.564453]  [A loss: 0.856599, acc: 0.242188]\n",
      "12: [D loss: 0.702109, acc: 0.529297]  [A loss: 0.970112, acc: 0.089844]\n",
      "13: [D loss: 0.668507, acc: 0.583984]  [A loss: 0.886611, acc: 0.207031]\n",
      "14: [D loss: 0.690541, acc: 0.554688]  [A loss: 0.917398, acc: 0.132812]\n",
      "15: [D loss: 0.674477, acc: 0.564453]  [A loss: 0.872382, acc: 0.222656]\n",
      "16: [D loss: 0.697956, acc: 0.562500]  [A loss: 1.006964, acc: 0.109375]\n",
      "17: [D loss: 0.685481, acc: 0.578125]  [A loss: 0.841176, acc: 0.265625]\n",
      "18: [D loss: 0.696766, acc: 0.558594]  [A loss: 1.002920, acc: 0.089844]\n",
      "19: [D loss: 0.697656, acc: 0.515625]  [A loss: 0.840778, acc: 0.289062]\n",
      "20: [D loss: 0.692052, acc: 0.539062]  [A loss: 1.117471, acc: 0.066406]\n",
      "21: [D loss: 0.674209, acc: 0.582031]  [A loss: 0.666803, acc: 0.597656]\n",
      "22: [D loss: 0.721334, acc: 0.513672]  [A loss: 1.133430, acc: 0.039062]\n",
      "23: [D loss: 0.688037, acc: 0.566406]  [A loss: 0.696525, acc: 0.519531]\n",
      "24: [D loss: 0.712986, acc: 0.535156]  [A loss: 1.011339, acc: 0.105469]\n",
      "25: [D loss: 0.684193, acc: 0.558594]  [A loss: 0.685569, acc: 0.511719]\n",
      "26: [D loss: 0.706944, acc: 0.537109]  [A loss: 1.006989, acc: 0.082031]\n",
      "27: [D loss: 0.679242, acc: 0.580078]  [A loss: 0.735022, acc: 0.441406]\n",
      "28: [D loss: 0.701347, acc: 0.531250]  [A loss: 0.947471, acc: 0.148438]\n",
      "29: [D loss: 0.688335, acc: 0.562500]  [A loss: 0.743794, acc: 0.421875]\n",
      "30: [D loss: 0.699618, acc: 0.541016]  [A loss: 0.922552, acc: 0.160156]\n",
      "31: [D loss: 0.677298, acc: 0.546875]  [A loss: 0.841457, acc: 0.234375]\n",
      "32: [D loss: 0.678902, acc: 0.591797]  [A loss: 0.831881, acc: 0.253906]\n",
      "33: [D loss: 0.685365, acc: 0.570312]  [A loss: 0.880985, acc: 0.199219]\n",
      "34: [D loss: 0.675831, acc: 0.585938]  [A loss: 0.895452, acc: 0.210938]\n",
      "35: [D loss: 0.694820, acc: 0.537109]  [A loss: 0.857362, acc: 0.210938]\n",
      "36: [D loss: 0.713462, acc: 0.523438]  [A loss: 0.895714, acc: 0.187500]\n",
      "37: [D loss: 0.669366, acc: 0.601562]  [A loss: 0.840406, acc: 0.257812]\n",
      "38: [D loss: 0.691720, acc: 0.566406]  [A loss: 0.930519, acc: 0.164062]\n",
      "39: [D loss: 0.677807, acc: 0.541016]  [A loss: 0.734589, acc: 0.453125]\n",
      "40: [D loss: 0.704074, acc: 0.523438]  [A loss: 1.077031, acc: 0.074219]\n",
      "41: [D loss: 0.679342, acc: 0.574219]  [A loss: 0.659181, acc: 0.625000]\n",
      "42: [D loss: 0.718621, acc: 0.513672]  [A loss: 1.025620, acc: 0.066406]\n",
      "43: [D loss: 0.673759, acc: 0.591797]  [A loss: 0.735616, acc: 0.468750]\n",
      "44: [D loss: 0.692693, acc: 0.546875]  [A loss: 1.000460, acc: 0.082031]\n",
      "45: [D loss: 0.701448, acc: 0.535156]  [A loss: 0.739006, acc: 0.492188]\n",
      "46: [D loss: 0.698659, acc: 0.574219]  [A loss: 0.931808, acc: 0.152344]\n",
      "47: [D loss: 0.679827, acc: 0.589844]  [A loss: 0.797587, acc: 0.328125]\n",
      "48: [D loss: 0.702935, acc: 0.523438]  [A loss: 0.925125, acc: 0.140625]\n",
      "49: [D loss: 0.676971, acc: 0.583984]  [A loss: 0.822252, acc: 0.246094]\n",
      "50: [D loss: 0.689792, acc: 0.560547]  [A loss: 0.971318, acc: 0.097656]\n",
      "51: [D loss: 0.671252, acc: 0.582031]  [A loss: 0.801154, acc: 0.292969]\n",
      "52: [D loss: 0.704759, acc: 0.527344]  [A loss: 0.901566, acc: 0.187500]\n",
      "53: [D loss: 0.684264, acc: 0.560547]  [A loss: 0.882929, acc: 0.187500]\n",
      "54: [D loss: 0.683202, acc: 0.548828]  [A loss: 0.929757, acc: 0.156250]\n",
      "55: [D loss: 0.684499, acc: 0.539062]  [A loss: 0.829193, acc: 0.289062]\n",
      "56: [D loss: 0.696008, acc: 0.560547]  [A loss: 1.047289, acc: 0.085938]\n",
      "57: [D loss: 0.659552, acc: 0.611328]  [A loss: 0.767221, acc: 0.378906]\n",
      "58: [D loss: 0.693631, acc: 0.546875]  [A loss: 1.075459, acc: 0.074219]\n",
      "59: [D loss: 0.686086, acc: 0.576172]  [A loss: 0.691246, acc: 0.531250]\n",
      "60: [D loss: 0.730472, acc: 0.517578]  [A loss: 1.206457, acc: 0.015625]\n",
      "61: [D loss: 0.694123, acc: 0.546875]  [A loss: 0.655092, acc: 0.589844]\n",
      "62: [D loss: 0.736807, acc: 0.515625]  [A loss: 1.045213, acc: 0.093750]\n",
      "63: [D loss: 0.692718, acc: 0.548828]  [A loss: 0.741037, acc: 0.449219]\n",
      "64: [D loss: 0.705463, acc: 0.533203]  [A loss: 0.893369, acc: 0.195312]\n",
      "65: [D loss: 0.672768, acc: 0.570312]  [A loss: 0.835941, acc: 0.242188]\n",
      "66: [D loss: 0.684992, acc: 0.544922]  [A loss: 0.859266, acc: 0.230469]\n",
      "67: [D loss: 0.686186, acc: 0.542969]  [A loss: 0.866912, acc: 0.199219]\n",
      "68: [D loss: 0.682530, acc: 0.574219]  [A loss: 0.843462, acc: 0.246094]\n",
      "69: [D loss: 0.688364, acc: 0.527344]  [A loss: 0.870241, acc: 0.226562]\n",
      "70: [D loss: 0.691703, acc: 0.572266]  [A loss: 0.834345, acc: 0.257812]\n",
      "71: [D loss: 0.689354, acc: 0.564453]  [A loss: 0.999727, acc: 0.089844]\n",
      "72: [D loss: 0.682890, acc: 0.568359]  [A loss: 0.790436, acc: 0.371094]\n",
      "73: [D loss: 0.675855, acc: 0.570312]  [A loss: 0.940275, acc: 0.132812]\n",
      "74: [D loss: 0.693950, acc: 0.548828]  [A loss: 0.893458, acc: 0.171875]\n",
      "75: [D loss: 0.685327, acc: 0.537109]  [A loss: 0.897150, acc: 0.160156]\n",
      "76: [D loss: 0.685627, acc: 0.564453]  [A loss: 0.873298, acc: 0.203125]\n",
      "77: [D loss: 0.691945, acc: 0.552734]  [A loss: 0.901127, acc: 0.136719]\n",
      "78: [D loss: 0.676142, acc: 0.560547]  [A loss: 0.890456, acc: 0.207031]\n",
      "79: [D loss: 0.683208, acc: 0.580078]  [A loss: 0.928165, acc: 0.128906]\n",
      "80: [D loss: 0.690441, acc: 0.550781]  [A loss: 0.869927, acc: 0.226562]\n",
      "81: [D loss: 0.685000, acc: 0.548828]  [A loss: 0.917661, acc: 0.144531]\n",
      "82: [D loss: 0.677398, acc: 0.583984]  [A loss: 0.890440, acc: 0.214844]\n",
      "83: [D loss: 0.681476, acc: 0.564453]  [A loss: 0.959116, acc: 0.136719]\n",
      "84: [D loss: 0.665282, acc: 0.583984]  [A loss: 0.988590, acc: 0.128906]\n",
      "85: [D loss: 0.685278, acc: 0.585938]  [A loss: 0.801498, acc: 0.296875]\n",
      "86: [D loss: 0.707698, acc: 0.531250]  [A loss: 1.240674, acc: 0.019531]\n",
      "87: [D loss: 0.686351, acc: 0.552734]  [A loss: 0.561016, acc: 0.796875]\n",
      "88: [D loss: 0.764622, acc: 0.507812]  [A loss: 1.142102, acc: 0.023438]\n",
      "89: [D loss: 0.693772, acc: 0.535156]  [A loss: 0.704815, acc: 0.472656]\n",
      "90: [D loss: 0.710641, acc: 0.527344]  [A loss: 0.932359, acc: 0.125000]\n",
      "91: [D loss: 0.691000, acc: 0.558594]  [A loss: 0.784786, acc: 0.363281]\n",
      "92: [D loss: 0.697916, acc: 0.531250]  [A loss: 0.874530, acc: 0.210938]\n",
      "93: [D loss: 0.682789, acc: 0.568359]  [A loss: 0.827925, acc: 0.289062]\n",
      "94: [D loss: 0.705168, acc: 0.531250]  [A loss: 0.858896, acc: 0.203125]\n",
      "95: [D loss: 0.684320, acc: 0.544922]  [A loss: 0.880543, acc: 0.210938]\n",
      "96: [D loss: 0.685196, acc: 0.554688]  [A loss: 0.865994, acc: 0.179688]\n",
      "97: [D loss: 0.668709, acc: 0.582031]  [A loss: 0.921278, acc: 0.152344]\n",
      "98: [D loss: 0.689895, acc: 0.562500]  [A loss: 0.827955, acc: 0.265625]\n",
      "99: [D loss: 0.687003, acc: 0.556641]  [A loss: 0.906511, acc: 0.132812]\n",
      "100: [D loss: 0.683432, acc: 0.580078]  [A loss: 0.883040, acc: 0.226562]\n",
      "101: [D loss: 0.694114, acc: 0.517578]  [A loss: 0.799288, acc: 0.316406]\n",
      "102: [D loss: 0.673953, acc: 0.572266]  [A loss: 0.930093, acc: 0.171875]\n",
      "103: [D loss: 0.676462, acc: 0.585938]  [A loss: 0.792253, acc: 0.332031]\n",
      "104: [D loss: 0.694859, acc: 0.541016]  [A loss: 1.085126, acc: 0.054688]\n",
      "105: [D loss: 0.678796, acc: 0.568359]  [A loss: 0.726018, acc: 0.472656]\n",
      "106: [D loss: 0.672393, acc: 0.572266]  [A loss: 1.060718, acc: 0.062500]\n",
      "107: [D loss: 0.687308, acc: 0.542969]  [A loss: 0.739141, acc: 0.425781]\n",
      "108: [D loss: 0.707095, acc: 0.519531]  [A loss: 1.058956, acc: 0.078125]\n",
      "109: [D loss: 0.685971, acc: 0.537109]  [A loss: 0.693016, acc: 0.535156]\n",
      "110: [D loss: 0.695559, acc: 0.550781]  [A loss: 1.003366, acc: 0.078125]\n",
      "111: [D loss: 0.684885, acc: 0.578125]  [A loss: 0.775192, acc: 0.371094]\n",
      "112: [D loss: 0.682248, acc: 0.560547]  [A loss: 0.876746, acc: 0.226562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113: [D loss: 0.677893, acc: 0.572266]  [A loss: 0.856291, acc: 0.222656]\n",
      "114: [D loss: 0.688014, acc: 0.556641]  [A loss: 0.935318, acc: 0.132812]\n",
      "115: [D loss: 0.685481, acc: 0.564453]  [A loss: 0.876294, acc: 0.187500]\n",
      "116: [D loss: 0.674746, acc: 0.580078]  [A loss: 0.862453, acc: 0.230469]\n",
      "117: [D loss: 0.711542, acc: 0.505859]  [A loss: 0.885852, acc: 0.175781]\n",
      "118: [D loss: 0.692854, acc: 0.570312]  [A loss: 0.920363, acc: 0.167969]\n",
      "119: [D loss: 0.698082, acc: 0.546875]  [A loss: 0.839432, acc: 0.257812]\n",
      "120: [D loss: 0.704180, acc: 0.515625]  [A loss: 0.958950, acc: 0.105469]\n",
      "121: [D loss: 0.669245, acc: 0.583984]  [A loss: 0.799800, acc: 0.351562]\n",
      "122: [D loss: 0.701054, acc: 0.537109]  [A loss: 0.994728, acc: 0.097656]\n",
      "123: [D loss: 0.677974, acc: 0.591797]  [A loss: 0.759847, acc: 0.406250]\n",
      "124: [D loss: 0.713397, acc: 0.515625]  [A loss: 1.038977, acc: 0.089844]\n",
      "125: [D loss: 0.674996, acc: 0.570312]  [A loss: 0.686765, acc: 0.515625]\n",
      "126: [D loss: 0.717894, acc: 0.525391]  [A loss: 1.057758, acc: 0.066406]\n",
      "127: [D loss: 0.683882, acc: 0.550781]  [A loss: 0.735783, acc: 0.457031]\n",
      "128: [D loss: 0.688047, acc: 0.541016]  [A loss: 0.995704, acc: 0.101562]\n",
      "129: [D loss: 0.684605, acc: 0.564453]  [A loss: 0.776775, acc: 0.347656]\n",
      "130: [D loss: 0.697132, acc: 0.556641]  [A loss: 0.916569, acc: 0.152344]\n",
      "131: [D loss: 0.702218, acc: 0.500000]  [A loss: 0.824382, acc: 0.261719]\n",
      "132: [D loss: 0.677555, acc: 0.566406]  [A loss: 0.874016, acc: 0.199219]\n",
      "133: [D loss: 0.685922, acc: 0.548828]  [A loss: 0.879884, acc: 0.191406]\n",
      "134: [D loss: 0.688771, acc: 0.542969]  [A loss: 0.806513, acc: 0.316406]\n",
      "135: [D loss: 0.687294, acc: 0.570312]  [A loss: 0.928041, acc: 0.171875]\n",
      "136: [D loss: 0.677693, acc: 0.554688]  [A loss: 0.840996, acc: 0.238281]\n",
      "137: [D loss: 0.674962, acc: 0.578125]  [A loss: 0.959240, acc: 0.128906]\n",
      "138: [D loss: 0.692972, acc: 0.537109]  [A loss: 0.850972, acc: 0.250000]\n",
      "139: [D loss: 0.691937, acc: 0.542969]  [A loss: 0.998445, acc: 0.082031]\n",
      "140: [D loss: 0.680768, acc: 0.560547]  [A loss: 0.786198, acc: 0.359375]\n",
      "141: [D loss: 0.707055, acc: 0.498047]  [A loss: 0.961488, acc: 0.128906]\n",
      "142: [D loss: 0.678418, acc: 0.580078]  [A loss: 0.843372, acc: 0.242188]\n",
      "143: [D loss: 0.695913, acc: 0.548828]  [A loss: 0.995764, acc: 0.085938]\n",
      "144: [D loss: 0.671571, acc: 0.580078]  [A loss: 0.726470, acc: 0.449219]\n",
      "145: [D loss: 0.727429, acc: 0.505859]  [A loss: 1.171617, acc: 0.035156]\n",
      "146: [D loss: 0.669259, acc: 0.562500]  [A loss: 0.658271, acc: 0.621094]\n",
      "147: [D loss: 0.718590, acc: 0.509766]  [A loss: 1.115766, acc: 0.070312]\n",
      "148: [D loss: 0.687459, acc: 0.550781]  [A loss: 0.746928, acc: 0.417969]\n",
      "149: [D loss: 0.704313, acc: 0.533203]  [A loss: 0.947742, acc: 0.082031]\n",
      "150: [D loss: 0.681848, acc: 0.556641]  [A loss: 0.780110, acc: 0.335938]\n",
      "151: [D loss: 0.693621, acc: 0.560547]  [A loss: 0.911843, acc: 0.152344]\n",
      "152: [D loss: 0.680461, acc: 0.564453]  [A loss: 0.770042, acc: 0.339844]\n",
      "153: [D loss: 0.683738, acc: 0.560547]  [A loss: 0.949423, acc: 0.101562]\n",
      "154: [D loss: 0.674560, acc: 0.583984]  [A loss: 0.807353, acc: 0.320312]\n",
      "155: [D loss: 0.696227, acc: 0.546875]  [A loss: 0.895274, acc: 0.164062]\n",
      "156: [D loss: 0.673199, acc: 0.554688]  [A loss: 0.896265, acc: 0.175781]\n",
      "157: [D loss: 0.693951, acc: 0.562500]  [A loss: 0.958189, acc: 0.121094]\n",
      "158: [D loss: 0.685589, acc: 0.562500]  [A loss: 0.865942, acc: 0.218750]\n",
      "159: [D loss: 0.692765, acc: 0.556641]  [A loss: 0.970627, acc: 0.128906]\n",
      "160: [D loss: 0.676566, acc: 0.576172]  [A loss: 0.827727, acc: 0.320312]\n",
      "161: [D loss: 0.688032, acc: 0.568359]  [A loss: 1.054860, acc: 0.062500]\n",
      "162: [D loss: 0.673858, acc: 0.578125]  [A loss: 0.754730, acc: 0.406250]\n",
      "163: [D loss: 0.693316, acc: 0.546875]  [A loss: 1.084446, acc: 0.058594]\n",
      "164: [D loss: 0.697545, acc: 0.537109]  [A loss: 0.651755, acc: 0.648438]\n",
      "165: [D loss: 0.729798, acc: 0.505859]  [A loss: 1.092710, acc: 0.085938]\n",
      "166: [D loss: 0.710574, acc: 0.503906]  [A loss: 0.730977, acc: 0.433594]\n",
      "167: [D loss: 0.693003, acc: 0.552734]  [A loss: 0.981899, acc: 0.101562]\n",
      "168: [D loss: 0.690154, acc: 0.539062]  [A loss: 0.700979, acc: 0.507812]\n",
      "169: [D loss: 0.704555, acc: 0.519531]  [A loss: 0.983476, acc: 0.089844]\n",
      "170: [D loss: 0.673289, acc: 0.578125]  [A loss: 0.768165, acc: 0.359375]\n",
      "171: [D loss: 0.686087, acc: 0.574219]  [A loss: 0.971857, acc: 0.101562]\n",
      "172: [D loss: 0.693076, acc: 0.556641]  [A loss: 0.766763, acc: 0.335938]\n",
      "173: [D loss: 0.689901, acc: 0.550781]  [A loss: 0.879714, acc: 0.210938]\n",
      "174: [D loss: 0.686744, acc: 0.546875]  [A loss: 0.889967, acc: 0.160156]\n",
      "175: [D loss: 0.684291, acc: 0.537109]  [A loss: 0.896717, acc: 0.171875]\n",
      "176: [D loss: 0.681992, acc: 0.570312]  [A loss: 0.859589, acc: 0.214844]\n",
      "177: [D loss: 0.699965, acc: 0.539062]  [A loss: 0.999179, acc: 0.082031]\n",
      "178: [D loss: 0.674617, acc: 0.562500]  [A loss: 0.744803, acc: 0.437500]\n",
      "179: [D loss: 0.702375, acc: 0.535156]  [A loss: 1.044106, acc: 0.062500]\n",
      "180: [D loss: 0.673265, acc: 0.587891]  [A loss: 0.786597, acc: 0.347656]\n",
      "181: [D loss: 0.681169, acc: 0.570312]  [A loss: 0.915624, acc: 0.183594]\n",
      "182: [D loss: 0.693441, acc: 0.541016]  [A loss: 0.835047, acc: 0.257812]\n",
      "183: [D loss: 0.710467, acc: 0.505859]  [A loss: 0.929127, acc: 0.136719]\n",
      "184: [D loss: 0.682878, acc: 0.562500]  [A loss: 0.899000, acc: 0.175781]\n",
      "185: [D loss: 0.685819, acc: 0.546875]  [A loss: 0.809608, acc: 0.312500]\n",
      "186: [D loss: 0.701557, acc: 0.531250]  [A loss: 0.895964, acc: 0.187500]\n",
      "187: [D loss: 0.698426, acc: 0.554688]  [A loss: 1.019661, acc: 0.078125]\n",
      "188: [D loss: 0.689341, acc: 0.533203]  [A loss: 0.704218, acc: 0.539062]\n",
      "189: [D loss: 0.726467, acc: 0.505859]  [A loss: 1.066751, acc: 0.050781]\n",
      "190: [D loss: 0.662581, acc: 0.601562]  [A loss: 0.734120, acc: 0.421875]\n",
      "191: [D loss: 0.710502, acc: 0.509766]  [A loss: 0.991670, acc: 0.074219]\n",
      "192: [D loss: 0.684125, acc: 0.558594]  [A loss: 0.765108, acc: 0.382812]\n",
      "193: [D loss: 0.695585, acc: 0.550781]  [A loss: 0.970320, acc: 0.089844]\n",
      "194: [D loss: 0.683830, acc: 0.556641]  [A loss: 0.759902, acc: 0.386719]\n",
      "195: [D loss: 0.695634, acc: 0.552734]  [A loss: 0.985158, acc: 0.113281]\n",
      "196: [D loss: 0.683387, acc: 0.578125]  [A loss: 0.816063, acc: 0.265625]\n",
      "197: [D loss: 0.691131, acc: 0.541016]  [A loss: 0.919593, acc: 0.144531]\n",
      "198: [D loss: 0.674968, acc: 0.556641]  [A loss: 0.796769, acc: 0.312500]\n",
      "199: [D loss: 0.702729, acc: 0.541016]  [A loss: 0.995672, acc: 0.074219]\n",
      "200: [D loss: 0.678204, acc: 0.564453]  [A loss: 0.765226, acc: 0.402344]\n",
      "201: [D loss: 0.707682, acc: 0.517578]  [A loss: 1.038494, acc: 0.089844]\n",
      "202: [D loss: 0.682786, acc: 0.529297]  [A loss: 0.689590, acc: 0.523438]\n",
      "203: [D loss: 0.707830, acc: 0.529297]  [A loss: 1.008722, acc: 0.085938]\n",
      "204: [D loss: 0.694931, acc: 0.539062]  [A loss: 0.799713, acc: 0.308594]\n",
      "205: [D loss: 0.680902, acc: 0.574219]  [A loss: 0.885390, acc: 0.191406]\n",
      "206: [D loss: 0.686289, acc: 0.546875]  [A loss: 0.766790, acc: 0.386719]\n",
      "207: [D loss: 0.710111, acc: 0.505859]  [A loss: 0.979300, acc: 0.105469]\n",
      "208: [D loss: 0.692297, acc: 0.546875]  [A loss: 0.787543, acc: 0.351562]\n",
      "209: [D loss: 0.696030, acc: 0.550781]  [A loss: 0.964785, acc: 0.121094]\n",
      "210: [D loss: 0.672943, acc: 0.576172]  [A loss: 0.830356, acc: 0.273438]\n",
      "211: [D loss: 0.681916, acc: 0.572266]  [A loss: 0.912371, acc: 0.156250]\n",
      "212: [D loss: 0.671435, acc: 0.587891]  [A loss: 0.858268, acc: 0.199219]\n",
      "213: [D loss: 0.688628, acc: 0.556641]  [A loss: 0.984943, acc: 0.085938]\n",
      "214: [D loss: 0.673005, acc: 0.562500]  [A loss: 0.772483, acc: 0.386719]\n",
      "215: [D loss: 0.689088, acc: 0.582031]  [A loss: 1.033300, acc: 0.070312]\n",
      "216: [D loss: 0.663652, acc: 0.603516]  [A loss: 0.677459, acc: 0.558594]\n",
      "217: [D loss: 0.713783, acc: 0.523438]  [A loss: 1.079845, acc: 0.078125]\n",
      "218: [D loss: 0.677427, acc: 0.554688]  [A loss: 0.688694, acc: 0.535156]\n",
      "219: [D loss: 0.707172, acc: 0.531250]  [A loss: 0.997887, acc: 0.085938]\n",
      "220: [D loss: 0.680939, acc: 0.562500]  [A loss: 0.739789, acc: 0.480469]\n",
      "221: [D loss: 0.705378, acc: 0.544922]  [A loss: 1.017628, acc: 0.078125]\n",
      "222: [D loss: 0.682347, acc: 0.550781]  [A loss: 0.720122, acc: 0.472656]\n",
      "223: [D loss: 0.701726, acc: 0.523438]  [A loss: 0.987447, acc: 0.113281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224: [D loss: 0.664574, acc: 0.603516]  [A loss: 0.736762, acc: 0.437500]\n",
      "225: [D loss: 0.692342, acc: 0.544922]  [A loss: 0.953597, acc: 0.113281]\n",
      "226: [D loss: 0.697309, acc: 0.509766]  [A loss: 0.765792, acc: 0.351562]\n",
      "227: [D loss: 0.695272, acc: 0.535156]  [A loss: 0.882610, acc: 0.207031]\n",
      "228: [D loss: 0.680572, acc: 0.580078]  [A loss: 0.803922, acc: 0.281250]\n",
      "229: [D loss: 0.701832, acc: 0.535156]  [A loss: 0.919088, acc: 0.183594]\n",
      "230: [D loss: 0.692172, acc: 0.576172]  [A loss: 0.832827, acc: 0.277344]\n",
      "231: [D loss: 0.695020, acc: 0.564453]  [A loss: 1.027460, acc: 0.078125]\n",
      "232: [D loss: 0.690860, acc: 0.548828]  [A loss: 0.674241, acc: 0.593750]\n",
      "233: [D loss: 0.705528, acc: 0.525391]  [A loss: 1.009466, acc: 0.101562]\n",
      "234: [D loss: 0.667598, acc: 0.591797]  [A loss: 0.762600, acc: 0.398438]\n",
      "235: [D loss: 0.686470, acc: 0.556641]  [A loss: 0.966195, acc: 0.105469]\n",
      "236: [D loss: 0.685773, acc: 0.554688]  [A loss: 0.712556, acc: 0.480469]\n",
      "237: [D loss: 0.711950, acc: 0.511719]  [A loss: 1.004702, acc: 0.078125]\n",
      "238: [D loss: 0.667264, acc: 0.595703]  [A loss: 0.727573, acc: 0.464844]\n",
      "239: [D loss: 0.711870, acc: 0.527344]  [A loss: 0.939388, acc: 0.117188]\n",
      "240: [D loss: 0.673230, acc: 0.568359]  [A loss: 0.766068, acc: 0.371094]\n",
      "241: [D loss: 0.689709, acc: 0.548828]  [A loss: 0.938623, acc: 0.128906]\n",
      "242: [D loss: 0.685012, acc: 0.582031]  [A loss: 0.836623, acc: 0.238281]\n",
      "243: [D loss: 0.688762, acc: 0.562500]  [A loss: 0.873561, acc: 0.203125]\n",
      "244: [D loss: 0.693881, acc: 0.544922]  [A loss: 0.871360, acc: 0.250000]\n",
      "245: [D loss: 0.687575, acc: 0.546875]  [A loss: 0.906958, acc: 0.199219]\n",
      "246: [D loss: 0.697368, acc: 0.552734]  [A loss: 0.917641, acc: 0.152344]\n",
      "247: [D loss: 0.688408, acc: 0.558594]  [A loss: 0.910170, acc: 0.156250]\n",
      "248: [D loss: 0.695190, acc: 0.558594]  [A loss: 0.915218, acc: 0.179688]\n",
      "249: [D loss: 0.692278, acc: 0.533203]  [A loss: 0.877699, acc: 0.191406]\n",
      "250: [D loss: 0.675253, acc: 0.589844]  [A loss: 1.007142, acc: 0.144531]\n",
      "251: [D loss: 0.695404, acc: 0.542969]  [A loss: 0.854236, acc: 0.226562]\n",
      "252: [D loss: 0.681703, acc: 0.566406]  [A loss: 0.862398, acc: 0.218750]\n",
      "253: [D loss: 0.674054, acc: 0.570312]  [A loss: 0.954014, acc: 0.140625]\n",
      "254: [D loss: 0.675641, acc: 0.570312]  [A loss: 0.797371, acc: 0.324219]\n",
      "255: [D loss: 0.703821, acc: 0.537109]  [A loss: 1.077991, acc: 0.046875]\n",
      "256: [D loss: 0.690709, acc: 0.564453]  [A loss: 0.733067, acc: 0.496094]\n",
      "257: [D loss: 0.707509, acc: 0.546875]  [A loss: 1.117377, acc: 0.039062]\n",
      "258: [D loss: 0.688296, acc: 0.542969]  [A loss: 0.669473, acc: 0.562500]\n",
      "259: [D loss: 0.713377, acc: 0.537109]  [A loss: 1.129150, acc: 0.031250]\n",
      "260: [D loss: 0.688927, acc: 0.562500]  [A loss: 0.653490, acc: 0.609375]\n",
      "261: [D loss: 0.692026, acc: 0.554688]  [A loss: 0.991405, acc: 0.093750]\n",
      "262: [D loss: 0.684010, acc: 0.583984]  [A loss: 0.750443, acc: 0.402344]\n",
      "263: [D loss: 0.697945, acc: 0.544922]  [A loss: 0.940548, acc: 0.109375]\n",
      "264: [D loss: 0.687768, acc: 0.556641]  [A loss: 0.766698, acc: 0.375000]\n",
      "265: [D loss: 0.706845, acc: 0.521484]  [A loss: 0.907619, acc: 0.167969]\n",
      "266: [D loss: 0.687176, acc: 0.572266]  [A loss: 0.819737, acc: 0.230469]\n",
      "267: [D loss: 0.693519, acc: 0.558594]  [A loss: 0.933059, acc: 0.121094]\n",
      "268: [D loss: 0.678922, acc: 0.550781]  [A loss: 0.835562, acc: 0.273438]\n",
      "269: [D loss: 0.693628, acc: 0.552734]  [A loss: 0.954297, acc: 0.113281]\n",
      "270: [D loss: 0.679340, acc: 0.566406]  [A loss: 0.739795, acc: 0.421875]\n",
      "271: [D loss: 0.708333, acc: 0.537109]  [A loss: 1.007515, acc: 0.074219]\n",
      "272: [D loss: 0.681407, acc: 0.574219]  [A loss: 0.728062, acc: 0.476562]\n",
      "273: [D loss: 0.689965, acc: 0.531250]  [A loss: 1.005273, acc: 0.117188]\n",
      "274: [D loss: 0.689231, acc: 0.537109]  [A loss: 0.836865, acc: 0.261719]\n",
      "275: [D loss: 0.706587, acc: 0.531250]  [A loss: 0.966439, acc: 0.109375]\n",
      "276: [D loss: 0.696574, acc: 0.529297]  [A loss: 0.773485, acc: 0.347656]\n",
      "277: [D loss: 0.709709, acc: 0.527344]  [A loss: 0.974351, acc: 0.074219]\n",
      "278: [D loss: 0.699069, acc: 0.541016]  [A loss: 0.801898, acc: 0.289062]\n",
      "279: [D loss: 0.678859, acc: 0.556641]  [A loss: 0.936072, acc: 0.152344]\n",
      "280: [D loss: 0.698757, acc: 0.544922]  [A loss: 0.792430, acc: 0.335938]\n",
      "281: [D loss: 0.696171, acc: 0.541016]  [A loss: 1.003454, acc: 0.089844]\n",
      "282: [D loss: 0.684201, acc: 0.562500]  [A loss: 0.676670, acc: 0.542969]\n",
      "283: [D loss: 0.722646, acc: 0.531250]  [A loss: 1.025499, acc: 0.074219]\n",
      "284: [D loss: 0.686145, acc: 0.585938]  [A loss: 0.694953, acc: 0.511719]\n",
      "285: [D loss: 0.728304, acc: 0.509766]  [A loss: 1.013509, acc: 0.085938]\n",
      "286: [D loss: 0.682859, acc: 0.574219]  [A loss: 0.747089, acc: 0.355469]\n",
      "287: [D loss: 0.702400, acc: 0.560547]  [A loss: 0.878877, acc: 0.167969]\n",
      "288: [D loss: 0.683206, acc: 0.560547]  [A loss: 0.807109, acc: 0.269531]\n",
      "289: [D loss: 0.697061, acc: 0.529297]  [A loss: 0.914286, acc: 0.171875]\n",
      "290: [D loss: 0.685974, acc: 0.548828]  [A loss: 0.814868, acc: 0.269531]\n",
      "291: [D loss: 0.691095, acc: 0.546875]  [A loss: 0.846735, acc: 0.242188]\n",
      "292: [D loss: 0.695835, acc: 0.558594]  [A loss: 0.906608, acc: 0.128906]\n",
      "293: [D loss: 0.680487, acc: 0.572266]  [A loss: 0.873861, acc: 0.179688]\n",
      "294: [D loss: 0.698267, acc: 0.580078]  [A loss: 0.920492, acc: 0.132812]\n",
      "295: [D loss: 0.696626, acc: 0.525391]  [A loss: 0.803576, acc: 0.296875]\n",
      "296: [D loss: 0.692902, acc: 0.537109]  [A loss: 0.989020, acc: 0.105469]\n",
      "297: [D loss: 0.679872, acc: 0.572266]  [A loss: 0.721222, acc: 0.464844]\n",
      "298: [D loss: 0.708596, acc: 0.505859]  [A loss: 1.089649, acc: 0.031250]\n",
      "299: [D loss: 0.683254, acc: 0.568359]  [A loss: 0.702583, acc: 0.503906]\n",
      "300: [D loss: 0.714451, acc: 0.521484]  [A loss: 1.042373, acc: 0.082031]\n",
      "301: [D loss: 0.682319, acc: 0.580078]  [A loss: 0.699759, acc: 0.507812]\n",
      "302: [D loss: 0.715097, acc: 0.515625]  [A loss: 0.973622, acc: 0.082031]\n",
      "303: [D loss: 0.677946, acc: 0.556641]  [A loss: 0.809168, acc: 0.265625]\n",
      "304: [D loss: 0.671374, acc: 0.601562]  [A loss: 0.918337, acc: 0.171875]\n",
      "305: [D loss: 0.685466, acc: 0.546875]  [A loss: 0.782793, acc: 0.375000]\n",
      "306: [D loss: 0.674534, acc: 0.583984]  [A loss: 0.982973, acc: 0.089844]\n",
      "307: [D loss: 0.672641, acc: 0.623047]  [A loss: 0.776091, acc: 0.343750]\n",
      "308: [D loss: 0.686686, acc: 0.585938]  [A loss: 0.951879, acc: 0.093750]\n",
      "309: [D loss: 0.681987, acc: 0.570312]  [A loss: 0.752396, acc: 0.433594]\n",
      "310: [D loss: 0.712159, acc: 0.533203]  [A loss: 0.967970, acc: 0.093750]\n",
      "311: [D loss: 0.706927, acc: 0.515625]  [A loss: 0.905491, acc: 0.109375]\n",
      "312: [D loss: 0.692193, acc: 0.548828]  [A loss: 0.927370, acc: 0.136719]\n",
      "313: [D loss: 0.689819, acc: 0.560547]  [A loss: 0.800182, acc: 0.257812]\n",
      "314: [D loss: 0.697374, acc: 0.542969]  [A loss: 0.969728, acc: 0.125000]\n",
      "315: [D loss: 0.671970, acc: 0.589844]  [A loss: 0.785218, acc: 0.355469]\n",
      "316: [D loss: 0.691372, acc: 0.546875]  [A loss: 0.993338, acc: 0.066406]\n",
      "317: [D loss: 0.670802, acc: 0.558594]  [A loss: 0.750063, acc: 0.355469]\n",
      "318: [D loss: 0.694760, acc: 0.541016]  [A loss: 1.018725, acc: 0.062500]\n",
      "319: [D loss: 0.683675, acc: 0.552734]  [A loss: 0.774173, acc: 0.347656]\n",
      "320: [D loss: 0.708937, acc: 0.507812]  [A loss: 0.934170, acc: 0.148438]\n",
      "321: [D loss: 0.675019, acc: 0.585938]  [A loss: 0.725333, acc: 0.445312]\n",
      "322: [D loss: 0.692414, acc: 0.529297]  [A loss: 1.008696, acc: 0.101562]\n",
      "323: [D loss: 0.684060, acc: 0.562500]  [A loss: 0.766570, acc: 0.367188]\n",
      "324: [D loss: 0.700307, acc: 0.537109]  [A loss: 0.960746, acc: 0.093750]\n",
      "325: [D loss: 0.688155, acc: 0.554688]  [A loss: 0.759163, acc: 0.363281]\n",
      "326: [D loss: 0.700493, acc: 0.527344]  [A loss: 0.965524, acc: 0.097656]\n",
      "327: [D loss: 0.675115, acc: 0.580078]  [A loss: 0.762571, acc: 0.410156]\n",
      "328: [D loss: 0.702201, acc: 0.527344]  [A loss: 1.003744, acc: 0.074219]\n",
      "329: [D loss: 0.694148, acc: 0.517578]  [A loss: 0.676918, acc: 0.589844]\n",
      "330: [D loss: 0.712767, acc: 0.537109]  [A loss: 1.023848, acc: 0.070312]\n",
      "331: [D loss: 0.683846, acc: 0.578125]  [A loss: 0.727482, acc: 0.433594]\n",
      "332: [D loss: 0.717815, acc: 0.525391]  [A loss: 0.921227, acc: 0.125000]\n",
      "333: [D loss: 0.685446, acc: 0.562500]  [A loss: 0.796080, acc: 0.324219]\n",
      "334: [D loss: 0.697502, acc: 0.521484]  [A loss: 0.885864, acc: 0.203125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335: [D loss: 0.687853, acc: 0.568359]  [A loss: 0.777139, acc: 0.316406]\n",
      "336: [D loss: 0.703183, acc: 0.517578]  [A loss: 1.037040, acc: 0.101562]\n",
      "337: [D loss: 0.685336, acc: 0.564453]  [A loss: 0.709512, acc: 0.488281]\n",
      "338: [D loss: 0.686759, acc: 0.560547]  [A loss: 0.917395, acc: 0.179688]\n",
      "339: [D loss: 0.677903, acc: 0.568359]  [A loss: 0.789671, acc: 0.339844]\n",
      "340: [D loss: 0.681247, acc: 0.562500]  [A loss: 0.870312, acc: 0.203125]\n",
      "341: [D loss: 0.702190, acc: 0.525391]  [A loss: 0.809745, acc: 0.296875]\n",
      "342: [D loss: 0.693947, acc: 0.539062]  [A loss: 0.947172, acc: 0.113281]\n",
      "343: [D loss: 0.686812, acc: 0.541016]  [A loss: 0.753005, acc: 0.371094]\n",
      "344: [D loss: 0.706476, acc: 0.529297]  [A loss: 0.934899, acc: 0.117188]\n",
      "345: [D loss: 0.685236, acc: 0.556641]  [A loss: 0.781853, acc: 0.351562]\n",
      "346: [D loss: 0.701401, acc: 0.517578]  [A loss: 0.899064, acc: 0.148438]\n",
      "347: [D loss: 0.695356, acc: 0.521484]  [A loss: 0.890066, acc: 0.160156]\n",
      "348: [D loss: 0.686770, acc: 0.558594]  [A loss: 0.876604, acc: 0.167969]\n",
      "349: [D loss: 0.693942, acc: 0.554688]  [A loss: 0.841223, acc: 0.203125]\n",
      "350: [D loss: 0.691767, acc: 0.542969]  [A loss: 0.927191, acc: 0.152344]\n",
      "351: [D loss: 0.704827, acc: 0.519531]  [A loss: 0.778090, acc: 0.347656]\n",
      "352: [D loss: 0.699028, acc: 0.548828]  [A loss: 0.949785, acc: 0.105469]\n",
      "353: [D loss: 0.681721, acc: 0.558594]  [A loss: 0.761630, acc: 0.378906]\n",
      "354: [D loss: 0.700175, acc: 0.552734]  [A loss: 0.982043, acc: 0.097656]\n",
      "355: [D loss: 0.700765, acc: 0.503906]  [A loss: 0.785855, acc: 0.316406]\n",
      "356: [D loss: 0.688506, acc: 0.593750]  [A loss: 0.977354, acc: 0.105469]\n",
      "357: [D loss: 0.672524, acc: 0.593750]  [A loss: 0.725455, acc: 0.453125]\n",
      "358: [D loss: 0.694768, acc: 0.537109]  [A loss: 0.998617, acc: 0.093750]\n",
      "359: [D loss: 0.673245, acc: 0.595703]  [A loss: 0.700881, acc: 0.496094]\n",
      "360: [D loss: 0.706383, acc: 0.531250]  [A loss: 0.950854, acc: 0.101562]\n",
      "361: [D loss: 0.687456, acc: 0.558594]  [A loss: 0.786568, acc: 0.312500]\n",
      "362: [D loss: 0.703907, acc: 0.546875]  [A loss: 1.007684, acc: 0.074219]\n",
      "363: [D loss: 0.681434, acc: 0.574219]  [A loss: 0.706543, acc: 0.445312]\n",
      "364: [D loss: 0.710118, acc: 0.539062]  [A loss: 0.950805, acc: 0.093750]\n",
      "365: [D loss: 0.679473, acc: 0.556641]  [A loss: 0.733153, acc: 0.433594]\n",
      "366: [D loss: 0.718723, acc: 0.486328]  [A loss: 0.909794, acc: 0.125000]\n",
      "367: [D loss: 0.698249, acc: 0.550781]  [A loss: 0.885553, acc: 0.167969]\n",
      "368: [D loss: 0.680888, acc: 0.566406]  [A loss: 0.848166, acc: 0.222656]\n",
      "369: [D loss: 0.689910, acc: 0.539062]  [A loss: 0.872701, acc: 0.167969]\n",
      "370: [D loss: 0.681738, acc: 0.550781]  [A loss: 0.840891, acc: 0.234375]\n",
      "371: [D loss: 0.688420, acc: 0.552734]  [A loss: 0.816321, acc: 0.230469]\n",
      "372: [D loss: 0.698744, acc: 0.531250]  [A loss: 0.936621, acc: 0.132812]\n",
      "373: [D loss: 0.676400, acc: 0.583984]  [A loss: 0.769822, acc: 0.332031]\n",
      "374: [D loss: 0.701010, acc: 0.531250]  [A loss: 0.953227, acc: 0.101562]\n",
      "375: [D loss: 0.678916, acc: 0.554688]  [A loss: 0.816571, acc: 0.253906]\n",
      "376: [D loss: 0.684884, acc: 0.564453]  [A loss: 0.971079, acc: 0.101562]\n",
      "377: [D loss: 0.688241, acc: 0.556641]  [A loss: 0.762989, acc: 0.394531]\n",
      "378: [D loss: 0.701360, acc: 0.515625]  [A loss: 1.045512, acc: 0.046875]\n",
      "379: [D loss: 0.672417, acc: 0.576172]  [A loss: 0.702461, acc: 0.503906]\n",
      "380: [D loss: 0.702076, acc: 0.529297]  [A loss: 1.099822, acc: 0.066406]\n",
      "381: [D loss: 0.690806, acc: 0.531250]  [A loss: 0.683342, acc: 0.558594]\n",
      "382: [D loss: 0.706862, acc: 0.517578]  [A loss: 1.005314, acc: 0.039062]\n",
      "383: [D loss: 0.680421, acc: 0.568359]  [A loss: 0.696616, acc: 0.539062]\n",
      "384: [D loss: 0.696781, acc: 0.525391]  [A loss: 0.913418, acc: 0.109375]\n",
      "385: [D loss: 0.688629, acc: 0.541016]  [A loss: 0.746845, acc: 0.378906]\n",
      "386: [D loss: 0.708104, acc: 0.521484]  [A loss: 0.930005, acc: 0.140625]\n",
      "387: [D loss: 0.687682, acc: 0.542969]  [A loss: 0.771793, acc: 0.347656]\n",
      "388: [D loss: 0.693393, acc: 0.556641]  [A loss: 0.931687, acc: 0.128906]\n",
      "389: [D loss: 0.697487, acc: 0.513672]  [A loss: 0.821423, acc: 0.234375]\n",
      "390: [D loss: 0.692470, acc: 0.546875]  [A loss: 0.916462, acc: 0.105469]\n",
      "391: [D loss: 0.686602, acc: 0.554688]  [A loss: 0.798840, acc: 0.257812]\n",
      "392: [D loss: 0.694149, acc: 0.531250]  [A loss: 0.919697, acc: 0.152344]\n",
      "393: [D loss: 0.696488, acc: 0.529297]  [A loss: 0.792122, acc: 0.296875]\n",
      "394: [D loss: 0.704241, acc: 0.542969]  [A loss: 0.960101, acc: 0.113281]\n",
      "395: [D loss: 0.687821, acc: 0.525391]  [A loss: 0.754754, acc: 0.398438]\n",
      "396: [D loss: 0.696416, acc: 0.533203]  [A loss: 0.914585, acc: 0.160156]\n",
      "397: [D loss: 0.691374, acc: 0.552734]  [A loss: 0.748056, acc: 0.351562]\n",
      "398: [D loss: 0.692085, acc: 0.537109]  [A loss: 0.948438, acc: 0.089844]\n",
      "399: [D loss: 0.690525, acc: 0.546875]  [A loss: 0.706281, acc: 0.527344]\n",
      "400: [D loss: 0.703893, acc: 0.492188]  [A loss: 0.961174, acc: 0.085938]\n",
      "401: [D loss: 0.694586, acc: 0.533203]  [A loss: 0.745019, acc: 0.386719]\n",
      "402: [D loss: 0.695671, acc: 0.539062]  [A loss: 0.891877, acc: 0.152344]\n",
      "403: [D loss: 0.687815, acc: 0.542969]  [A loss: 0.786378, acc: 0.335938]\n",
      "404: [D loss: 0.685016, acc: 0.539062]  [A loss: 0.860581, acc: 0.210938]\n",
      "405: [D loss: 0.680127, acc: 0.572266]  [A loss: 0.834915, acc: 0.253906]\n",
      "406: [D loss: 0.685962, acc: 0.533203]  [A loss: 0.857200, acc: 0.203125]\n",
      "407: [D loss: 0.688633, acc: 0.554688]  [A loss: 0.858573, acc: 0.195312]\n",
      "408: [D loss: 0.697241, acc: 0.535156]  [A loss: 0.879208, acc: 0.171875]\n",
      "409: [D loss: 0.689231, acc: 0.537109]  [A loss: 0.763597, acc: 0.390625]\n",
      "410: [D loss: 0.691350, acc: 0.546875]  [A loss: 0.974152, acc: 0.078125]\n",
      "411: [D loss: 0.678307, acc: 0.587891]  [A loss: 0.754113, acc: 0.371094]\n",
      "412: [D loss: 0.691962, acc: 0.544922]  [A loss: 0.941132, acc: 0.140625]\n",
      "413: [D loss: 0.685469, acc: 0.574219]  [A loss: 0.759501, acc: 0.375000]\n",
      "414: [D loss: 0.705360, acc: 0.509766]  [A loss: 0.956196, acc: 0.121094]\n",
      "415: [D loss: 0.683014, acc: 0.572266]  [A loss: 0.787158, acc: 0.292969]\n",
      "416: [D loss: 0.689640, acc: 0.546875]  [A loss: 0.883260, acc: 0.164062]\n",
      "417: [D loss: 0.678690, acc: 0.582031]  [A loss: 0.875391, acc: 0.191406]\n",
      "418: [D loss: 0.679077, acc: 0.585938]  [A loss: 0.874460, acc: 0.167969]\n",
      "419: [D loss: 0.678650, acc: 0.585938]  [A loss: 0.854440, acc: 0.238281]\n",
      "420: [D loss: 0.671849, acc: 0.611328]  [A loss: 0.818298, acc: 0.273438]\n",
      "421: [D loss: 0.691991, acc: 0.541016]  [A loss: 0.893180, acc: 0.152344]\n",
      "422: [D loss: 0.684536, acc: 0.554688]  [A loss: 0.851066, acc: 0.226562]\n",
      "423: [D loss: 0.683191, acc: 0.554688]  [A loss: 0.851244, acc: 0.207031]\n",
      "424: [D loss: 0.686918, acc: 0.576172]  [A loss: 0.875327, acc: 0.203125]\n",
      "425: [D loss: 0.675788, acc: 0.576172]  [A loss: 0.903444, acc: 0.152344]\n",
      "426: [D loss: 0.687990, acc: 0.546875]  [A loss: 0.842886, acc: 0.265625]\n",
      "427: [D loss: 0.707747, acc: 0.505859]  [A loss: 0.994230, acc: 0.070312]\n",
      "428: [D loss: 0.689068, acc: 0.521484]  [A loss: 0.736683, acc: 0.414062]\n",
      "429: [D loss: 0.700582, acc: 0.523438]  [A loss: 1.052523, acc: 0.046875]\n",
      "430: [D loss: 0.676839, acc: 0.570312]  [A loss: 0.685321, acc: 0.531250]\n",
      "431: [D loss: 0.709555, acc: 0.523438]  [A loss: 1.102287, acc: 0.050781]\n",
      "432: [D loss: 0.716883, acc: 0.494141]  [A loss: 0.647781, acc: 0.644531]\n",
      "433: [D loss: 0.722917, acc: 0.515625]  [A loss: 0.946352, acc: 0.121094]\n",
      "434: [D loss: 0.687843, acc: 0.550781]  [A loss: 0.759150, acc: 0.355469]\n",
      "435: [D loss: 0.694441, acc: 0.556641]  [A loss: 0.881328, acc: 0.144531]\n",
      "436: [D loss: 0.683709, acc: 0.542969]  [A loss: 0.781600, acc: 0.343750]\n",
      "437: [D loss: 0.681732, acc: 0.576172]  [A loss: 0.847043, acc: 0.230469]\n",
      "438: [D loss: 0.708493, acc: 0.500000]  [A loss: 0.835512, acc: 0.214844]\n",
      "439: [D loss: 0.686401, acc: 0.562500]  [A loss: 0.794256, acc: 0.328125]\n",
      "440: [D loss: 0.693713, acc: 0.544922]  [A loss: 0.833284, acc: 0.234375]\n",
      "441: [D loss: 0.686517, acc: 0.583984]  [A loss: 0.904932, acc: 0.136719]\n",
      "442: [D loss: 0.684699, acc: 0.546875]  [A loss: 0.813626, acc: 0.316406]\n",
      "443: [D loss: 0.690792, acc: 0.548828]  [A loss: 0.912343, acc: 0.132812]\n",
      "444: [D loss: 0.692756, acc: 0.542969]  [A loss: 0.776178, acc: 0.347656]\n",
      "445: [D loss: 0.700113, acc: 0.523438]  [A loss: 0.907099, acc: 0.121094]\n",
      "446: [D loss: 0.690090, acc: 0.542969]  [A loss: 0.772929, acc: 0.339844]\n",
      "447: [D loss: 0.697663, acc: 0.523438]  [A loss: 0.956946, acc: 0.113281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448: [D loss: 0.693419, acc: 0.535156]  [A loss: 0.761629, acc: 0.367188]\n",
      "449: [D loss: 0.692602, acc: 0.556641]  [A loss: 0.913591, acc: 0.156250]\n",
      "450: [D loss: 0.675563, acc: 0.587891]  [A loss: 0.783466, acc: 0.355469]\n",
      "451: [D loss: 0.681560, acc: 0.542969]  [A loss: 0.888405, acc: 0.175781]\n",
      "452: [D loss: 0.686233, acc: 0.515625]  [A loss: 0.800643, acc: 0.253906]\n",
      "453: [D loss: 0.697550, acc: 0.517578]  [A loss: 0.918673, acc: 0.152344]\n",
      "454: [D loss: 0.686726, acc: 0.548828]  [A loss: 0.856974, acc: 0.222656]\n",
      "455: [D loss: 0.709297, acc: 0.505859]  [A loss: 0.848750, acc: 0.191406]\n",
      "456: [D loss: 0.695824, acc: 0.544922]  [A loss: 0.940321, acc: 0.125000]\n",
      "457: [D loss: 0.701499, acc: 0.521484]  [A loss: 0.723380, acc: 0.468750]\n",
      "458: [D loss: 0.720771, acc: 0.501953]  [A loss: 1.031152, acc: 0.054688]\n",
      "459: [D loss: 0.685491, acc: 0.550781]  [A loss: 0.683120, acc: 0.507812]\n",
      "460: [D loss: 0.716022, acc: 0.525391]  [A loss: 0.946399, acc: 0.113281]\n",
      "461: [D loss: 0.681274, acc: 0.560547]  [A loss: 0.746147, acc: 0.429688]\n",
      "462: [D loss: 0.702511, acc: 0.544922]  [A loss: 0.887401, acc: 0.164062]\n",
      "463: [D loss: 0.702564, acc: 0.517578]  [A loss: 0.814149, acc: 0.253906]\n",
      "464: [D loss: 0.696596, acc: 0.548828]  [A loss: 0.814755, acc: 0.253906]\n",
      "465: [D loss: 0.693652, acc: 0.537109]  [A loss: 0.861231, acc: 0.171875]\n",
      "466: [D loss: 0.691293, acc: 0.562500]  [A loss: 0.852345, acc: 0.195312]\n",
      "467: [D loss: 0.672764, acc: 0.593750]  [A loss: 0.854691, acc: 0.179688]\n",
      "468: [D loss: 0.689923, acc: 0.558594]  [A loss: 0.903830, acc: 0.148438]\n",
      "469: [D loss: 0.700579, acc: 0.537109]  [A loss: 0.883677, acc: 0.187500]\n",
      "470: [D loss: 0.700741, acc: 0.511719]  [A loss: 0.887635, acc: 0.148438]\n",
      "471: [D loss: 0.678609, acc: 0.591797]  [A loss: 0.800315, acc: 0.304688]\n",
      "472: [D loss: 0.695168, acc: 0.562500]  [A loss: 0.892582, acc: 0.148438]\n",
      "473: [D loss: 0.681961, acc: 0.548828]  [A loss: 0.832634, acc: 0.238281]\n",
      "474: [D loss: 0.690327, acc: 0.548828]  [A loss: 0.893486, acc: 0.144531]\n",
      "475: [D loss: 0.678898, acc: 0.566406]  [A loss: 0.815219, acc: 0.273438]\n",
      "476: [D loss: 0.698180, acc: 0.509766]  [A loss: 0.969340, acc: 0.078125]\n",
      "477: [D loss: 0.695619, acc: 0.519531]  [A loss: 0.718665, acc: 0.488281]\n",
      "478: [D loss: 0.707092, acc: 0.521484]  [A loss: 1.050237, acc: 0.035156]\n",
      "479: [D loss: 0.682146, acc: 0.542969]  [A loss: 0.698518, acc: 0.507812]\n",
      "480: [D loss: 0.705251, acc: 0.517578]  [A loss: 0.923896, acc: 0.136719]\n",
      "481: [D loss: 0.696898, acc: 0.525391]  [A loss: 0.833872, acc: 0.218750]\n",
      "482: [D loss: 0.674696, acc: 0.593750]  [A loss: 0.822456, acc: 0.273438]\n",
      "483: [D loss: 0.685858, acc: 0.539062]  [A loss: 0.806243, acc: 0.269531]\n",
      "484: [D loss: 0.692029, acc: 0.541016]  [A loss: 0.896246, acc: 0.132812]\n",
      "485: [D loss: 0.678311, acc: 0.585938]  [A loss: 0.789354, acc: 0.343750]\n",
      "486: [D loss: 0.694079, acc: 0.558594]  [A loss: 0.969061, acc: 0.097656]\n",
      "487: [D loss: 0.677616, acc: 0.576172]  [A loss: 0.722469, acc: 0.453125]\n",
      "488: [D loss: 0.695442, acc: 0.542969]  [A loss: 0.971117, acc: 0.074219]\n",
      "489: [D loss: 0.674474, acc: 0.587891]  [A loss: 0.760725, acc: 0.367188]\n",
      "490: [D loss: 0.705875, acc: 0.529297]  [A loss: 1.030359, acc: 0.097656]\n",
      "491: [D loss: 0.680936, acc: 0.556641]  [A loss: 0.720494, acc: 0.468750]\n",
      "492: [D loss: 0.713519, acc: 0.529297]  [A loss: 0.904990, acc: 0.128906]\n",
      "493: [D loss: 0.691263, acc: 0.544922]  [A loss: 0.891539, acc: 0.136719]\n",
      "494: [D loss: 0.687477, acc: 0.552734]  [A loss: 0.861001, acc: 0.199219]\n",
      "495: [D loss: 0.690433, acc: 0.533203]  [A loss: 0.867886, acc: 0.187500]\n",
      "496: [D loss: 0.683225, acc: 0.541016]  [A loss: 0.848114, acc: 0.179688]\n",
      "497: [D loss: 0.683565, acc: 0.527344]  [A loss: 0.853611, acc: 0.242188]\n",
      "498: [D loss: 0.701291, acc: 0.546875]  [A loss: 0.804175, acc: 0.300781]\n",
      "499: [D loss: 0.682699, acc: 0.539062]  [A loss: 0.866194, acc: 0.187500]\n",
      "500: [D loss: 0.706447, acc: 0.496094]  [A loss: 0.831717, acc: 0.218750]\n",
      "501: [D loss: 0.691499, acc: 0.521484]  [A loss: 0.881938, acc: 0.171875]\n",
      "502: [D loss: 0.700062, acc: 0.500000]  [A loss: 0.878511, acc: 0.175781]\n",
      "503: [D loss: 0.683321, acc: 0.589844]  [A loss: 0.874206, acc: 0.175781]\n",
      "504: [D loss: 0.680195, acc: 0.570312]  [A loss: 0.865123, acc: 0.218750]\n",
      "505: [D loss: 0.696087, acc: 0.548828]  [A loss: 0.870748, acc: 0.183594]\n",
      "506: [D loss: 0.682905, acc: 0.554688]  [A loss: 0.922936, acc: 0.160156]\n",
      "507: [D loss: 0.690672, acc: 0.554688]  [A loss: 0.768251, acc: 0.351562]\n",
      "508: [D loss: 0.685913, acc: 0.554688]  [A loss: 0.921478, acc: 0.156250]\n",
      "509: [D loss: 0.689884, acc: 0.537109]  [A loss: 0.893619, acc: 0.179688]\n",
      "510: [D loss: 0.681886, acc: 0.550781]  [A loss: 0.919850, acc: 0.148438]\n",
      "511: [D loss: 0.693877, acc: 0.554688]  [A loss: 0.856941, acc: 0.191406]\n",
      "512: [D loss: 0.694392, acc: 0.541016]  [A loss: 0.975343, acc: 0.097656]\n",
      "513: [D loss: 0.683634, acc: 0.580078]  [A loss: 0.713846, acc: 0.484375]\n",
      "514: [D loss: 0.707652, acc: 0.542969]  [A loss: 1.166323, acc: 0.011719]\n",
      "515: [D loss: 0.698970, acc: 0.521484]  [A loss: 0.617923, acc: 0.707031]\n",
      "516: [D loss: 0.738692, acc: 0.503906]  [A loss: 1.013243, acc: 0.046875]\n",
      "517: [D loss: 0.683780, acc: 0.556641]  [A loss: 0.675081, acc: 0.558594]\n",
      "518: [D loss: 0.717085, acc: 0.513672]  [A loss: 0.944964, acc: 0.128906]\n",
      "519: [D loss: 0.684660, acc: 0.576172]  [A loss: 0.788627, acc: 0.324219]\n",
      "520: [D loss: 0.685927, acc: 0.572266]  [A loss: 0.832188, acc: 0.195312]\n",
      "521: [D loss: 0.703236, acc: 0.521484]  [A loss: 0.787422, acc: 0.335938]\n",
      "522: [D loss: 0.694677, acc: 0.544922]  [A loss: 0.897829, acc: 0.125000]\n",
      "523: [D loss: 0.687937, acc: 0.550781]  [A loss: 0.821940, acc: 0.222656]\n",
      "524: [D loss: 0.698755, acc: 0.500000]  [A loss: 0.884780, acc: 0.179688]\n",
      "525: [D loss: 0.686950, acc: 0.544922]  [A loss: 0.793273, acc: 0.300781]\n",
      "526: [D loss: 0.705455, acc: 0.521484]  [A loss: 0.821105, acc: 0.238281]\n",
      "527: [D loss: 0.690615, acc: 0.556641]  [A loss: 0.813538, acc: 0.265625]\n",
      "528: [D loss: 0.685410, acc: 0.550781]  [A loss: 0.818217, acc: 0.238281]\n",
      "529: [D loss: 0.697161, acc: 0.535156]  [A loss: 0.837482, acc: 0.257812]\n",
      "530: [D loss: 0.686041, acc: 0.556641]  [A loss: 0.836486, acc: 0.183594]\n",
      "531: [D loss: 0.678697, acc: 0.578125]  [A loss: 0.868494, acc: 0.207031]\n",
      "532: [D loss: 0.695398, acc: 0.537109]  [A loss: 0.841040, acc: 0.238281]\n",
      "533: [D loss: 0.686980, acc: 0.570312]  [A loss: 0.884868, acc: 0.152344]\n",
      "534: [D loss: 0.677692, acc: 0.568359]  [A loss: 0.866467, acc: 0.203125]\n",
      "535: [D loss: 0.689275, acc: 0.554688]  [A loss: 0.904278, acc: 0.156250]\n",
      "536: [D loss: 0.681302, acc: 0.570312]  [A loss: 0.777613, acc: 0.339844]\n",
      "537: [D loss: 0.698158, acc: 0.500000]  [A loss: 0.937189, acc: 0.125000]\n",
      "538: [D loss: 0.690078, acc: 0.546875]  [A loss: 0.687347, acc: 0.500000]\n",
      "539: [D loss: 0.732446, acc: 0.509766]  [A loss: 1.056242, acc: 0.054688]\n",
      "540: [D loss: 0.685196, acc: 0.574219]  [A loss: 0.674188, acc: 0.562500]\n",
      "541: [D loss: 0.714549, acc: 0.501953]  [A loss: 0.945080, acc: 0.097656]\n",
      "542: [D loss: 0.687922, acc: 0.535156]  [A loss: 0.756231, acc: 0.394531]\n",
      "543: [D loss: 0.707804, acc: 0.531250]  [A loss: 0.965412, acc: 0.085938]\n",
      "544: [D loss: 0.677470, acc: 0.562500]  [A loss: 0.708329, acc: 0.476562]\n",
      "545: [D loss: 0.712898, acc: 0.548828]  [A loss: 0.830926, acc: 0.246094]\n",
      "546: [D loss: 0.690662, acc: 0.554688]  [A loss: 0.830929, acc: 0.222656]\n",
      "547: [D loss: 0.696887, acc: 0.541016]  [A loss: 0.828611, acc: 0.214844]\n",
      "548: [D loss: 0.695997, acc: 0.523438]  [A loss: 0.904972, acc: 0.144531]\n",
      "549: [D loss: 0.683584, acc: 0.558594]  [A loss: 0.750447, acc: 0.390625]\n",
      "550: [D loss: 0.698200, acc: 0.531250]  [A loss: 0.954139, acc: 0.101562]\n",
      "551: [D loss: 0.676308, acc: 0.576172]  [A loss: 0.733815, acc: 0.464844]\n",
      "552: [D loss: 0.690262, acc: 0.554688]  [A loss: 0.918759, acc: 0.144531]\n",
      "553: [D loss: 0.686979, acc: 0.546875]  [A loss: 0.796349, acc: 0.312500]\n",
      "554: [D loss: 0.704679, acc: 0.537109]  [A loss: 0.853624, acc: 0.175781]\n",
      "555: [D loss: 0.683773, acc: 0.558594]  [A loss: 0.787648, acc: 0.316406]\n",
      "556: [D loss: 0.690463, acc: 0.533203]  [A loss: 0.887114, acc: 0.152344]\n",
      "557: [D loss: 0.686061, acc: 0.562500]  [A loss: 0.764759, acc: 0.378906]\n",
      "558: [D loss: 0.722207, acc: 0.500000]  [A loss: 0.933334, acc: 0.121094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559: [D loss: 0.686899, acc: 0.566406]  [A loss: 0.754179, acc: 0.386719]\n",
      "560: [D loss: 0.698264, acc: 0.525391]  [A loss: 0.899351, acc: 0.132812]\n",
      "561: [D loss: 0.671211, acc: 0.580078]  [A loss: 0.828891, acc: 0.218750]\n",
      "562: [D loss: 0.695293, acc: 0.539062]  [A loss: 0.867600, acc: 0.164062]\n",
      "563: [D loss: 0.695971, acc: 0.548828]  [A loss: 0.847094, acc: 0.238281]\n",
      "564: [D loss: 0.693605, acc: 0.535156]  [A loss: 0.842684, acc: 0.164062]\n",
      "565: [D loss: 0.703434, acc: 0.539062]  [A loss: 0.902956, acc: 0.128906]\n",
      "566: [D loss: 0.686744, acc: 0.568359]  [A loss: 0.797244, acc: 0.281250]\n",
      "567: [D loss: 0.692663, acc: 0.564453]  [A loss: 0.862992, acc: 0.175781]\n",
      "568: [D loss: 0.694568, acc: 0.537109]  [A loss: 0.810221, acc: 0.292969]\n",
      "569: [D loss: 0.704407, acc: 0.535156]  [A loss: 0.974279, acc: 0.085938]\n",
      "570: [D loss: 0.689823, acc: 0.519531]  [A loss: 0.792121, acc: 0.289062]\n",
      "571: [D loss: 0.693333, acc: 0.527344]  [A loss: 0.963444, acc: 0.082031]\n",
      "572: [D loss: 0.673430, acc: 0.558594]  [A loss: 0.744879, acc: 0.410156]\n",
      "573: [D loss: 0.706898, acc: 0.529297]  [A loss: 0.963573, acc: 0.082031]\n",
      "574: [D loss: 0.690669, acc: 0.521484]  [A loss: 0.683115, acc: 0.539062]\n",
      "575: [D loss: 0.720528, acc: 0.507812]  [A loss: 0.971917, acc: 0.089844]\n",
      "576: [D loss: 0.695396, acc: 0.539062]  [A loss: 0.731764, acc: 0.410156]\n",
      "577: [D loss: 0.705643, acc: 0.537109]  [A loss: 0.914645, acc: 0.085938]\n",
      "578: [D loss: 0.678978, acc: 0.560547]  [A loss: 0.726160, acc: 0.464844]\n",
      "579: [D loss: 0.699488, acc: 0.542969]  [A loss: 0.871502, acc: 0.171875]\n",
      "580: [D loss: 0.691987, acc: 0.523438]  [A loss: 0.800246, acc: 0.332031]\n",
      "581: [D loss: 0.701309, acc: 0.537109]  [A loss: 0.888083, acc: 0.128906]\n",
      "582: [D loss: 0.683758, acc: 0.585938]  [A loss: 0.811100, acc: 0.238281]\n",
      "583: [D loss: 0.696397, acc: 0.523438]  [A loss: 0.867835, acc: 0.175781]\n",
      "584: [D loss: 0.691820, acc: 0.560547]  [A loss: 0.846025, acc: 0.214844]\n",
      "585: [D loss: 0.690229, acc: 0.523438]  [A loss: 0.857565, acc: 0.164062]\n",
      "586: [D loss: 0.692335, acc: 0.537109]  [A loss: 0.819737, acc: 0.238281]\n",
      "587: [D loss: 0.679239, acc: 0.562500]  [A loss: 0.875372, acc: 0.160156]\n",
      "588: [D loss: 0.690459, acc: 0.539062]  [A loss: 0.801410, acc: 0.269531]\n",
      "589: [D loss: 0.709099, acc: 0.525391]  [A loss: 0.973105, acc: 0.085938]\n",
      "590: [D loss: 0.695873, acc: 0.539062]  [A loss: 0.718384, acc: 0.492188]\n",
      "591: [D loss: 0.711309, acc: 0.527344]  [A loss: 0.977479, acc: 0.074219]\n",
      "592: [D loss: 0.693324, acc: 0.521484]  [A loss: 0.684510, acc: 0.535156]\n",
      "593: [D loss: 0.706210, acc: 0.519531]  [A loss: 0.990961, acc: 0.070312]\n",
      "594: [D loss: 0.681416, acc: 0.544922]  [A loss: 0.704116, acc: 0.468750]\n",
      "595: [D loss: 0.708434, acc: 0.486328]  [A loss: 0.899916, acc: 0.113281]\n",
      "596: [D loss: 0.686076, acc: 0.546875]  [A loss: 0.744489, acc: 0.390625]\n",
      "597: [D loss: 0.678703, acc: 0.570312]  [A loss: 0.894530, acc: 0.156250]\n",
      "598: [D loss: 0.687465, acc: 0.560547]  [A loss: 0.811817, acc: 0.300781]\n",
      "599: [D loss: 0.695603, acc: 0.537109]  [A loss: 0.868323, acc: 0.218750]\n",
      "600: [D loss: 0.688741, acc: 0.550781]  [A loss: 0.768910, acc: 0.332031]\n",
      "601: [D loss: 0.704446, acc: 0.515625]  [A loss: 0.942471, acc: 0.125000]\n",
      "602: [D loss: 0.697174, acc: 0.517578]  [A loss: 0.741727, acc: 0.398438]\n",
      "603: [D loss: 0.711859, acc: 0.537109]  [A loss: 0.924807, acc: 0.093750]\n",
      "604: [D loss: 0.678362, acc: 0.572266]  [A loss: 0.777792, acc: 0.300781]\n",
      "605: [D loss: 0.712133, acc: 0.496094]  [A loss: 0.904676, acc: 0.093750]\n",
      "606: [D loss: 0.684211, acc: 0.595703]  [A loss: 0.823962, acc: 0.226562]\n",
      "607: [D loss: 0.681697, acc: 0.572266]  [A loss: 0.847060, acc: 0.191406]\n",
      "608: [D loss: 0.669684, acc: 0.611328]  [A loss: 0.819228, acc: 0.285156]\n",
      "609: [D loss: 0.686028, acc: 0.527344]  [A loss: 0.850091, acc: 0.207031]\n",
      "610: [D loss: 0.692196, acc: 0.548828]  [A loss: 0.944960, acc: 0.125000]\n",
      "611: [D loss: 0.668772, acc: 0.591797]  [A loss: 0.740729, acc: 0.417969]\n",
      "612: [D loss: 0.701640, acc: 0.505859]  [A loss: 0.917219, acc: 0.125000]\n",
      "613: [D loss: 0.683527, acc: 0.539062]  [A loss: 0.727390, acc: 0.417969]\n",
      "614: [D loss: 0.712857, acc: 0.513672]  [A loss: 0.993954, acc: 0.074219]\n",
      "615: [D loss: 0.685986, acc: 0.556641]  [A loss: 0.697485, acc: 0.531250]\n",
      "616: [D loss: 0.694998, acc: 0.523438]  [A loss: 0.967610, acc: 0.078125]\n",
      "617: [D loss: 0.690246, acc: 0.523438]  [A loss: 0.716537, acc: 0.484375]\n",
      "618: [D loss: 0.689580, acc: 0.552734]  [A loss: 0.926363, acc: 0.117188]\n",
      "619: [D loss: 0.692292, acc: 0.546875]  [A loss: 0.783637, acc: 0.300781]\n",
      "620: [D loss: 0.685416, acc: 0.550781]  [A loss: 0.862551, acc: 0.171875]\n",
      "621: [D loss: 0.699570, acc: 0.509766]  [A loss: 0.807151, acc: 0.273438]\n",
      "622: [D loss: 0.685279, acc: 0.554688]  [A loss: 0.855178, acc: 0.210938]\n",
      "623: [D loss: 0.714858, acc: 0.490234]  [A loss: 0.848577, acc: 0.203125]\n",
      "624: [D loss: 0.671648, acc: 0.578125]  [A loss: 0.883633, acc: 0.160156]\n",
      "625: [D loss: 0.694497, acc: 0.542969]  [A loss: 0.807769, acc: 0.277344]\n",
      "626: [D loss: 0.707915, acc: 0.507812]  [A loss: 0.913226, acc: 0.097656]\n",
      "627: [D loss: 0.685038, acc: 0.570312]  [A loss: 0.798268, acc: 0.273438]\n",
      "628: [D loss: 0.693676, acc: 0.548828]  [A loss: 0.943798, acc: 0.101562]\n",
      "629: [D loss: 0.695689, acc: 0.525391]  [A loss: 0.755339, acc: 0.335938]\n",
      "630: [D loss: 0.718334, acc: 0.503906]  [A loss: 0.958748, acc: 0.093750]\n",
      "631: [D loss: 0.699732, acc: 0.541016]  [A loss: 0.770914, acc: 0.332031]\n",
      "632: [D loss: 0.697194, acc: 0.541016]  [A loss: 0.890926, acc: 0.113281]\n",
      "633: [D loss: 0.677649, acc: 0.574219]  [A loss: 0.775264, acc: 0.320312]\n",
      "634: [D loss: 0.701564, acc: 0.521484]  [A loss: 0.894610, acc: 0.125000]\n",
      "635: [D loss: 0.695816, acc: 0.562500]  [A loss: 0.777272, acc: 0.332031]\n",
      "636: [D loss: 0.694433, acc: 0.537109]  [A loss: 0.894535, acc: 0.171875]\n",
      "637: [D loss: 0.700706, acc: 0.505859]  [A loss: 0.791030, acc: 0.296875]\n",
      "638: [D loss: 0.696825, acc: 0.513672]  [A loss: 0.928404, acc: 0.144531]\n",
      "639: [D loss: 0.683568, acc: 0.564453]  [A loss: 0.776855, acc: 0.316406]\n",
      "640: [D loss: 0.696880, acc: 0.517578]  [A loss: 0.900949, acc: 0.105469]\n",
      "641: [D loss: 0.691807, acc: 0.525391]  [A loss: 0.744123, acc: 0.386719]\n",
      "642: [D loss: 0.693736, acc: 0.550781]  [A loss: 0.958211, acc: 0.101562]\n",
      "643: [D loss: 0.683350, acc: 0.580078]  [A loss: 0.725818, acc: 0.468750]\n",
      "644: [D loss: 0.708085, acc: 0.529297]  [A loss: 0.903820, acc: 0.156250]\n",
      "645: [D loss: 0.688903, acc: 0.537109]  [A loss: 0.784603, acc: 0.289062]\n",
      "646: [D loss: 0.701144, acc: 0.482422]  [A loss: 0.849351, acc: 0.179688]\n",
      "647: [D loss: 0.688200, acc: 0.539062]  [A loss: 0.796028, acc: 0.289062]\n",
      "648: [D loss: 0.688173, acc: 0.560547]  [A loss: 0.856939, acc: 0.175781]\n",
      "649: [D loss: 0.689984, acc: 0.523438]  [A loss: 0.802685, acc: 0.289062]\n",
      "650: [D loss: 0.701201, acc: 0.517578]  [A loss: 0.886203, acc: 0.140625]\n",
      "651: [D loss: 0.693264, acc: 0.560547]  [A loss: 0.785787, acc: 0.285156]\n",
      "652: [D loss: 0.695328, acc: 0.541016]  [A loss: 0.959860, acc: 0.085938]\n",
      "653: [D loss: 0.688332, acc: 0.537109]  [A loss: 0.723371, acc: 0.433594]\n",
      "654: [D loss: 0.708162, acc: 0.513672]  [A loss: 0.921623, acc: 0.105469]\n",
      "655: [D loss: 0.695320, acc: 0.537109]  [A loss: 0.776831, acc: 0.281250]\n",
      "656: [D loss: 0.705819, acc: 0.527344]  [A loss: 0.932101, acc: 0.082031]\n",
      "657: [D loss: 0.678847, acc: 0.552734]  [A loss: 0.754141, acc: 0.375000]\n",
      "658: [D loss: 0.712379, acc: 0.498047]  [A loss: 0.932698, acc: 0.089844]\n",
      "659: [D loss: 0.680063, acc: 0.591797]  [A loss: 0.775575, acc: 0.335938]\n",
      "660: [D loss: 0.696243, acc: 0.550781]  [A loss: 0.921005, acc: 0.117188]\n",
      "661: [D loss: 0.679570, acc: 0.580078]  [A loss: 0.746868, acc: 0.339844]\n",
      "662: [D loss: 0.691709, acc: 0.544922]  [A loss: 0.867987, acc: 0.156250]\n",
      "663: [D loss: 0.674391, acc: 0.572266]  [A loss: 0.792801, acc: 0.300781]\n",
      "664: [D loss: 0.687823, acc: 0.541016]  [A loss: 0.894961, acc: 0.132812]\n",
      "665: [D loss: 0.694529, acc: 0.505859]  [A loss: 0.775088, acc: 0.312500]\n",
      "666: [D loss: 0.679746, acc: 0.566406]  [A loss: 0.931989, acc: 0.171875]\n",
      "667: [D loss: 0.699188, acc: 0.531250]  [A loss: 0.706288, acc: 0.480469]\n",
      "668: [D loss: 0.702821, acc: 0.533203]  [A loss: 0.965200, acc: 0.074219]\n",
      "669: [D loss: 0.688799, acc: 0.564453]  [A loss: 0.753995, acc: 0.378906]\n",
      "670: [D loss: 0.703489, acc: 0.490234]  [A loss: 0.919008, acc: 0.109375]\n",
      "671: [D loss: 0.686338, acc: 0.552734]  [A loss: 0.773153, acc: 0.324219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672: [D loss: 0.702325, acc: 0.537109]  [A loss: 0.971405, acc: 0.078125]\n",
      "673: [D loss: 0.698992, acc: 0.498047]  [A loss: 0.710358, acc: 0.472656]\n",
      "674: [D loss: 0.702851, acc: 0.519531]  [A loss: 0.917505, acc: 0.148438]\n",
      "675: [D loss: 0.700723, acc: 0.529297]  [A loss: 0.775658, acc: 0.343750]\n",
      "676: [D loss: 0.685557, acc: 0.560547]  [A loss: 0.832374, acc: 0.269531]\n",
      "677: [D loss: 0.686862, acc: 0.544922]  [A loss: 0.789268, acc: 0.328125]\n",
      "678: [D loss: 0.698396, acc: 0.539062]  [A loss: 0.877815, acc: 0.164062]\n",
      "679: [D loss: 0.690748, acc: 0.535156]  [A loss: 0.828154, acc: 0.214844]\n",
      "680: [D loss: 0.689013, acc: 0.556641]  [A loss: 0.792301, acc: 0.335938]\n",
      "681: [D loss: 0.701961, acc: 0.539062]  [A loss: 0.933023, acc: 0.101562]\n",
      "682: [D loss: 0.692360, acc: 0.537109]  [A loss: 0.735812, acc: 0.390625]\n",
      "683: [D loss: 0.698735, acc: 0.527344]  [A loss: 0.963985, acc: 0.082031]\n",
      "684: [D loss: 0.682872, acc: 0.566406]  [A loss: 0.711233, acc: 0.484375]\n",
      "685: [D loss: 0.716470, acc: 0.521484]  [A loss: 1.003700, acc: 0.046875]\n",
      "686: [D loss: 0.700696, acc: 0.498047]  [A loss: 0.706070, acc: 0.476562]\n",
      "687: [D loss: 0.702873, acc: 0.546875]  [A loss: 0.869562, acc: 0.160156]\n",
      "688: [D loss: 0.711663, acc: 0.488281]  [A loss: 0.823935, acc: 0.222656]\n",
      "689: [D loss: 0.682965, acc: 0.535156]  [A loss: 0.836649, acc: 0.238281]\n",
      "690: [D loss: 0.671403, acc: 0.589844]  [A loss: 0.836147, acc: 0.234375]\n",
      "691: [D loss: 0.698988, acc: 0.521484]  [A loss: 0.836818, acc: 0.195312]\n",
      "692: [D loss: 0.695872, acc: 0.525391]  [A loss: 0.781594, acc: 0.300781]\n",
      "693: [D loss: 0.691450, acc: 0.562500]  [A loss: 0.850074, acc: 0.214844]\n",
      "694: [D loss: 0.690066, acc: 0.550781]  [A loss: 0.851616, acc: 0.203125]\n",
      "695: [D loss: 0.706188, acc: 0.474609]  [A loss: 0.854963, acc: 0.207031]\n",
      "696: [D loss: 0.690750, acc: 0.529297]  [A loss: 0.817382, acc: 0.269531]\n",
      "697: [D loss: 0.707885, acc: 0.496094]  [A loss: 0.856320, acc: 0.191406]\n",
      "698: [D loss: 0.685477, acc: 0.558594]  [A loss: 0.772896, acc: 0.351562]\n",
      "699: [D loss: 0.713741, acc: 0.537109]  [A loss: 0.929792, acc: 0.089844]\n",
      "700: [D loss: 0.696109, acc: 0.529297]  [A loss: 0.822443, acc: 0.230469]\n",
      "701: [D loss: 0.685423, acc: 0.568359]  [A loss: 0.902644, acc: 0.164062]\n",
      "702: [D loss: 0.693568, acc: 0.544922]  [A loss: 0.796662, acc: 0.246094]\n",
      "703: [D loss: 0.694124, acc: 0.539062]  [A loss: 0.947184, acc: 0.113281]\n",
      "704: [D loss: 0.680334, acc: 0.546875]  [A loss: 0.678263, acc: 0.558594]\n",
      "705: [D loss: 0.718455, acc: 0.519531]  [A loss: 1.053657, acc: 0.058594]\n",
      "706: [D loss: 0.701958, acc: 0.517578]  [A loss: 0.672828, acc: 0.531250]\n",
      "707: [D loss: 0.716217, acc: 0.521484]  [A loss: 0.951773, acc: 0.085938]\n",
      "708: [D loss: 0.681209, acc: 0.562500]  [A loss: 0.753047, acc: 0.386719]\n",
      "709: [D loss: 0.709191, acc: 0.500000]  [A loss: 0.847915, acc: 0.183594]\n",
      "710: [D loss: 0.692394, acc: 0.513672]  [A loss: 0.791557, acc: 0.261719]\n",
      "711: [D loss: 0.695946, acc: 0.546875]  [A loss: 0.809989, acc: 0.265625]\n",
      "712: [D loss: 0.694193, acc: 0.546875]  [A loss: 0.871319, acc: 0.152344]\n",
      "713: [D loss: 0.680043, acc: 0.582031]  [A loss: 0.875839, acc: 0.152344]\n",
      "714: [D loss: 0.697252, acc: 0.523438]  [A loss: 0.833703, acc: 0.226562]\n",
      "715: [D loss: 0.694871, acc: 0.519531]  [A loss: 0.807816, acc: 0.281250]\n",
      "716: [D loss: 0.683420, acc: 0.556641]  [A loss: 0.855247, acc: 0.187500]\n",
      "717: [D loss: 0.693559, acc: 0.558594]  [A loss: 0.850879, acc: 0.187500]\n",
      "718: [D loss: 0.694270, acc: 0.544922]  [A loss: 0.787459, acc: 0.308594]\n",
      "719: [D loss: 0.686049, acc: 0.552734]  [A loss: 0.844733, acc: 0.183594]\n",
      "720: [D loss: 0.684611, acc: 0.556641]  [A loss: 0.811624, acc: 0.230469]\n",
      "721: [D loss: 0.690505, acc: 0.548828]  [A loss: 0.870436, acc: 0.125000]\n",
      "722: [D loss: 0.700310, acc: 0.513672]  [A loss: 0.880785, acc: 0.144531]\n",
      "723: [D loss: 0.688674, acc: 0.546875]  [A loss: 0.774440, acc: 0.308594]\n",
      "724: [D loss: 0.705909, acc: 0.525391]  [A loss: 0.938182, acc: 0.128906]\n",
      "725: [D loss: 0.685063, acc: 0.546875]  [A loss: 0.847272, acc: 0.175781]\n",
      "726: [D loss: 0.703102, acc: 0.521484]  [A loss: 0.871603, acc: 0.167969]\n",
      "727: [D loss: 0.685772, acc: 0.537109]  [A loss: 0.775142, acc: 0.351562]\n",
      "728: [D loss: 0.700059, acc: 0.558594]  [A loss: 0.891103, acc: 0.125000]\n",
      "729: [D loss: 0.680426, acc: 0.566406]  [A loss: 0.697826, acc: 0.484375]\n",
      "730: [D loss: 0.706900, acc: 0.529297]  [A loss: 0.985750, acc: 0.078125]\n",
      "731: [D loss: 0.688997, acc: 0.570312]  [A loss: 0.680003, acc: 0.542969]\n",
      "732: [D loss: 0.700283, acc: 0.515625]  [A loss: 0.955634, acc: 0.101562]\n",
      "733: [D loss: 0.711477, acc: 0.492188]  [A loss: 0.759662, acc: 0.335938]\n",
      "734: [D loss: 0.694131, acc: 0.556641]  [A loss: 0.830537, acc: 0.183594]\n",
      "735: [D loss: 0.686152, acc: 0.541016]  [A loss: 0.836006, acc: 0.234375]\n",
      "736: [D loss: 0.697272, acc: 0.525391]  [A loss: 0.829136, acc: 0.199219]\n",
      "737: [D loss: 0.700306, acc: 0.519531]  [A loss: 0.841867, acc: 0.195312]\n",
      "738: [D loss: 0.684763, acc: 0.541016]  [A loss: 0.780087, acc: 0.324219]\n",
      "739: [D loss: 0.687371, acc: 0.542969]  [A loss: 0.932841, acc: 0.093750]\n",
      "740: [D loss: 0.713973, acc: 0.484375]  [A loss: 0.789283, acc: 0.273438]\n",
      "741: [D loss: 0.693122, acc: 0.535156]  [A loss: 0.844490, acc: 0.191406]\n",
      "742: [D loss: 0.690404, acc: 0.544922]  [A loss: 0.828539, acc: 0.214844]\n",
      "743: [D loss: 0.693876, acc: 0.546875]  [A loss: 0.850853, acc: 0.187500]\n",
      "744: [D loss: 0.698639, acc: 0.507812]  [A loss: 0.864627, acc: 0.191406]\n",
      "745: [D loss: 0.684478, acc: 0.562500]  [A loss: 0.818430, acc: 0.289062]\n",
      "746: [D loss: 0.697832, acc: 0.529297]  [A loss: 0.898670, acc: 0.105469]\n",
      "747: [D loss: 0.691506, acc: 0.525391]  [A loss: 0.752047, acc: 0.390625]\n",
      "748: [D loss: 0.694886, acc: 0.517578]  [A loss: 0.920210, acc: 0.113281]\n",
      "749: [D loss: 0.692952, acc: 0.501953]  [A loss: 0.735664, acc: 0.410156]\n",
      "750: [D loss: 0.692956, acc: 0.544922]  [A loss: 0.963520, acc: 0.089844]\n",
      "751: [D loss: 0.689065, acc: 0.542969]  [A loss: 0.725994, acc: 0.417969]\n",
      "752: [D loss: 0.709862, acc: 0.529297]  [A loss: 0.988200, acc: 0.050781]\n",
      "753: [D loss: 0.696723, acc: 0.527344]  [A loss: 0.692507, acc: 0.507812]\n",
      "754: [D loss: 0.710594, acc: 0.509766]  [A loss: 0.923562, acc: 0.085938]\n",
      "755: [D loss: 0.699264, acc: 0.517578]  [A loss: 0.747143, acc: 0.386719]\n",
      "756: [D loss: 0.704823, acc: 0.548828]  [A loss: 0.921953, acc: 0.105469]\n",
      "757: [D loss: 0.684681, acc: 0.556641]  [A loss: 0.726596, acc: 0.449219]\n",
      "758: [D loss: 0.696285, acc: 0.539062]  [A loss: 0.872158, acc: 0.140625]\n",
      "759: [D loss: 0.696954, acc: 0.523438]  [A loss: 0.757850, acc: 0.359375]\n",
      "760: [D loss: 0.699228, acc: 0.537109]  [A loss: 0.967847, acc: 0.078125]\n",
      "761: [D loss: 0.678119, acc: 0.556641]  [A loss: 0.698226, acc: 0.519531]\n",
      "762: [D loss: 0.698140, acc: 0.539062]  [A loss: 0.939091, acc: 0.105469]\n",
      "763: [D loss: 0.696850, acc: 0.521484]  [A loss: 0.740737, acc: 0.371094]\n",
      "764: [D loss: 0.694795, acc: 0.517578]  [A loss: 0.910275, acc: 0.117188]\n",
      "765: [D loss: 0.678015, acc: 0.601562]  [A loss: 0.761112, acc: 0.343750]\n",
      "766: [D loss: 0.684952, acc: 0.580078]  [A loss: 0.863719, acc: 0.191406]\n",
      "767: [D loss: 0.684478, acc: 0.537109]  [A loss: 0.802797, acc: 0.277344]\n",
      "768: [D loss: 0.701123, acc: 0.523438]  [A loss: 0.892814, acc: 0.121094]\n",
      "769: [D loss: 0.698150, acc: 0.521484]  [A loss: 0.755369, acc: 0.335938]\n",
      "770: [D loss: 0.706479, acc: 0.533203]  [A loss: 0.929021, acc: 0.078125]\n",
      "771: [D loss: 0.679836, acc: 0.568359]  [A loss: 0.758961, acc: 0.335938]\n",
      "772: [D loss: 0.695841, acc: 0.525391]  [A loss: 0.944031, acc: 0.074219]\n",
      "773: [D loss: 0.673657, acc: 0.570312]  [A loss: 0.694190, acc: 0.523438]\n",
      "774: [D loss: 0.716621, acc: 0.513672]  [A loss: 0.934072, acc: 0.105469]\n",
      "775: [D loss: 0.694118, acc: 0.544922]  [A loss: 0.733388, acc: 0.410156]\n",
      "776: [D loss: 0.731318, acc: 0.478516]  [A loss: 0.908574, acc: 0.140625]\n",
      "777: [D loss: 0.684475, acc: 0.558594]  [A loss: 0.782555, acc: 0.320312]\n",
      "778: [D loss: 0.692210, acc: 0.513672]  [A loss: 0.855014, acc: 0.175781]\n",
      "779: [D loss: 0.696818, acc: 0.527344]  [A loss: 0.814545, acc: 0.269531]\n",
      "780: [D loss: 0.700041, acc: 0.517578]  [A loss: 0.884533, acc: 0.136719]\n",
      "781: [D loss: 0.679273, acc: 0.548828]  [A loss: 0.761983, acc: 0.382812]\n",
      "782: [D loss: 0.700591, acc: 0.511719]  [A loss: 0.910505, acc: 0.128906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783: [D loss: 0.671938, acc: 0.597656]  [A loss: 0.722044, acc: 0.445312]\n",
      "784: [D loss: 0.706946, acc: 0.503906]  [A loss: 0.943255, acc: 0.113281]\n",
      "785: [D loss: 0.685975, acc: 0.527344]  [A loss: 0.685410, acc: 0.546875]\n",
      "786: [D loss: 0.712200, acc: 0.533203]  [A loss: 0.940622, acc: 0.128906]\n",
      "787: [D loss: 0.697868, acc: 0.486328]  [A loss: 0.779093, acc: 0.335938]\n",
      "788: [D loss: 0.724266, acc: 0.470703]  [A loss: 0.855409, acc: 0.156250]\n",
      "789: [D loss: 0.696604, acc: 0.535156]  [A loss: 0.822607, acc: 0.226562]\n",
      "790: [D loss: 0.676617, acc: 0.580078]  [A loss: 0.760656, acc: 0.351562]\n",
      "791: [D loss: 0.702124, acc: 0.539062]  [A loss: 0.927726, acc: 0.105469]\n",
      "792: [D loss: 0.693369, acc: 0.537109]  [A loss: 0.767889, acc: 0.347656]\n",
      "793: [D loss: 0.704713, acc: 0.503906]  [A loss: 0.872146, acc: 0.152344]\n",
      "794: [D loss: 0.696070, acc: 0.523438]  [A loss: 0.781742, acc: 0.316406]\n",
      "795: [D loss: 0.689536, acc: 0.572266]  [A loss: 0.853420, acc: 0.218750]\n",
      "796: [D loss: 0.713211, acc: 0.484375]  [A loss: 0.753536, acc: 0.375000]\n",
      "797: [D loss: 0.705703, acc: 0.525391]  [A loss: 0.849832, acc: 0.207031]\n",
      "798: [D loss: 0.697634, acc: 0.556641]  [A loss: 0.850511, acc: 0.191406]\n",
      "799: [D loss: 0.687009, acc: 0.546875]  [A loss: 0.888464, acc: 0.140625]\n",
      "800: [D loss: 0.683620, acc: 0.548828]  [A loss: 0.809736, acc: 0.269531]\n",
      "801: [D loss: 0.697337, acc: 0.529297]  [A loss: 0.864496, acc: 0.175781]\n",
      "802: [D loss: 0.686013, acc: 0.537109]  [A loss: 0.807401, acc: 0.257812]\n",
      "803: [D loss: 0.689612, acc: 0.541016]  [A loss: 0.860792, acc: 0.175781]\n",
      "804: [D loss: 0.698948, acc: 0.517578]  [A loss: 0.797185, acc: 0.289062]\n",
      "805: [D loss: 0.712962, acc: 0.507812]  [A loss: 0.948400, acc: 0.085938]\n",
      "806: [D loss: 0.689736, acc: 0.533203]  [A loss: 0.795532, acc: 0.265625]\n",
      "807: [D loss: 0.697072, acc: 0.542969]  [A loss: 0.944149, acc: 0.093750]\n",
      "808: [D loss: 0.683582, acc: 0.546875]  [A loss: 0.768135, acc: 0.355469]\n",
      "809: [D loss: 0.713477, acc: 0.527344]  [A loss: 0.906187, acc: 0.125000]\n",
      "810: [D loss: 0.695400, acc: 0.533203]  [A loss: 0.796441, acc: 0.265625]\n",
      "811: [D loss: 0.688229, acc: 0.550781]  [A loss: 0.868499, acc: 0.160156]\n",
      "812: [D loss: 0.694857, acc: 0.529297]  [A loss: 0.818537, acc: 0.234375]\n",
      "813: [D loss: 0.708194, acc: 0.503906]  [A loss: 0.864023, acc: 0.175781]\n",
      "814: [D loss: 0.686248, acc: 0.560547]  [A loss: 0.772552, acc: 0.371094]\n",
      "815: [D loss: 0.689175, acc: 0.544922]  [A loss: 0.958438, acc: 0.101562]\n",
      "816: [D loss: 0.688512, acc: 0.556641]  [A loss: 0.723808, acc: 0.410156]\n",
      "817: [D loss: 0.711822, acc: 0.515625]  [A loss: 0.969998, acc: 0.085938]\n",
      "818: [D loss: 0.699371, acc: 0.500000]  [A loss: 0.699029, acc: 0.476562]\n",
      "819: [D loss: 0.711425, acc: 0.527344]  [A loss: 0.976057, acc: 0.089844]\n",
      "820: [D loss: 0.687542, acc: 0.535156]  [A loss: 0.712297, acc: 0.480469]\n",
      "821: [D loss: 0.717767, acc: 0.509766]  [A loss: 0.988871, acc: 0.050781]\n",
      "822: [D loss: 0.698640, acc: 0.519531]  [A loss: 0.703708, acc: 0.531250]\n",
      "823: [D loss: 0.703175, acc: 0.511719]  [A loss: 0.916007, acc: 0.085938]\n",
      "824: [D loss: 0.685924, acc: 0.537109]  [A loss: 0.725987, acc: 0.421875]\n",
      "825: [D loss: 0.708837, acc: 0.529297]  [A loss: 0.857593, acc: 0.175781]\n",
      "826: [D loss: 0.694819, acc: 0.535156]  [A loss: 0.771214, acc: 0.300781]\n",
      "827: [D loss: 0.695120, acc: 0.544922]  [A loss: 0.869372, acc: 0.167969]\n",
      "828: [D loss: 0.697984, acc: 0.521484]  [A loss: 0.781410, acc: 0.324219]\n",
      "829: [D loss: 0.686942, acc: 0.554688]  [A loss: 0.856308, acc: 0.175781]\n",
      "830: [D loss: 0.695734, acc: 0.535156]  [A loss: 0.828927, acc: 0.171875]\n",
      "831: [D loss: 0.686916, acc: 0.539062]  [A loss: 0.820552, acc: 0.222656]\n",
      "832: [D loss: 0.683527, acc: 0.560547]  [A loss: 0.857913, acc: 0.167969]\n",
      "833: [D loss: 0.698022, acc: 0.527344]  [A loss: 0.873299, acc: 0.160156]\n",
      "834: [D loss: 0.698138, acc: 0.537109]  [A loss: 0.795586, acc: 0.312500]\n",
      "835: [D loss: 0.688677, acc: 0.548828]  [A loss: 0.873474, acc: 0.187500]\n",
      "836: [D loss: 0.711447, acc: 0.496094]  [A loss: 0.879300, acc: 0.125000]\n",
      "837: [D loss: 0.699146, acc: 0.548828]  [A loss: 0.856908, acc: 0.183594]\n",
      "838: [D loss: 0.683196, acc: 0.550781]  [A loss: 0.818216, acc: 0.250000]\n",
      "839: [D loss: 0.681686, acc: 0.541016]  [A loss: 0.893167, acc: 0.160156]\n",
      "840: [D loss: 0.694699, acc: 0.529297]  [A loss: 0.860239, acc: 0.179688]\n",
      "841: [D loss: 0.695120, acc: 0.521484]  [A loss: 0.803811, acc: 0.285156]\n",
      "842: [D loss: 0.705540, acc: 0.515625]  [A loss: 0.896619, acc: 0.121094]\n",
      "843: [D loss: 0.703524, acc: 0.515625]  [A loss: 0.805747, acc: 0.269531]\n",
      "844: [D loss: 0.690638, acc: 0.570312]  [A loss: 0.932624, acc: 0.113281]\n",
      "845: [D loss: 0.694235, acc: 0.525391]  [A loss: 0.711555, acc: 0.480469]\n",
      "846: [D loss: 0.715240, acc: 0.513672]  [A loss: 1.022989, acc: 0.054688]\n",
      "847: [D loss: 0.685120, acc: 0.539062]  [A loss: 0.666154, acc: 0.574219]\n",
      "848: [D loss: 0.711937, acc: 0.511719]  [A loss: 0.984951, acc: 0.062500]\n",
      "849: [D loss: 0.695992, acc: 0.523438]  [A loss: 0.719601, acc: 0.445312]\n",
      "850: [D loss: 0.700853, acc: 0.521484]  [A loss: 0.898068, acc: 0.136719]\n",
      "851: [D loss: 0.685346, acc: 0.572266]  [A loss: 0.810535, acc: 0.242188]\n",
      "852: [D loss: 0.691773, acc: 0.550781]  [A loss: 0.860361, acc: 0.210938]\n",
      "853: [D loss: 0.697448, acc: 0.548828]  [A loss: 0.827225, acc: 0.246094]\n",
      "854: [D loss: 0.696148, acc: 0.546875]  [A loss: 0.810280, acc: 0.238281]\n",
      "855: [D loss: 0.688623, acc: 0.542969]  [A loss: 0.826596, acc: 0.214844]\n",
      "856: [D loss: 0.696802, acc: 0.535156]  [A loss: 0.813713, acc: 0.269531]\n",
      "857: [D loss: 0.697112, acc: 0.527344]  [A loss: 0.835468, acc: 0.195312]\n",
      "858: [D loss: 0.690916, acc: 0.533203]  [A loss: 0.840697, acc: 0.214844]\n",
      "859: [D loss: 0.699882, acc: 0.525391]  [A loss: 0.818340, acc: 0.242188]\n",
      "860: [D loss: 0.693903, acc: 0.529297]  [A loss: 0.907813, acc: 0.101562]\n",
      "861: [D loss: 0.689661, acc: 0.533203]  [A loss: 0.751048, acc: 0.406250]\n",
      "862: [D loss: 0.709737, acc: 0.523438]  [A loss: 0.977244, acc: 0.078125]\n",
      "863: [D loss: 0.703061, acc: 0.503906]  [A loss: 0.681843, acc: 0.535156]\n",
      "864: [D loss: 0.710838, acc: 0.527344]  [A loss: 0.936024, acc: 0.085938]\n",
      "865: [D loss: 0.684225, acc: 0.554688]  [A loss: 0.707024, acc: 0.480469]\n",
      "866: [D loss: 0.704124, acc: 0.519531]  [A loss: 0.921067, acc: 0.121094]\n",
      "867: [D loss: 0.681810, acc: 0.554688]  [A loss: 0.717686, acc: 0.460938]\n",
      "868: [D loss: 0.700005, acc: 0.515625]  [A loss: 0.901115, acc: 0.132812]\n",
      "869: [D loss: 0.693978, acc: 0.533203]  [A loss: 0.713878, acc: 0.492188]\n",
      "870: [D loss: 0.716953, acc: 0.529297]  [A loss: 0.978150, acc: 0.082031]\n",
      "871: [D loss: 0.700703, acc: 0.529297]  [A loss: 0.689099, acc: 0.511719]\n",
      "872: [D loss: 0.698030, acc: 0.492188]  [A loss: 0.887960, acc: 0.109375]\n",
      "873: [D loss: 0.698480, acc: 0.533203]  [A loss: 0.802197, acc: 0.242188]\n",
      "874: [D loss: 0.692361, acc: 0.554688]  [A loss: 0.798888, acc: 0.269531]\n",
      "875: [D loss: 0.695329, acc: 0.539062]  [A loss: 0.765613, acc: 0.378906]\n",
      "876: [D loss: 0.719527, acc: 0.484375]  [A loss: 0.884617, acc: 0.125000]\n",
      "877: [D loss: 0.685352, acc: 0.542969]  [A loss: 0.752718, acc: 0.378906]\n",
      "878: [D loss: 0.696491, acc: 0.544922]  [A loss: 0.880263, acc: 0.144531]\n",
      "879: [D loss: 0.703285, acc: 0.501953]  [A loss: 0.809670, acc: 0.246094]\n",
      "880: [D loss: 0.691928, acc: 0.517578]  [A loss: 0.824828, acc: 0.234375]\n",
      "881: [D loss: 0.700312, acc: 0.533203]  [A loss: 0.798369, acc: 0.269531]\n",
      "882: [D loss: 0.686015, acc: 0.562500]  [A loss: 0.786314, acc: 0.324219]\n",
      "883: [D loss: 0.707877, acc: 0.523438]  [A loss: 0.909291, acc: 0.136719]\n",
      "884: [D loss: 0.685800, acc: 0.529297]  [A loss: 0.715646, acc: 0.496094]\n",
      "885: [D loss: 0.701217, acc: 0.554688]  [A loss: 0.904615, acc: 0.121094]\n",
      "886: [D loss: 0.685645, acc: 0.552734]  [A loss: 0.795056, acc: 0.300781]\n",
      "887: [D loss: 0.715563, acc: 0.505859]  [A loss: 0.906628, acc: 0.113281]\n",
      "888: [D loss: 0.697298, acc: 0.513672]  [A loss: 0.731062, acc: 0.429688]\n",
      "889: [D loss: 0.692363, acc: 0.539062]  [A loss: 0.860072, acc: 0.167969]\n",
      "890: [D loss: 0.692615, acc: 0.552734]  [A loss: 0.758872, acc: 0.375000]\n",
      "891: [D loss: 0.689734, acc: 0.552734]  [A loss: 0.901551, acc: 0.148438]\n",
      "892: [D loss: 0.690850, acc: 0.527344]  [A loss: 0.779240, acc: 0.300781]\n",
      "893: [D loss: 0.695822, acc: 0.529297]  [A loss: 0.838806, acc: 0.234375]\n",
      "894: [D loss: 0.679336, acc: 0.562500]  [A loss: 0.796872, acc: 0.312500]\n",
      "895: [D loss: 0.715917, acc: 0.496094]  [A loss: 0.863508, acc: 0.164062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896: [D loss: 0.688301, acc: 0.541016]  [A loss: 0.806214, acc: 0.250000]\n",
      "897: [D loss: 0.697920, acc: 0.525391]  [A loss: 0.868556, acc: 0.191406]\n",
      "898: [D loss: 0.696711, acc: 0.537109]  [A loss: 0.787628, acc: 0.277344]\n",
      "899: [D loss: 0.691261, acc: 0.521484]  [A loss: 0.809578, acc: 0.289062]\n",
      "900: [D loss: 0.697690, acc: 0.509766]  [A loss: 0.889564, acc: 0.128906]\n",
      "901: [D loss: 0.686948, acc: 0.529297]  [A loss: 0.763477, acc: 0.386719]\n",
      "902: [D loss: 0.715947, acc: 0.509766]  [A loss: 0.959382, acc: 0.070312]\n",
      "903: [D loss: 0.684043, acc: 0.562500]  [A loss: 0.710276, acc: 0.460938]\n",
      "904: [D loss: 0.702879, acc: 0.541016]  [A loss: 0.935858, acc: 0.082031]\n",
      "905: [D loss: 0.690506, acc: 0.539062]  [A loss: 0.774344, acc: 0.351562]\n",
      "906: [D loss: 0.698668, acc: 0.507812]  [A loss: 0.816956, acc: 0.242188]\n",
      "907: [D loss: 0.696024, acc: 0.556641]  [A loss: 0.840512, acc: 0.203125]\n",
      "908: [D loss: 0.695016, acc: 0.548828]  [A loss: 0.804074, acc: 0.261719]\n",
      "909: [D loss: 0.688095, acc: 0.529297]  [A loss: 0.823396, acc: 0.257812]\n",
      "910: [D loss: 0.701441, acc: 0.519531]  [A loss: 0.835481, acc: 0.230469]\n",
      "911: [D loss: 0.688653, acc: 0.554688]  [A loss: 0.899007, acc: 0.144531]\n",
      "912: [D loss: 0.694090, acc: 0.560547]  [A loss: 0.774165, acc: 0.332031]\n",
      "913: [D loss: 0.694523, acc: 0.564453]  [A loss: 0.878831, acc: 0.164062]\n",
      "914: [D loss: 0.694540, acc: 0.529297]  [A loss: 0.797754, acc: 0.281250]\n",
      "915: [D loss: 0.698256, acc: 0.515625]  [A loss: 0.847968, acc: 0.203125]\n",
      "916: [D loss: 0.689640, acc: 0.542969]  [A loss: 0.768455, acc: 0.355469]\n",
      "917: [D loss: 0.704169, acc: 0.521484]  [A loss: 0.896926, acc: 0.136719]\n",
      "918: [D loss: 0.688774, acc: 0.542969]  [A loss: 0.842700, acc: 0.203125]\n",
      "919: [D loss: 0.694207, acc: 0.527344]  [A loss: 0.805002, acc: 0.218750]\n",
      "920: [D loss: 0.685880, acc: 0.554688]  [A loss: 0.881295, acc: 0.160156]\n",
      "921: [D loss: 0.689397, acc: 0.527344]  [A loss: 0.767956, acc: 0.324219]\n",
      "922: [D loss: 0.707821, acc: 0.529297]  [A loss: 0.882860, acc: 0.199219]\n",
      "923: [D loss: 0.687243, acc: 0.562500]  [A loss: 0.752156, acc: 0.355469]\n",
      "924: [D loss: 0.696483, acc: 0.529297]  [A loss: 0.941963, acc: 0.074219]\n",
      "925: [D loss: 0.691271, acc: 0.542969]  [A loss: 0.707751, acc: 0.480469]\n",
      "926: [D loss: 0.710176, acc: 0.523438]  [A loss: 0.931289, acc: 0.082031]\n",
      "927: [D loss: 0.679951, acc: 0.560547]  [A loss: 0.699360, acc: 0.531250]\n",
      "928: [D loss: 0.713070, acc: 0.505859]  [A loss: 0.948668, acc: 0.101562]\n",
      "929: [D loss: 0.690853, acc: 0.533203]  [A loss: 0.709645, acc: 0.445312]\n",
      "930: [D loss: 0.714873, acc: 0.494141]  [A loss: 0.864198, acc: 0.199219]\n",
      "931: [D loss: 0.685685, acc: 0.574219]  [A loss: 0.777936, acc: 0.324219]\n",
      "932: [D loss: 0.686068, acc: 0.546875]  [A loss: 0.827162, acc: 0.289062]\n",
      "933: [D loss: 0.688533, acc: 0.537109]  [A loss: 0.810377, acc: 0.285156]\n",
      "934: [D loss: 0.703584, acc: 0.501953]  [A loss: 0.836985, acc: 0.191406]\n",
      "935: [D loss: 0.688389, acc: 0.541016]  [A loss: 0.798381, acc: 0.265625]\n",
      "936: [D loss: 0.687696, acc: 0.552734]  [A loss: 0.883940, acc: 0.140625]\n",
      "937: [D loss: 0.688237, acc: 0.521484]  [A loss: 0.765015, acc: 0.359375]\n",
      "938: [D loss: 0.702835, acc: 0.531250]  [A loss: 0.883593, acc: 0.167969]\n",
      "939: [D loss: 0.698190, acc: 0.498047]  [A loss: 0.818018, acc: 0.250000]\n",
      "940: [D loss: 0.709504, acc: 0.507812]  [A loss: 0.857966, acc: 0.156250]\n",
      "941: [D loss: 0.681020, acc: 0.576172]  [A loss: 0.814050, acc: 0.304688]\n",
      "942: [D loss: 0.699640, acc: 0.515625]  [A loss: 0.848895, acc: 0.226562]\n",
      "943: [D loss: 0.697095, acc: 0.525391]  [A loss: 0.798110, acc: 0.308594]\n",
      "944: [D loss: 0.698067, acc: 0.529297]  [A loss: 0.841350, acc: 0.210938]\n",
      "945: [D loss: 0.701384, acc: 0.511719]  [A loss: 0.800210, acc: 0.269531]\n",
      "946: [D loss: 0.691323, acc: 0.529297]  [A loss: 0.925457, acc: 0.148438]\n",
      "947: [D loss: 0.687278, acc: 0.548828]  [A loss: 0.726977, acc: 0.449219]\n",
      "948: [D loss: 0.703104, acc: 0.527344]  [A loss: 0.915427, acc: 0.125000]\n",
      "949: [D loss: 0.684261, acc: 0.583984]  [A loss: 0.748747, acc: 0.359375]\n",
      "950: [D loss: 0.704723, acc: 0.521484]  [A loss: 0.974448, acc: 0.089844]\n",
      "951: [D loss: 0.696596, acc: 0.513672]  [A loss: 0.715449, acc: 0.457031]\n",
      "952: [D loss: 0.715980, acc: 0.529297]  [A loss: 0.951437, acc: 0.089844]\n",
      "953: [D loss: 0.694246, acc: 0.531250]  [A loss: 0.715649, acc: 0.468750]\n",
      "954: [D loss: 0.697864, acc: 0.541016]  [A loss: 0.918530, acc: 0.136719]\n",
      "955: [D loss: 0.702762, acc: 0.535156]  [A loss: 0.763234, acc: 0.363281]\n",
      "956: [D loss: 0.697264, acc: 0.533203]  [A loss: 0.860811, acc: 0.160156]\n",
      "957: [D loss: 0.696297, acc: 0.541016]  [A loss: 0.870372, acc: 0.222656]\n",
      "958: [D loss: 0.707207, acc: 0.492188]  [A loss: 0.821563, acc: 0.230469]\n",
      "959: [D loss: 0.689844, acc: 0.529297]  [A loss: 0.918569, acc: 0.132812]\n",
      "960: [D loss: 0.698555, acc: 0.519531]  [A loss: 0.772635, acc: 0.343750]\n",
      "961: [D loss: 0.710900, acc: 0.501953]  [A loss: 0.954530, acc: 0.109375]\n",
      "962: [D loss: 0.684466, acc: 0.564453]  [A loss: 0.677597, acc: 0.574219]\n",
      "963: [D loss: 0.715556, acc: 0.519531]  [A loss: 0.934055, acc: 0.105469]\n",
      "964: [D loss: 0.700699, acc: 0.509766]  [A loss: 0.711908, acc: 0.460938]\n",
      "965: [D loss: 0.704963, acc: 0.523438]  [A loss: 0.940911, acc: 0.078125]\n",
      "966: [D loss: 0.693461, acc: 0.550781]  [A loss: 0.731746, acc: 0.410156]\n",
      "967: [D loss: 0.694602, acc: 0.527344]  [A loss: 0.898178, acc: 0.152344]\n",
      "968: [D loss: 0.690023, acc: 0.558594]  [A loss: 0.776427, acc: 0.367188]\n",
      "969: [D loss: 0.715616, acc: 0.482422]  [A loss: 0.845523, acc: 0.175781]\n",
      "970: [D loss: 0.705380, acc: 0.505859]  [A loss: 0.815733, acc: 0.253906]\n",
      "971: [D loss: 0.699553, acc: 0.531250]  [A loss: 0.836669, acc: 0.234375]\n",
      "972: [D loss: 0.696043, acc: 0.544922]  [A loss: 0.816638, acc: 0.253906]\n",
      "973: [D loss: 0.695276, acc: 0.527344]  [A loss: 0.776016, acc: 0.359375]\n",
      "974: [D loss: 0.697340, acc: 0.525391]  [A loss: 0.839048, acc: 0.203125]\n",
      "975: [D loss: 0.683843, acc: 0.546875]  [A loss: 0.835079, acc: 0.250000]\n",
      "976: [D loss: 0.682814, acc: 0.519531]  [A loss: 0.768363, acc: 0.351562]\n",
      "977: [D loss: 0.700354, acc: 0.531250]  [A loss: 0.832916, acc: 0.210938]\n",
      "978: [D loss: 0.695964, acc: 0.517578]  [A loss: 0.813024, acc: 0.253906]\n",
      "979: [D loss: 0.701335, acc: 0.507812]  [A loss: 0.869780, acc: 0.136719]\n",
      "980: [D loss: 0.705053, acc: 0.509766]  [A loss: 0.812348, acc: 0.332031]\n",
      "981: [D loss: 0.712121, acc: 0.515625]  [A loss: 0.947499, acc: 0.097656]\n",
      "982: [D loss: 0.696238, acc: 0.525391]  [A loss: 0.766789, acc: 0.363281]\n",
      "983: [D loss: 0.697189, acc: 0.546875]  [A loss: 0.910227, acc: 0.109375]\n",
      "984: [D loss: 0.700808, acc: 0.509766]  [A loss: 0.705118, acc: 0.492188]\n",
      "985: [D loss: 0.705874, acc: 0.521484]  [A loss: 0.887201, acc: 0.156250]\n",
      "986: [D loss: 0.693942, acc: 0.539062]  [A loss: 0.781765, acc: 0.281250]\n",
      "987: [D loss: 0.708560, acc: 0.482422]  [A loss: 0.861901, acc: 0.152344]\n",
      "988: [D loss: 0.677203, acc: 0.582031]  [A loss: 0.752639, acc: 0.343750]\n",
      "989: [D loss: 0.703556, acc: 0.527344]  [A loss: 0.856872, acc: 0.199219]\n",
      "990: [D loss: 0.694372, acc: 0.560547]  [A loss: 0.795781, acc: 0.296875]\n",
      "991: [D loss: 0.708099, acc: 0.486328]  [A loss: 0.866256, acc: 0.156250]\n",
      "992: [D loss: 0.695251, acc: 0.521484]  [A loss: 0.778753, acc: 0.316406]\n",
      "993: [D loss: 0.693136, acc: 0.498047]  [A loss: 0.877049, acc: 0.160156]\n",
      "994: [D loss: 0.695978, acc: 0.529297]  [A loss: 0.839454, acc: 0.214844]\n",
      "995: [D loss: 0.694791, acc: 0.519531]  [A loss: 0.779542, acc: 0.328125]\n",
      "996: [D loss: 0.696522, acc: 0.548828]  [A loss: 0.910351, acc: 0.136719]\n",
      "997: [D loss: 0.695525, acc: 0.535156]  [A loss: 0.774288, acc: 0.324219]\n",
      "998: [D loss: 0.681396, acc: 0.572266]  [A loss: 0.867247, acc: 0.179688]\n",
      "999: [D loss: 0.707567, acc: 0.492188]  [A loss: 0.807663, acc: 0.226562]\n",
      "1000: [D loss: 0.678340, acc: 0.544922]  [A loss: 0.851765, acc: 0.183594]\n",
      "1001: [D loss: 0.692898, acc: 0.544922]  [A loss: 0.836149, acc: 0.250000]\n",
      "1002: [D loss: 0.696052, acc: 0.541016]  [A loss: 0.944516, acc: 0.105469]\n",
      "1003: [D loss: 0.701397, acc: 0.521484]  [A loss: 0.733360, acc: 0.382812]\n",
      "1004: [D loss: 0.713763, acc: 0.501953]  [A loss: 0.970038, acc: 0.062500]\n",
      "1005: [D loss: 0.699644, acc: 0.525391]  [A loss: 0.709409, acc: 0.492188]\n",
      "1006: [D loss: 0.700430, acc: 0.515625]  [A loss: 0.922694, acc: 0.109375]\n",
      "1007: [D loss: 0.693974, acc: 0.531250]  [A loss: 0.748107, acc: 0.375000]\n",
      "1008: [D loss: 0.700381, acc: 0.533203]  [A loss: 0.907751, acc: 0.101562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009: [D loss: 0.695051, acc: 0.535156]  [A loss: 0.800250, acc: 0.277344]\n",
      "1010: [D loss: 0.676206, acc: 0.548828]  [A loss: 0.808577, acc: 0.257812]\n",
      "1011: [D loss: 0.696402, acc: 0.525391]  [A loss: 0.901071, acc: 0.128906]\n",
      "1012: [D loss: 0.696815, acc: 0.500000]  [A loss: 0.779147, acc: 0.351562]\n",
      "1013: [D loss: 0.700291, acc: 0.523438]  [A loss: 0.854357, acc: 0.195312]\n",
      "1014: [D loss: 0.704539, acc: 0.486328]  [A loss: 0.811046, acc: 0.285156]\n",
      "1015: [D loss: 0.693936, acc: 0.523438]  [A loss: 0.752581, acc: 0.386719]\n",
      "1016: [D loss: 0.717485, acc: 0.503906]  [A loss: 0.956222, acc: 0.082031]\n",
      "1017: [D loss: 0.679933, acc: 0.560547]  [A loss: 0.703342, acc: 0.531250]\n",
      "1018: [D loss: 0.715180, acc: 0.531250]  [A loss: 0.953397, acc: 0.082031]\n",
      "1019: [D loss: 0.694212, acc: 0.527344]  [A loss: 0.740686, acc: 0.410156]\n",
      "1020: [D loss: 0.694880, acc: 0.533203]  [A loss: 0.860994, acc: 0.191406]\n",
      "1021: [D loss: 0.703887, acc: 0.511719]  [A loss: 0.809492, acc: 0.253906]\n",
      "1022: [D loss: 0.699681, acc: 0.539062]  [A loss: 0.909162, acc: 0.101562]\n",
      "1023: [D loss: 0.693449, acc: 0.527344]  [A loss: 0.762565, acc: 0.312500]\n",
      "1024: [D loss: 0.694581, acc: 0.535156]  [A loss: 0.950353, acc: 0.082031]\n",
      "1025: [D loss: 0.690902, acc: 0.546875]  [A loss: 0.738892, acc: 0.382812]\n",
      "1026: [D loss: 0.677939, acc: 0.572266]  [A loss: 0.901774, acc: 0.148438]\n",
      "1027: [D loss: 0.705188, acc: 0.498047]  [A loss: 0.730769, acc: 0.453125]\n",
      "1028: [D loss: 0.710470, acc: 0.519531]  [A loss: 0.886864, acc: 0.164062]\n",
      "1029: [D loss: 0.687736, acc: 0.535156]  [A loss: 0.717559, acc: 0.441406]\n",
      "1030: [D loss: 0.703975, acc: 0.511719]  [A loss: 0.909798, acc: 0.171875]\n",
      "1031: [D loss: 0.684570, acc: 0.562500]  [A loss: 0.710871, acc: 0.460938]\n",
      "1032: [D loss: 0.702284, acc: 0.498047]  [A loss: 0.964149, acc: 0.093750]\n",
      "1033: [D loss: 0.690917, acc: 0.537109]  [A loss: 0.694450, acc: 0.535156]\n",
      "1034: [D loss: 0.711368, acc: 0.523438]  [A loss: 0.885849, acc: 0.148438]\n",
      "1035: [D loss: 0.678482, acc: 0.576172]  [A loss: 0.703048, acc: 0.449219]\n",
      "1036: [D loss: 0.703814, acc: 0.542969]  [A loss: 0.916422, acc: 0.132812]\n",
      "1037: [D loss: 0.691127, acc: 0.537109]  [A loss: 0.729686, acc: 0.425781]\n",
      "1038: [D loss: 0.714883, acc: 0.519531]  [A loss: 0.924171, acc: 0.101562]\n",
      "1039: [D loss: 0.706755, acc: 0.486328]  [A loss: 0.732497, acc: 0.433594]\n",
      "1040: [D loss: 0.701122, acc: 0.517578]  [A loss: 0.883906, acc: 0.160156]\n",
      "1041: [D loss: 0.688633, acc: 0.556641]  [A loss: 0.737802, acc: 0.414062]\n",
      "1042: [D loss: 0.717462, acc: 0.488281]  [A loss: 0.895637, acc: 0.128906]\n",
      "1043: [D loss: 0.695941, acc: 0.546875]  [A loss: 0.754279, acc: 0.398438]\n",
      "1044: [D loss: 0.690519, acc: 0.533203]  [A loss: 0.886739, acc: 0.121094]\n",
      "1045: [D loss: 0.698046, acc: 0.527344]  [A loss: 0.735533, acc: 0.414062]\n",
      "1046: [D loss: 0.709899, acc: 0.486328]  [A loss: 0.824156, acc: 0.207031]\n",
      "1047: [D loss: 0.710475, acc: 0.490234]  [A loss: 0.859561, acc: 0.175781]\n",
      "1048: [D loss: 0.679507, acc: 0.541016]  [A loss: 0.718247, acc: 0.453125]\n",
      "1049: [D loss: 0.693966, acc: 0.527344]  [A loss: 0.917966, acc: 0.140625]\n",
      "1050: [D loss: 0.686252, acc: 0.544922]  [A loss: 0.739292, acc: 0.417969]\n",
      "1051: [D loss: 0.691582, acc: 0.548828]  [A loss: 0.861076, acc: 0.156250]\n",
      "1052: [D loss: 0.678059, acc: 0.607422]  [A loss: 0.762813, acc: 0.308594]\n",
      "1053: [D loss: 0.705211, acc: 0.517578]  [A loss: 0.866180, acc: 0.183594]\n",
      "1054: [D loss: 0.687617, acc: 0.580078]  [A loss: 0.796941, acc: 0.285156]\n",
      "1055: [D loss: 0.706997, acc: 0.531250]  [A loss: 0.922953, acc: 0.128906]\n",
      "1056: [D loss: 0.689664, acc: 0.539062]  [A loss: 0.737684, acc: 0.472656]\n",
      "1057: [D loss: 0.696163, acc: 0.525391]  [A loss: 0.912828, acc: 0.125000]\n",
      "1058: [D loss: 0.695016, acc: 0.541016]  [A loss: 0.747351, acc: 0.378906]\n",
      "1059: [D loss: 0.708070, acc: 0.503906]  [A loss: 0.916517, acc: 0.132812]\n",
      "1060: [D loss: 0.685728, acc: 0.558594]  [A loss: 0.724898, acc: 0.453125]\n",
      "1061: [D loss: 0.697633, acc: 0.541016]  [A loss: 0.949229, acc: 0.128906]\n",
      "1062: [D loss: 0.691706, acc: 0.552734]  [A loss: 0.712502, acc: 0.468750]\n",
      "1063: [D loss: 0.696992, acc: 0.541016]  [A loss: 0.829556, acc: 0.214844]\n",
      "1064: [D loss: 0.704198, acc: 0.525391]  [A loss: 0.848569, acc: 0.210938]\n",
      "1065: [D loss: 0.705154, acc: 0.496094]  [A loss: 0.842115, acc: 0.242188]\n",
      "1066: [D loss: 0.697565, acc: 0.515625]  [A loss: 0.748874, acc: 0.378906]\n",
      "1067: [D loss: 0.703056, acc: 0.531250]  [A loss: 0.924168, acc: 0.148438]\n",
      "1068: [D loss: 0.683825, acc: 0.578125]  [A loss: 0.789227, acc: 0.320312]\n",
      "1069: [D loss: 0.705266, acc: 0.517578]  [A loss: 0.890821, acc: 0.121094]\n",
      "1070: [D loss: 0.688688, acc: 0.558594]  [A loss: 0.720685, acc: 0.472656]\n",
      "1071: [D loss: 0.716210, acc: 0.501953]  [A loss: 0.922772, acc: 0.144531]\n",
      "1072: [D loss: 0.687811, acc: 0.552734]  [A loss: 0.725504, acc: 0.457031]\n",
      "1073: [D loss: 0.708842, acc: 0.517578]  [A loss: 0.917319, acc: 0.109375]\n",
      "1074: [D loss: 0.685673, acc: 0.542969]  [A loss: 0.730922, acc: 0.433594]\n",
      "1075: [D loss: 0.694222, acc: 0.544922]  [A loss: 0.883967, acc: 0.171875]\n",
      "1076: [D loss: 0.679521, acc: 0.585938]  [A loss: 0.712613, acc: 0.472656]\n",
      "1077: [D loss: 0.704728, acc: 0.533203]  [A loss: 0.893497, acc: 0.164062]\n",
      "1078: [D loss: 0.706010, acc: 0.500000]  [A loss: 0.766892, acc: 0.343750]\n",
      "1079: [D loss: 0.712319, acc: 0.474609]  [A loss: 0.906847, acc: 0.125000]\n",
      "1080: [D loss: 0.708892, acc: 0.470703]  [A loss: 0.775225, acc: 0.312500]\n",
      "1081: [D loss: 0.699906, acc: 0.541016]  [A loss: 0.898343, acc: 0.132812]\n",
      "1082: [D loss: 0.704778, acc: 0.513672]  [A loss: 0.781919, acc: 0.312500]\n",
      "1083: [D loss: 0.694019, acc: 0.552734]  [A loss: 0.856132, acc: 0.199219]\n",
      "1084: [D loss: 0.694995, acc: 0.533203]  [A loss: 0.748318, acc: 0.398438]\n",
      "1085: [D loss: 0.706037, acc: 0.501953]  [A loss: 0.912967, acc: 0.125000]\n",
      "1086: [D loss: 0.690153, acc: 0.537109]  [A loss: 0.759000, acc: 0.324219]\n",
      "1087: [D loss: 0.688195, acc: 0.550781]  [A loss: 0.888736, acc: 0.160156]\n",
      "1088: [D loss: 0.685293, acc: 0.554688]  [A loss: 0.758691, acc: 0.371094]\n",
      "1089: [D loss: 0.709530, acc: 0.515625]  [A loss: 0.901956, acc: 0.125000]\n",
      "1090: [D loss: 0.690558, acc: 0.541016]  [A loss: 0.757342, acc: 0.359375]\n",
      "1091: [D loss: 0.692906, acc: 0.554688]  [A loss: 0.868442, acc: 0.191406]\n",
      "1092: [D loss: 0.691403, acc: 0.548828]  [A loss: 0.736402, acc: 0.437500]\n",
      "1093: [D loss: 0.705229, acc: 0.527344]  [A loss: 0.931673, acc: 0.097656]\n",
      "1094: [D loss: 0.690846, acc: 0.537109]  [A loss: 0.700477, acc: 0.535156]\n",
      "1095: [D loss: 0.709352, acc: 0.515625]  [A loss: 0.895569, acc: 0.113281]\n",
      "1096: [D loss: 0.691932, acc: 0.527344]  [A loss: 0.775171, acc: 0.304688]\n",
      "1097: [D loss: 0.705223, acc: 0.542969]  [A loss: 0.945158, acc: 0.062500]\n",
      "1098: [D loss: 0.695074, acc: 0.535156]  [A loss: 0.708218, acc: 0.476562]\n",
      "1099: [D loss: 0.712952, acc: 0.521484]  [A loss: 0.916751, acc: 0.140625]\n",
      "1100: [D loss: 0.689733, acc: 0.519531]  [A loss: 0.717607, acc: 0.441406]\n",
      "1101: [D loss: 0.694895, acc: 0.529297]  [A loss: 0.902003, acc: 0.101562]\n",
      "1102: [D loss: 0.695251, acc: 0.550781]  [A loss: 0.731253, acc: 0.457031]\n",
      "1103: [D loss: 0.715752, acc: 0.507812]  [A loss: 0.959058, acc: 0.058594]\n",
      "1104: [D loss: 0.683267, acc: 0.566406]  [A loss: 0.740508, acc: 0.410156]\n",
      "1105: [D loss: 0.690757, acc: 0.564453]  [A loss: 0.869146, acc: 0.167969]\n",
      "1106: [D loss: 0.696800, acc: 0.511719]  [A loss: 0.828917, acc: 0.238281]\n",
      "1107: [D loss: 0.682726, acc: 0.583984]  [A loss: 0.794103, acc: 0.308594]\n",
      "1108: [D loss: 0.695300, acc: 0.527344]  [A loss: 0.851503, acc: 0.183594]\n",
      "1109: [D loss: 0.692888, acc: 0.533203]  [A loss: 0.797146, acc: 0.292969]\n",
      "1110: [D loss: 0.694179, acc: 0.525391]  [A loss: 0.744775, acc: 0.398438]\n",
      "1111: [D loss: 0.705824, acc: 0.500000]  [A loss: 0.883469, acc: 0.132812]\n",
      "1112: [D loss: 0.694341, acc: 0.541016]  [A loss: 0.852817, acc: 0.179688]\n",
      "1113: [D loss: 0.694491, acc: 0.546875]  [A loss: 0.805291, acc: 0.238281]\n",
      "1114: [D loss: 0.692014, acc: 0.560547]  [A loss: 0.901743, acc: 0.117188]\n",
      "1115: [D loss: 0.705266, acc: 0.500000]  [A loss: 0.800349, acc: 0.285156]\n",
      "1116: [D loss: 0.704761, acc: 0.498047]  [A loss: 0.881274, acc: 0.156250]\n",
      "1117: [D loss: 0.690436, acc: 0.539062]  [A loss: 0.806221, acc: 0.289062]\n",
      "1118: [D loss: 0.694117, acc: 0.541016]  [A loss: 0.860794, acc: 0.179688]\n",
      "1119: [D loss: 0.689183, acc: 0.531250]  [A loss: 0.726756, acc: 0.437500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120: [D loss: 0.694568, acc: 0.544922]  [A loss: 0.926248, acc: 0.113281]\n",
      "1121: [D loss: 0.689877, acc: 0.523438]  [A loss: 0.734940, acc: 0.410156]\n",
      "1122: [D loss: 0.701266, acc: 0.552734]  [A loss: 0.905387, acc: 0.113281]\n",
      "1123: [D loss: 0.689037, acc: 0.548828]  [A loss: 0.708187, acc: 0.507812]\n",
      "1124: [D loss: 0.700221, acc: 0.531250]  [A loss: 0.897633, acc: 0.136719]\n",
      "1125: [D loss: 0.709963, acc: 0.521484]  [A loss: 0.775653, acc: 0.312500]\n",
      "1126: [D loss: 0.702894, acc: 0.546875]  [A loss: 0.962493, acc: 0.097656]\n",
      "1127: [D loss: 0.697124, acc: 0.505859]  [A loss: 0.669843, acc: 0.593750]\n",
      "1128: [D loss: 0.720304, acc: 0.527344]  [A loss: 0.968299, acc: 0.070312]\n",
      "1129: [D loss: 0.691568, acc: 0.544922]  [A loss: 0.715150, acc: 0.449219]\n",
      "1130: [D loss: 0.716442, acc: 0.521484]  [A loss: 0.908583, acc: 0.132812]\n",
      "1131: [D loss: 0.696066, acc: 0.521484]  [A loss: 0.733934, acc: 0.437500]\n",
      "1132: [D loss: 0.715691, acc: 0.513672]  [A loss: 0.849780, acc: 0.226562]\n",
      "1133: [D loss: 0.709510, acc: 0.494141]  [A loss: 0.817598, acc: 0.210938]\n",
      "1134: [D loss: 0.684259, acc: 0.554688]  [A loss: 0.798635, acc: 0.296875]\n",
      "1135: [D loss: 0.687564, acc: 0.529297]  [A loss: 0.815526, acc: 0.265625]\n",
      "1136: [D loss: 0.693204, acc: 0.550781]  [A loss: 0.851815, acc: 0.164062]\n",
      "1137: [D loss: 0.681659, acc: 0.568359]  [A loss: 0.804608, acc: 0.269531]\n",
      "1138: [D loss: 0.700082, acc: 0.521484]  [A loss: 0.823164, acc: 0.234375]\n",
      "1139: [D loss: 0.700768, acc: 0.509766]  [A loss: 0.882004, acc: 0.171875]\n",
      "1140: [D loss: 0.693977, acc: 0.542969]  [A loss: 0.777180, acc: 0.367188]\n",
      "1141: [D loss: 0.695829, acc: 0.542969]  [A loss: 0.829622, acc: 0.187500]\n",
      "1142: [D loss: 0.694872, acc: 0.529297]  [A loss: 0.861723, acc: 0.167969]\n",
      "1143: [D loss: 0.696211, acc: 0.558594]  [A loss: 0.851228, acc: 0.183594]\n",
      "1144: [D loss: 0.681727, acc: 0.525391]  [A loss: 0.782010, acc: 0.351562]\n",
      "1145: [D loss: 0.703154, acc: 0.533203]  [A loss: 0.872184, acc: 0.191406]\n",
      "1146: [D loss: 0.695634, acc: 0.519531]  [A loss: 0.771279, acc: 0.375000]\n",
      "1147: [D loss: 0.706533, acc: 0.500000]  [A loss: 0.903551, acc: 0.140625]\n",
      "1148: [D loss: 0.704116, acc: 0.521484]  [A loss: 0.795815, acc: 0.320312]\n",
      "1149: [D loss: 0.697194, acc: 0.511719]  [A loss: 0.781851, acc: 0.304688]\n",
      "1150: [D loss: 0.730727, acc: 0.484375]  [A loss: 0.925343, acc: 0.097656]\n",
      "1151: [D loss: 0.700581, acc: 0.541016]  [A loss: 0.812255, acc: 0.246094]\n",
      "1152: [D loss: 0.684207, acc: 0.546875]  [A loss: 0.861054, acc: 0.175781]\n",
      "1153: [D loss: 0.685955, acc: 0.539062]  [A loss: 0.782722, acc: 0.367188]\n",
      "1154: [D loss: 0.696023, acc: 0.531250]  [A loss: 0.938934, acc: 0.109375]\n",
      "1155: [D loss: 0.680014, acc: 0.556641]  [A loss: 0.791790, acc: 0.316406]\n",
      "1156: [D loss: 0.697155, acc: 0.533203]  [A loss: 0.846019, acc: 0.214844]\n",
      "1157: [D loss: 0.689585, acc: 0.560547]  [A loss: 0.788189, acc: 0.320312]\n",
      "1158: [D loss: 0.700102, acc: 0.523438]  [A loss: 0.980178, acc: 0.074219]\n",
      "1159: [D loss: 0.695611, acc: 0.525391]  [A loss: 0.705995, acc: 0.488281]\n",
      "1160: [D loss: 0.718443, acc: 0.517578]  [A loss: 1.004280, acc: 0.085938]\n",
      "1161: [D loss: 0.698660, acc: 0.505859]  [A loss: 0.723097, acc: 0.449219]\n",
      "1162: [D loss: 0.710271, acc: 0.513672]  [A loss: 0.941730, acc: 0.093750]\n",
      "1163: [D loss: 0.677891, acc: 0.582031]  [A loss: 0.742293, acc: 0.425781]\n",
      "1164: [D loss: 0.695075, acc: 0.552734]  [A loss: 0.905857, acc: 0.152344]\n",
      "1165: [D loss: 0.697751, acc: 0.525391]  [A loss: 0.746352, acc: 0.394531]\n",
      "1166: [D loss: 0.702000, acc: 0.541016]  [A loss: 0.872862, acc: 0.203125]\n",
      "1167: [D loss: 0.689423, acc: 0.556641]  [A loss: 0.790789, acc: 0.281250]\n",
      "1168: [D loss: 0.711863, acc: 0.507812]  [A loss: 0.855878, acc: 0.222656]\n",
      "1169: [D loss: 0.710359, acc: 0.505859]  [A loss: 0.828855, acc: 0.234375]\n",
      "1170: [D loss: 0.684232, acc: 0.568359]  [A loss: 0.851644, acc: 0.230469]\n",
      "1171: [D loss: 0.705639, acc: 0.507812]  [A loss: 0.814752, acc: 0.218750]\n",
      "1172: [D loss: 0.690163, acc: 0.550781]  [A loss: 0.833808, acc: 0.269531]\n",
      "1173: [D loss: 0.688870, acc: 0.529297]  [A loss: 0.877805, acc: 0.187500]\n",
      "1174: [D loss: 0.698150, acc: 0.519531]  [A loss: 0.816650, acc: 0.222656]\n",
      "1175: [D loss: 0.700371, acc: 0.507812]  [A loss: 0.861745, acc: 0.179688]\n",
      "1176: [D loss: 0.689172, acc: 0.544922]  [A loss: 0.768485, acc: 0.355469]\n",
      "1177: [D loss: 0.698483, acc: 0.531250]  [A loss: 0.937369, acc: 0.089844]\n",
      "1178: [D loss: 0.688382, acc: 0.529297]  [A loss: 0.801931, acc: 0.300781]\n",
      "1179: [D loss: 0.699687, acc: 0.544922]  [A loss: 0.935433, acc: 0.097656]\n",
      "1180: [D loss: 0.715173, acc: 0.488281]  [A loss: 0.826262, acc: 0.234375]\n",
      "1181: [D loss: 0.691676, acc: 0.529297]  [A loss: 0.846130, acc: 0.222656]\n",
      "1182: [D loss: 0.690990, acc: 0.537109]  [A loss: 0.795966, acc: 0.277344]\n",
      "1183: [D loss: 0.699325, acc: 0.535156]  [A loss: 0.928551, acc: 0.125000]\n",
      "1184: [D loss: 0.690369, acc: 0.558594]  [A loss: 0.762844, acc: 0.347656]\n",
      "1185: [D loss: 0.707581, acc: 0.525391]  [A loss: 0.941292, acc: 0.105469]\n",
      "1186: [D loss: 0.684004, acc: 0.560547]  [A loss: 0.746351, acc: 0.441406]\n",
      "1187: [D loss: 0.700791, acc: 0.539062]  [A loss: 0.947336, acc: 0.132812]\n",
      "1188: [D loss: 0.688232, acc: 0.541016]  [A loss: 0.698574, acc: 0.488281]\n",
      "1189: [D loss: 0.741016, acc: 0.515625]  [A loss: 1.030365, acc: 0.066406]\n",
      "1190: [D loss: 0.702676, acc: 0.519531]  [A loss: 0.687621, acc: 0.535156]\n",
      "1191: [D loss: 0.716291, acc: 0.517578]  [A loss: 0.952756, acc: 0.085938]\n",
      "1192: [D loss: 0.701228, acc: 0.509766]  [A loss: 0.715435, acc: 0.476562]\n",
      "1193: [D loss: 0.708195, acc: 0.529297]  [A loss: 0.847357, acc: 0.183594]\n",
      "1194: [D loss: 0.691291, acc: 0.552734]  [A loss: 0.758883, acc: 0.343750]\n",
      "1195: [D loss: 0.692078, acc: 0.533203]  [A loss: 0.870378, acc: 0.140625]\n",
      "1196: [D loss: 0.682360, acc: 0.570312]  [A loss: 0.805929, acc: 0.246094]\n",
      "1197: [D loss: 0.709312, acc: 0.501953]  [A loss: 0.861803, acc: 0.164062]\n",
      "1198: [D loss: 0.686259, acc: 0.552734]  [A loss: 0.816455, acc: 0.308594]\n",
      "1199: [D loss: 0.702646, acc: 0.535156]  [A loss: 0.853023, acc: 0.191406]\n",
      "1200: [D loss: 0.682886, acc: 0.541016]  [A loss: 0.832424, acc: 0.238281]\n",
      "1201: [D loss: 0.705284, acc: 0.529297]  [A loss: 0.810061, acc: 0.285156]\n",
      "1202: [D loss: 0.685720, acc: 0.560547]  [A loss: 0.821385, acc: 0.273438]\n",
      "1203: [D loss: 0.669363, acc: 0.580078]  [A loss: 0.849407, acc: 0.257812]\n",
      "1204: [D loss: 0.695789, acc: 0.527344]  [A loss: 0.779696, acc: 0.335938]\n",
      "1205: [D loss: 0.704048, acc: 0.527344]  [A loss: 0.897826, acc: 0.144531]\n",
      "1206: [D loss: 0.705967, acc: 0.482422]  [A loss: 0.751932, acc: 0.406250]\n",
      "1207: [D loss: 0.710368, acc: 0.509766]  [A loss: 0.940159, acc: 0.117188]\n",
      "1208: [D loss: 0.692567, acc: 0.529297]  [A loss: 0.791234, acc: 0.292969]\n",
      "1209: [D loss: 0.707897, acc: 0.515625]  [A loss: 0.931470, acc: 0.125000]\n",
      "1210: [D loss: 0.699907, acc: 0.529297]  [A loss: 0.741615, acc: 0.390625]\n",
      "1211: [D loss: 0.712764, acc: 0.501953]  [A loss: 1.020753, acc: 0.054688]\n",
      "1212: [D loss: 0.703549, acc: 0.486328]  [A loss: 0.638121, acc: 0.664062]\n",
      "1213: [D loss: 0.741665, acc: 0.488281]  [A loss: 0.981905, acc: 0.082031]\n",
      "1214: [D loss: 0.695247, acc: 0.503906]  [A loss: 0.678925, acc: 0.554688]\n",
      "1215: [D loss: 0.736771, acc: 0.500000]  [A loss: 0.911947, acc: 0.097656]\n",
      "1216: [D loss: 0.701435, acc: 0.501953]  [A loss: 0.758851, acc: 0.328125]\n",
      "1217: [D loss: 0.701698, acc: 0.542969]  [A loss: 0.835359, acc: 0.234375]\n",
      "1218: [D loss: 0.694197, acc: 0.523438]  [A loss: 0.768459, acc: 0.343750]\n",
      "1219: [D loss: 0.696029, acc: 0.546875]  [A loss: 0.841789, acc: 0.207031]\n",
      "1220: [D loss: 0.707813, acc: 0.498047]  [A loss: 0.835168, acc: 0.226562]\n",
      "1221: [D loss: 0.698076, acc: 0.523438]  [A loss: 0.762095, acc: 0.398438]\n",
      "1222: [D loss: 0.696121, acc: 0.521484]  [A loss: 0.888564, acc: 0.156250]\n",
      "1223: [D loss: 0.688582, acc: 0.513672]  [A loss: 0.731991, acc: 0.425781]\n",
      "1224: [D loss: 0.710543, acc: 0.521484]  [A loss: 0.893522, acc: 0.136719]\n",
      "1225: [D loss: 0.697647, acc: 0.509766]  [A loss: 0.762312, acc: 0.308594]\n",
      "1226: [D loss: 0.699920, acc: 0.541016]  [A loss: 0.837497, acc: 0.191406]\n",
      "1227: [D loss: 0.699387, acc: 0.531250]  [A loss: 0.788094, acc: 0.292969]\n",
      "1228: [D loss: 0.709496, acc: 0.513672]  [A loss: 0.855702, acc: 0.171875]\n",
      "1229: [D loss: 0.697433, acc: 0.548828]  [A loss: 0.874432, acc: 0.187500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1230: [D loss: 0.709045, acc: 0.492188]  [A loss: 0.810815, acc: 0.277344]\n",
      "1231: [D loss: 0.697675, acc: 0.527344]  [A loss: 0.873804, acc: 0.179688]\n",
      "1232: [D loss: 0.711265, acc: 0.478516]  [A loss: 0.802182, acc: 0.285156]\n",
      "1233: [D loss: 0.700574, acc: 0.509766]  [A loss: 0.874488, acc: 0.152344]\n",
      "1234: [D loss: 0.694124, acc: 0.527344]  [A loss: 0.826034, acc: 0.214844]\n",
      "1235: [D loss: 0.710664, acc: 0.523438]  [A loss: 0.963978, acc: 0.085938]\n",
      "1236: [D loss: 0.706341, acc: 0.523438]  [A loss: 0.694875, acc: 0.515625]\n",
      "1237: [D loss: 0.705455, acc: 0.523438]  [A loss: 0.980866, acc: 0.085938]\n",
      "1238: [D loss: 0.701483, acc: 0.517578]  [A loss: 0.664618, acc: 0.585938]\n",
      "1239: [D loss: 0.725836, acc: 0.507812]  [A loss: 0.985820, acc: 0.078125]\n",
      "1240: [D loss: 0.700815, acc: 0.511719]  [A loss: 0.722301, acc: 0.445312]\n",
      "1241: [D loss: 0.706692, acc: 0.517578]  [A loss: 0.827586, acc: 0.226562]\n",
      "1242: [D loss: 0.683008, acc: 0.572266]  [A loss: 0.782381, acc: 0.300781]\n",
      "1243: [D loss: 0.690216, acc: 0.544922]  [A loss: 0.836441, acc: 0.238281]\n",
      "1244: [D loss: 0.691456, acc: 0.556641]  [A loss: 0.746634, acc: 0.414062]\n",
      "1245: [D loss: 0.696500, acc: 0.531250]  [A loss: 0.815361, acc: 0.238281]\n",
      "1246: [D loss: 0.701965, acc: 0.525391]  [A loss: 0.833469, acc: 0.222656]\n",
      "1247: [D loss: 0.677902, acc: 0.562500]  [A loss: 0.810110, acc: 0.269531]\n",
      "1248: [D loss: 0.697743, acc: 0.544922]  [A loss: 0.858347, acc: 0.187500]\n",
      "1249: [D loss: 0.698970, acc: 0.531250]  [A loss: 0.822027, acc: 0.222656]\n",
      "1250: [D loss: 0.694244, acc: 0.519531]  [A loss: 0.794960, acc: 0.320312]\n",
      "1251: [D loss: 0.697451, acc: 0.513672]  [A loss: 0.886012, acc: 0.171875]\n",
      "1252: [D loss: 0.674161, acc: 0.595703]  [A loss: 0.820602, acc: 0.218750]\n",
      "1253: [D loss: 0.706844, acc: 0.505859]  [A loss: 0.855217, acc: 0.179688]\n",
      "1254: [D loss: 0.702836, acc: 0.531250]  [A loss: 0.839120, acc: 0.234375]\n",
      "1255: [D loss: 0.700832, acc: 0.533203]  [A loss: 0.741642, acc: 0.414062]\n",
      "1256: [D loss: 0.706279, acc: 0.523438]  [A loss: 0.876252, acc: 0.160156]\n",
      "1257: [D loss: 0.696571, acc: 0.527344]  [A loss: 0.747511, acc: 0.390625]\n",
      "1258: [D loss: 0.719938, acc: 0.507812]  [A loss: 0.973044, acc: 0.070312]\n",
      "1259: [D loss: 0.695460, acc: 0.544922]  [A loss: 0.736985, acc: 0.417969]\n",
      "1260: [D loss: 0.694538, acc: 0.541016]  [A loss: 0.899367, acc: 0.109375]\n",
      "1261: [D loss: 0.682089, acc: 0.558594]  [A loss: 0.745091, acc: 0.394531]\n",
      "1262: [D loss: 0.712198, acc: 0.511719]  [A loss: 0.952183, acc: 0.066406]\n",
      "1263: [D loss: 0.696638, acc: 0.539062]  [A loss: 0.703087, acc: 0.480469]\n",
      "1264: [D loss: 0.733130, acc: 0.519531]  [A loss: 0.927279, acc: 0.152344]\n",
      "1265: [D loss: 0.694762, acc: 0.541016]  [A loss: 0.786969, acc: 0.296875]\n",
      "1266: [D loss: 0.697321, acc: 0.525391]  [A loss: 0.850270, acc: 0.167969]\n",
      "1267: [D loss: 0.689155, acc: 0.564453]  [A loss: 0.805845, acc: 0.269531]\n",
      "1268: [D loss: 0.693017, acc: 0.537109]  [A loss: 0.866943, acc: 0.160156]\n",
      "1269: [D loss: 0.695495, acc: 0.523438]  [A loss: 0.839493, acc: 0.195312]\n",
      "1270: [D loss: 0.690076, acc: 0.548828]  [A loss: 0.876181, acc: 0.183594]\n",
      "1271: [D loss: 0.678466, acc: 0.593750]  [A loss: 0.739311, acc: 0.433594]\n",
      "1272: [D loss: 0.695205, acc: 0.529297]  [A loss: 0.883277, acc: 0.203125]\n",
      "1273: [D loss: 0.693865, acc: 0.558594]  [A loss: 0.811060, acc: 0.265625]\n",
      "1274: [D loss: 0.705373, acc: 0.501953]  [A loss: 0.866346, acc: 0.203125]\n",
      "1275: [D loss: 0.689749, acc: 0.539062]  [A loss: 0.823913, acc: 0.265625]\n",
      "1276: [D loss: 0.718453, acc: 0.494141]  [A loss: 0.878404, acc: 0.199219]\n",
      "1277: [D loss: 0.690617, acc: 0.521484]  [A loss: 0.800085, acc: 0.289062]\n",
      "1278: [D loss: 0.706623, acc: 0.513672]  [A loss: 0.942661, acc: 0.105469]\n",
      "1279: [D loss: 0.691891, acc: 0.533203]  [A loss: 0.705857, acc: 0.480469]\n",
      "1280: [D loss: 0.733739, acc: 0.492188]  [A loss: 1.061501, acc: 0.042969]\n",
      "1281: [D loss: 0.697908, acc: 0.535156]  [A loss: 0.633457, acc: 0.679688]\n",
      "1282: [D loss: 0.739032, acc: 0.501953]  [A loss: 0.934446, acc: 0.125000]\n",
      "1283: [D loss: 0.700934, acc: 0.503906]  [A loss: 0.759670, acc: 0.371094]\n",
      "1284: [D loss: 0.711340, acc: 0.523438]  [A loss: 0.829380, acc: 0.230469]\n",
      "1285: [D loss: 0.693059, acc: 0.548828]  [A loss: 0.867183, acc: 0.191406]\n",
      "1286: [D loss: 0.698971, acc: 0.513672]  [A loss: 0.800030, acc: 0.289062]\n",
      "1287: [D loss: 0.687885, acc: 0.574219]  [A loss: 0.868938, acc: 0.242188]\n",
      "1288: [D loss: 0.704600, acc: 0.505859]  [A loss: 0.787265, acc: 0.296875]\n",
      "1289: [D loss: 0.699515, acc: 0.554688]  [A loss: 0.849271, acc: 0.207031]\n",
      "1290: [D loss: 0.680302, acc: 0.554688]  [A loss: 0.832358, acc: 0.222656]\n",
      "1291: [D loss: 0.709558, acc: 0.517578]  [A loss: 0.856146, acc: 0.218750]\n",
      "1292: [D loss: 0.692915, acc: 0.529297]  [A loss: 0.884687, acc: 0.187500]\n",
      "1293: [D loss: 0.694962, acc: 0.542969]  [A loss: 0.822278, acc: 0.253906]\n",
      "1294: [D loss: 0.714628, acc: 0.498047]  [A loss: 0.872335, acc: 0.210938]\n",
      "1295: [D loss: 0.694152, acc: 0.533203]  [A loss: 0.701490, acc: 0.519531]\n",
      "1296: [D loss: 0.707062, acc: 0.507812]  [A loss: 1.041129, acc: 0.062500]\n",
      "1297: [D loss: 0.686911, acc: 0.550781]  [A loss: 0.714844, acc: 0.468750]\n",
      "1298: [D loss: 0.719781, acc: 0.525391]  [A loss: 1.000763, acc: 0.062500]\n",
      "1299: [D loss: 0.691642, acc: 0.568359]  [A loss: 0.715657, acc: 0.488281]\n",
      "1300: [D loss: 0.725314, acc: 0.511719]  [A loss: 0.923066, acc: 0.144531]\n",
      "1301: [D loss: 0.691275, acc: 0.511719]  [A loss: 0.719319, acc: 0.425781]\n",
      "1302: [D loss: 0.715997, acc: 0.498047]  [A loss: 0.830865, acc: 0.234375]\n",
      "1303: [D loss: 0.687639, acc: 0.550781]  [A loss: 0.777028, acc: 0.328125]\n",
      "1304: [D loss: 0.708152, acc: 0.517578]  [A loss: 0.862639, acc: 0.175781]\n",
      "1305: [D loss: 0.700477, acc: 0.519531]  [A loss: 0.791194, acc: 0.316406]\n",
      "1306: [D loss: 0.703557, acc: 0.509766]  [A loss: 0.849685, acc: 0.199219]\n",
      "1307: [D loss: 0.694654, acc: 0.513672]  [A loss: 0.829732, acc: 0.246094]\n",
      "1308: [D loss: 0.700919, acc: 0.511719]  [A loss: 0.762969, acc: 0.378906]\n",
      "1309: [D loss: 0.695862, acc: 0.558594]  [A loss: 0.832208, acc: 0.246094]\n",
      "1310: [D loss: 0.706780, acc: 0.503906]  [A loss: 0.802292, acc: 0.265625]\n",
      "1311: [D loss: 0.704719, acc: 0.501953]  [A loss: 0.930160, acc: 0.101562]\n",
      "1312: [D loss: 0.690311, acc: 0.529297]  [A loss: 0.726431, acc: 0.425781]\n",
      "1313: [D loss: 0.716889, acc: 0.521484]  [A loss: 0.922318, acc: 0.101562]\n",
      "1314: [D loss: 0.703502, acc: 0.501953]  [A loss: 0.720455, acc: 0.453125]\n",
      "1315: [D loss: 0.717010, acc: 0.501953]  [A loss: 0.985752, acc: 0.082031]\n",
      "1316: [D loss: 0.687369, acc: 0.531250]  [A loss: 0.711107, acc: 0.484375]\n",
      "1317: [D loss: 0.708257, acc: 0.521484]  [A loss: 0.878110, acc: 0.140625]\n",
      "1318: [D loss: 0.699833, acc: 0.527344]  [A loss: 0.756792, acc: 0.367188]\n",
      "1319: [D loss: 0.695181, acc: 0.537109]  [A loss: 0.801967, acc: 0.292969]\n",
      "1320: [D loss: 0.695534, acc: 0.533203]  [A loss: 0.773635, acc: 0.320312]\n",
      "1321: [D loss: 0.701220, acc: 0.503906]  [A loss: 0.877222, acc: 0.187500]\n",
      "1322: [D loss: 0.704594, acc: 0.503906]  [A loss: 0.864498, acc: 0.183594]\n",
      "1323: [D loss: 0.690225, acc: 0.537109]  [A loss: 0.822116, acc: 0.238281]\n",
      "1324: [D loss: 0.689522, acc: 0.546875]  [A loss: 0.822467, acc: 0.250000]\n",
      "1325: [D loss: 0.692587, acc: 0.556641]  [A loss: 0.850290, acc: 0.195312]\n",
      "1326: [D loss: 0.688646, acc: 0.525391]  [A loss: 0.750887, acc: 0.367188]\n",
      "1327: [D loss: 0.716825, acc: 0.496094]  [A loss: 0.939871, acc: 0.121094]\n",
      "1328: [D loss: 0.693455, acc: 0.529297]  [A loss: 0.654630, acc: 0.593750]\n",
      "1329: [D loss: 0.741321, acc: 0.519531]  [A loss: 1.069239, acc: 0.031250]\n",
      "1330: [D loss: 0.701014, acc: 0.531250]  [A loss: 0.721868, acc: 0.429688]\n",
      "1331: [D loss: 0.709193, acc: 0.513672]  [A loss: 0.920395, acc: 0.132812]\n",
      "1332: [D loss: 0.706018, acc: 0.480469]  [A loss: 0.781013, acc: 0.339844]\n",
      "1333: [D loss: 0.714832, acc: 0.541016]  [A loss: 0.864309, acc: 0.179688]\n",
      "1334: [D loss: 0.687650, acc: 0.541016]  [A loss: 0.801108, acc: 0.289062]\n",
      "1335: [D loss: 0.699411, acc: 0.517578]  [A loss: 0.881813, acc: 0.171875]\n",
      "1336: [D loss: 0.693407, acc: 0.539062]  [A loss: 0.754638, acc: 0.375000]\n",
      "1337: [D loss: 0.707391, acc: 0.488281]  [A loss: 0.899477, acc: 0.132812]\n",
      "1338: [D loss: 0.688583, acc: 0.531250]  [A loss: 0.753478, acc: 0.367188]\n",
      "1339: [D loss: 0.696238, acc: 0.537109]  [A loss: 0.872382, acc: 0.199219]\n",
      "1340: [D loss: 0.688862, acc: 0.556641]  [A loss: 0.801347, acc: 0.308594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1341: [D loss: 0.700695, acc: 0.517578]  [A loss: 0.857126, acc: 0.207031]\n",
      "1342: [D loss: 0.686450, acc: 0.556641]  [A loss: 0.817956, acc: 0.218750]\n",
      "1343: [D loss: 0.701854, acc: 0.525391]  [A loss: 0.788917, acc: 0.304688]\n",
      "1344: [D loss: 0.690489, acc: 0.556641]  [A loss: 0.868134, acc: 0.199219]\n",
      "1345: [D loss: 0.695478, acc: 0.542969]  [A loss: 0.775326, acc: 0.351562]\n",
      "1346: [D loss: 0.698071, acc: 0.564453]  [A loss: 0.911434, acc: 0.113281]\n",
      "1347: [D loss: 0.687350, acc: 0.509766]  [A loss: 0.758076, acc: 0.421875]\n",
      "1348: [D loss: 0.699461, acc: 0.533203]  [A loss: 0.847911, acc: 0.226562]\n",
      "1349: [D loss: 0.700516, acc: 0.517578]  [A loss: 0.735563, acc: 0.460938]\n",
      "1350: [D loss: 0.710009, acc: 0.539062]  [A loss: 0.950441, acc: 0.113281]\n",
      "1351: [D loss: 0.701796, acc: 0.519531]  [A loss: 0.693443, acc: 0.550781]\n",
      "1352: [D loss: 0.720222, acc: 0.509766]  [A loss: 1.013639, acc: 0.070312]\n",
      "1353: [D loss: 0.697744, acc: 0.515625]  [A loss: 0.673573, acc: 0.566406]\n",
      "1354: [D loss: 0.715158, acc: 0.519531]  [A loss: 0.969375, acc: 0.070312]\n",
      "1355: [D loss: 0.701432, acc: 0.519531]  [A loss: 0.739443, acc: 0.433594]\n",
      "1356: [D loss: 0.694816, acc: 0.525391]  [A loss: 0.946357, acc: 0.113281]\n",
      "1357: [D loss: 0.703370, acc: 0.544922]  [A loss: 0.762699, acc: 0.386719]\n",
      "1358: [D loss: 0.708309, acc: 0.517578]  [A loss: 0.934982, acc: 0.125000]\n",
      "1359: [D loss: 0.692109, acc: 0.546875]  [A loss: 0.754098, acc: 0.367188]\n",
      "1360: [D loss: 0.723635, acc: 0.505859]  [A loss: 0.928604, acc: 0.117188]\n",
      "1361: [D loss: 0.682483, acc: 0.562500]  [A loss: 0.780865, acc: 0.320312]\n",
      "1362: [D loss: 0.709295, acc: 0.515625]  [A loss: 0.904843, acc: 0.144531]\n",
      "1363: [D loss: 0.681997, acc: 0.556641]  [A loss: 0.769121, acc: 0.343750]\n",
      "1364: [D loss: 0.709310, acc: 0.515625]  [A loss: 0.868356, acc: 0.226562]\n",
      "1365: [D loss: 0.690846, acc: 0.523438]  [A loss: 0.786896, acc: 0.320312]\n",
      "1366: [D loss: 0.694981, acc: 0.535156]  [A loss: 0.855902, acc: 0.203125]\n",
      "1367: [D loss: 0.692638, acc: 0.537109]  [A loss: 0.794354, acc: 0.308594]\n",
      "1368: [D loss: 0.701938, acc: 0.535156]  [A loss: 0.852216, acc: 0.230469]\n",
      "1369: [D loss: 0.697989, acc: 0.542969]  [A loss: 0.833132, acc: 0.246094]\n",
      "1370: [D loss: 0.685604, acc: 0.568359]  [A loss: 0.811713, acc: 0.269531]\n",
      "1371: [D loss: 0.696887, acc: 0.519531]  [A loss: 0.896739, acc: 0.128906]\n",
      "1372: [D loss: 0.701450, acc: 0.521484]  [A loss: 0.758739, acc: 0.414062]\n",
      "1373: [D loss: 0.684763, acc: 0.564453]  [A loss: 0.965505, acc: 0.093750]\n",
      "1374: [D loss: 0.694709, acc: 0.529297]  [A loss: 0.718384, acc: 0.464844]\n",
      "1375: [D loss: 0.700310, acc: 0.537109]  [A loss: 1.008110, acc: 0.066406]\n",
      "1376: [D loss: 0.702154, acc: 0.521484]  [A loss: 0.726588, acc: 0.453125]\n",
      "1377: [D loss: 0.703368, acc: 0.533203]  [A loss: 0.944429, acc: 0.113281]\n",
      "1378: [D loss: 0.695421, acc: 0.535156]  [A loss: 0.727507, acc: 0.449219]\n",
      "1379: [D loss: 0.713358, acc: 0.531250]  [A loss: 0.972879, acc: 0.093750]\n",
      "1380: [D loss: 0.689877, acc: 0.537109]  [A loss: 0.666896, acc: 0.601562]\n",
      "1381: [D loss: 0.722717, acc: 0.503906]  [A loss: 0.900694, acc: 0.128906]\n",
      "1382: [D loss: 0.700097, acc: 0.509766]  [A loss: 0.771577, acc: 0.324219]\n",
      "1383: [D loss: 0.709660, acc: 0.494141]  [A loss: 0.888069, acc: 0.156250]\n",
      "1384: [D loss: 0.675581, acc: 0.572266]  [A loss: 0.715936, acc: 0.468750]\n",
      "1385: [D loss: 0.695127, acc: 0.560547]  [A loss: 0.908798, acc: 0.140625]\n",
      "1386: [D loss: 0.701277, acc: 0.521484]  [A loss: 0.761598, acc: 0.378906]\n",
      "1387: [D loss: 0.706246, acc: 0.511719]  [A loss: 0.916215, acc: 0.140625]\n",
      "1388: [D loss: 0.707170, acc: 0.531250]  [A loss: 0.764644, acc: 0.371094]\n",
      "1389: [D loss: 0.703415, acc: 0.517578]  [A loss: 0.874219, acc: 0.175781]\n",
      "1390: [D loss: 0.697269, acc: 0.529297]  [A loss: 0.778876, acc: 0.320312]\n",
      "1391: [D loss: 0.691513, acc: 0.546875]  [A loss: 0.855304, acc: 0.191406]\n",
      "1392: [D loss: 0.687443, acc: 0.537109]  [A loss: 0.763168, acc: 0.386719]\n",
      "1393: [D loss: 0.686192, acc: 0.572266]  [A loss: 0.906389, acc: 0.144531]\n",
      "1394: [D loss: 0.709943, acc: 0.476562]  [A loss: 0.743796, acc: 0.453125]\n",
      "1395: [D loss: 0.716081, acc: 0.535156]  [A loss: 0.929238, acc: 0.121094]\n",
      "1396: [D loss: 0.685720, acc: 0.544922]  [A loss: 0.767086, acc: 0.355469]\n",
      "1397: [D loss: 0.700194, acc: 0.527344]  [A loss: 0.856750, acc: 0.199219]\n",
      "1398: [D loss: 0.683238, acc: 0.558594]  [A loss: 0.759966, acc: 0.371094]\n",
      "1399: [D loss: 0.707987, acc: 0.531250]  [A loss: 0.878716, acc: 0.183594]\n",
      "1400: [D loss: 0.675702, acc: 0.568359]  [A loss: 0.772211, acc: 0.351562]\n",
      "1401: [D loss: 0.712886, acc: 0.507812]  [A loss: 0.894095, acc: 0.148438]\n",
      "1402: [D loss: 0.695903, acc: 0.533203]  [A loss: 0.741179, acc: 0.406250]\n",
      "1403: [D loss: 0.702880, acc: 0.539062]  [A loss: 0.967028, acc: 0.085938]\n",
      "1404: [D loss: 0.691488, acc: 0.541016]  [A loss: 0.680720, acc: 0.531250]\n",
      "1405: [D loss: 0.728419, acc: 0.513672]  [A loss: 0.937437, acc: 0.148438]\n",
      "1406: [D loss: 0.695555, acc: 0.525391]  [A loss: 0.754401, acc: 0.363281]\n",
      "1407: [D loss: 0.698582, acc: 0.527344]  [A loss: 0.874278, acc: 0.226562]\n",
      "1408: [D loss: 0.710214, acc: 0.519531]  [A loss: 0.785857, acc: 0.316406]\n",
      "1409: [D loss: 0.685099, acc: 0.548828]  [A loss: 0.734671, acc: 0.414062]\n",
      "1410: [D loss: 0.708353, acc: 0.507812]  [A loss: 0.882206, acc: 0.175781]\n",
      "1411: [D loss: 0.700169, acc: 0.527344]  [A loss: 0.819176, acc: 0.226562]\n",
      "1412: [D loss: 0.678828, acc: 0.583984]  [A loss: 0.823923, acc: 0.222656]\n",
      "1413: [D loss: 0.708396, acc: 0.527344]  [A loss: 0.819180, acc: 0.265625]\n",
      "1414: [D loss: 0.699481, acc: 0.523438]  [A loss: 0.840723, acc: 0.238281]\n",
      "1415: [D loss: 0.682653, acc: 0.570312]  [A loss: 0.875967, acc: 0.187500]\n",
      "1416: [D loss: 0.687070, acc: 0.542969]  [A loss: 0.833827, acc: 0.238281]\n",
      "1417: [D loss: 0.694327, acc: 0.519531]  [A loss: 0.866783, acc: 0.160156]\n",
      "1418: [D loss: 0.705601, acc: 0.505859]  [A loss: 0.824935, acc: 0.242188]\n",
      "1419: [D loss: 0.699103, acc: 0.515625]  [A loss: 0.862703, acc: 0.210938]\n",
      "1420: [D loss: 0.701105, acc: 0.505859]  [A loss: 0.838005, acc: 0.218750]\n",
      "1421: [D loss: 0.702433, acc: 0.517578]  [A loss: 0.876791, acc: 0.148438]\n",
      "1422: [D loss: 0.689423, acc: 0.552734]  [A loss: 0.823096, acc: 0.261719]\n",
      "1423: [D loss: 0.700379, acc: 0.511719]  [A loss: 0.948072, acc: 0.144531]\n",
      "1424: [D loss: 0.699659, acc: 0.501953]  [A loss: 0.696043, acc: 0.503906]\n",
      "1425: [D loss: 0.712828, acc: 0.535156]  [A loss: 0.975467, acc: 0.093750]\n",
      "1426: [D loss: 0.715114, acc: 0.490234]  [A loss: 0.627917, acc: 0.683594]\n",
      "1427: [D loss: 0.738828, acc: 0.494141]  [A loss: 1.069194, acc: 0.046875]\n",
      "1428: [D loss: 0.695605, acc: 0.541016]  [A loss: 0.655061, acc: 0.597656]\n",
      "1429: [D loss: 0.722918, acc: 0.511719]  [A loss: 0.870102, acc: 0.191406]\n",
      "1430: [D loss: 0.691549, acc: 0.505859]  [A loss: 0.748303, acc: 0.355469]\n",
      "1431: [D loss: 0.698344, acc: 0.544922]  [A loss: 0.871943, acc: 0.207031]\n",
      "1432: [D loss: 0.689152, acc: 0.556641]  [A loss: 0.759539, acc: 0.398438]\n",
      "1433: [D loss: 0.695822, acc: 0.556641]  [A loss: 0.921369, acc: 0.132812]\n",
      "1434: [D loss: 0.693084, acc: 0.537109]  [A loss: 0.750512, acc: 0.390625]\n",
      "1435: [D loss: 0.714461, acc: 0.501953]  [A loss: 0.837887, acc: 0.222656]\n",
      "1436: [D loss: 0.691959, acc: 0.537109]  [A loss: 0.834267, acc: 0.253906]\n",
      "1437: [D loss: 0.695645, acc: 0.541016]  [A loss: 0.859811, acc: 0.203125]\n",
      "1438: [D loss: 0.704342, acc: 0.546875]  [A loss: 0.807999, acc: 0.257812]\n",
      "1439: [D loss: 0.696471, acc: 0.554688]  [A loss: 0.891197, acc: 0.136719]\n",
      "1440: [D loss: 0.690110, acc: 0.550781]  [A loss: 0.748102, acc: 0.410156]\n",
      "1441: [D loss: 0.719356, acc: 0.509766]  [A loss: 0.915008, acc: 0.117188]\n",
      "1442: [D loss: 0.698463, acc: 0.523438]  [A loss: 0.770282, acc: 0.367188]\n",
      "1443: [D loss: 0.704182, acc: 0.503906]  [A loss: 0.875530, acc: 0.164062]\n",
      "1444: [D loss: 0.698859, acc: 0.515625]  [A loss: 0.745982, acc: 0.339844]\n",
      "1445: [D loss: 0.707191, acc: 0.531250]  [A loss: 0.943749, acc: 0.136719]\n",
      "1446: [D loss: 0.699278, acc: 0.529297]  [A loss: 0.712504, acc: 0.503906]\n",
      "1447: [D loss: 0.695225, acc: 0.560547]  [A loss: 0.894590, acc: 0.179688]\n",
      "1448: [D loss: 0.698880, acc: 0.535156]  [A loss: 0.729664, acc: 0.445312]\n",
      "1449: [D loss: 0.715902, acc: 0.503906]  [A loss: 0.985464, acc: 0.082031]\n",
      "1450: [D loss: 0.695491, acc: 0.541016]  [A loss: 0.699134, acc: 0.476562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1451: [D loss: 0.714624, acc: 0.505859]  [A loss: 0.925239, acc: 0.125000]\n",
      "1452: [D loss: 0.703130, acc: 0.515625]  [A loss: 0.748724, acc: 0.375000]\n",
      "1453: [D loss: 0.708731, acc: 0.513672]  [A loss: 0.877599, acc: 0.191406]\n",
      "1454: [D loss: 0.687757, acc: 0.568359]  [A loss: 0.762089, acc: 0.355469]\n",
      "1455: [D loss: 0.702864, acc: 0.533203]  [A loss: 0.858401, acc: 0.183594]\n",
      "1456: [D loss: 0.698910, acc: 0.542969]  [A loss: 0.773688, acc: 0.363281]\n",
      "1457: [D loss: 0.716406, acc: 0.500000]  [A loss: 0.876767, acc: 0.160156]\n",
      "1458: [D loss: 0.682079, acc: 0.593750]  [A loss: 0.833169, acc: 0.246094]\n",
      "1459: [D loss: 0.711117, acc: 0.500000]  [A loss: 0.888361, acc: 0.164062]\n",
      "1460: [D loss: 0.695745, acc: 0.535156]  [A loss: 0.746831, acc: 0.378906]\n",
      "1461: [D loss: 0.714111, acc: 0.533203]  [A loss: 0.940918, acc: 0.113281]\n",
      "1462: [D loss: 0.692810, acc: 0.531250]  [A loss: 0.737109, acc: 0.414062]\n",
      "1463: [D loss: 0.707080, acc: 0.523438]  [A loss: 0.977501, acc: 0.105469]\n",
      "1464: [D loss: 0.692146, acc: 0.537109]  [A loss: 0.719928, acc: 0.527344]\n",
      "1465: [D loss: 0.727981, acc: 0.500000]  [A loss: 0.846798, acc: 0.222656]\n",
      "1466: [D loss: 0.694558, acc: 0.519531]  [A loss: 0.900624, acc: 0.132812]\n",
      "1467: [D loss: 0.694673, acc: 0.515625]  [A loss: 0.735147, acc: 0.441406]\n",
      "1468: [D loss: 0.723741, acc: 0.494141]  [A loss: 0.918323, acc: 0.128906]\n",
      "1469: [D loss: 0.718802, acc: 0.515625]  [A loss: 0.795187, acc: 0.308594]\n",
      "1470: [D loss: 0.703814, acc: 0.511719]  [A loss: 0.944503, acc: 0.125000]\n",
      "1471: [D loss: 0.686859, acc: 0.568359]  [A loss: 0.676285, acc: 0.554688]\n",
      "1472: [D loss: 0.715082, acc: 0.509766]  [A loss: 0.946058, acc: 0.117188]\n",
      "1473: [D loss: 0.691973, acc: 0.519531]  [A loss: 0.733160, acc: 0.421875]\n",
      "1474: [D loss: 0.702692, acc: 0.533203]  [A loss: 0.845367, acc: 0.230469]\n",
      "1475: [D loss: 0.691310, acc: 0.552734]  [A loss: 0.726106, acc: 0.453125]\n",
      "1476: [D loss: 0.719777, acc: 0.523438]  [A loss: 0.963532, acc: 0.117188]\n",
      "1477: [D loss: 0.692675, acc: 0.531250]  [A loss: 0.762796, acc: 0.359375]\n",
      "1478: [D loss: 0.730182, acc: 0.474609]  [A loss: 0.862193, acc: 0.195312]\n",
      "1479: [D loss: 0.680508, acc: 0.542969]  [A loss: 0.813327, acc: 0.253906]\n",
      "1480: [D loss: 0.707383, acc: 0.525391]  [A loss: 0.810751, acc: 0.277344]\n",
      "1481: [D loss: 0.712614, acc: 0.498047]  [A loss: 0.829077, acc: 0.222656]\n",
      "1482: [D loss: 0.701199, acc: 0.511719]  [A loss: 0.836890, acc: 0.238281]\n",
      "1483: [D loss: 0.699136, acc: 0.521484]  [A loss: 0.864429, acc: 0.175781]\n",
      "1484: [D loss: 0.704172, acc: 0.515625]  [A loss: 0.696357, acc: 0.515625]\n",
      "1485: [D loss: 0.715336, acc: 0.521484]  [A loss: 1.012652, acc: 0.058594]\n",
      "1486: [D loss: 0.696483, acc: 0.531250]  [A loss: 0.687150, acc: 0.531250]\n",
      "1487: [D loss: 0.727897, acc: 0.507812]  [A loss: 0.874817, acc: 0.167969]\n",
      "1488: [D loss: 0.685483, acc: 0.585938]  [A loss: 0.774412, acc: 0.347656]\n",
      "1489: [D loss: 0.703375, acc: 0.517578]  [A loss: 0.910783, acc: 0.125000]\n",
      "1490: [D loss: 0.689505, acc: 0.566406]  [A loss: 0.726205, acc: 0.460938]\n",
      "1491: [D loss: 0.699376, acc: 0.535156]  [A loss: 0.812004, acc: 0.285156]\n",
      "1492: [D loss: 0.679770, acc: 0.542969]  [A loss: 0.862561, acc: 0.187500]\n",
      "1493: [D loss: 0.693901, acc: 0.519531]  [A loss: 0.799872, acc: 0.300781]\n",
      "1494: [D loss: 0.715791, acc: 0.498047]  [A loss: 0.922929, acc: 0.109375]\n",
      "1495: [D loss: 0.695510, acc: 0.546875]  [A loss: 0.762558, acc: 0.367188]\n",
      "1496: [D loss: 0.710515, acc: 0.523438]  [A loss: 0.948636, acc: 0.128906]\n",
      "1497: [D loss: 0.703965, acc: 0.503906]  [A loss: 0.807433, acc: 0.281250]\n",
      "1498: [D loss: 0.705679, acc: 0.509766]  [A loss: 0.871004, acc: 0.167969]\n",
      "1499: [D loss: 0.698222, acc: 0.511719]  [A loss: 0.820968, acc: 0.253906]\n",
      "1500: [D loss: 0.702837, acc: 0.529297]  [A loss: 0.869043, acc: 0.203125]\n",
      "1501: [D loss: 0.693249, acc: 0.548828]  [A loss: 0.818305, acc: 0.265625]\n",
      "1502: [D loss: 0.704335, acc: 0.501953]  [A loss: 0.879153, acc: 0.183594]\n",
      "1503: [D loss: 0.705331, acc: 0.498047]  [A loss: 0.775025, acc: 0.359375]\n",
      "1504: [D loss: 0.705677, acc: 0.539062]  [A loss: 0.928729, acc: 0.132812]\n",
      "1505: [D loss: 0.705102, acc: 0.496094]  [A loss: 0.740785, acc: 0.445312]\n",
      "1506: [D loss: 0.708560, acc: 0.503906]  [A loss: 0.972360, acc: 0.101562]\n",
      "1507: [D loss: 0.699071, acc: 0.519531]  [A loss: 0.690744, acc: 0.535156]\n",
      "1508: [D loss: 0.726399, acc: 0.527344]  [A loss: 1.007802, acc: 0.054688]\n",
      "1509: [D loss: 0.703637, acc: 0.525391]  [A loss: 0.677557, acc: 0.554688]\n",
      "1510: [D loss: 0.730377, acc: 0.496094]  [A loss: 0.983259, acc: 0.089844]\n",
      "1511: [D loss: 0.702519, acc: 0.517578]  [A loss: 0.677523, acc: 0.574219]\n",
      "1512: [D loss: 0.711568, acc: 0.517578]  [A loss: 0.869469, acc: 0.187500]\n",
      "1513: [D loss: 0.704479, acc: 0.517578]  [A loss: 0.753449, acc: 0.390625]\n",
      "1514: [D loss: 0.705705, acc: 0.509766]  [A loss: 0.810827, acc: 0.265625]\n",
      "1515: [D loss: 0.699300, acc: 0.525391]  [A loss: 0.774613, acc: 0.343750]\n",
      "1516: [D loss: 0.696233, acc: 0.513672]  [A loss: 0.827964, acc: 0.234375]\n",
      "1517: [D loss: 0.694995, acc: 0.531250]  [A loss: 0.799156, acc: 0.316406]\n",
      "1518: [D loss: 0.687579, acc: 0.554688]  [A loss: 0.827156, acc: 0.261719]\n",
      "1519: [D loss: 0.698568, acc: 0.531250]  [A loss: 0.812981, acc: 0.261719]\n",
      "1520: [D loss: 0.696481, acc: 0.507812]  [A loss: 0.829570, acc: 0.234375]\n",
      "1521: [D loss: 0.696909, acc: 0.521484]  [A loss: 0.805006, acc: 0.304688]\n",
      "1522: [D loss: 0.721723, acc: 0.488281]  [A loss: 0.832825, acc: 0.257812]\n",
      "1523: [D loss: 0.698882, acc: 0.537109]  [A loss: 0.908005, acc: 0.140625]\n",
      "1524: [D loss: 0.696012, acc: 0.521484]  [A loss: 0.775823, acc: 0.343750]\n",
      "1525: [D loss: 0.701711, acc: 0.511719]  [A loss: 0.923782, acc: 0.132812]\n",
      "1526: [D loss: 0.699385, acc: 0.523438]  [A loss: 0.872175, acc: 0.164062]\n",
      "1527: [D loss: 0.688875, acc: 0.529297]  [A loss: 0.788240, acc: 0.316406]\n",
      "1528: [D loss: 0.703643, acc: 0.523438]  [A loss: 0.881565, acc: 0.199219]\n",
      "1529: [D loss: 0.692620, acc: 0.541016]  [A loss: 0.769211, acc: 0.351562]\n",
      "1530: [D loss: 0.713769, acc: 0.527344]  [A loss: 0.973976, acc: 0.074219]\n",
      "1531: [D loss: 0.698524, acc: 0.521484]  [A loss: 0.676695, acc: 0.539062]\n",
      "1532: [D loss: 0.706314, acc: 0.527344]  [A loss: 1.005709, acc: 0.078125]\n",
      "1533: [D loss: 0.693304, acc: 0.539062]  [A loss: 0.671144, acc: 0.585938]\n",
      "1534: [D loss: 0.701665, acc: 0.539062]  [A loss: 0.877051, acc: 0.179688]\n",
      "1535: [D loss: 0.706219, acc: 0.535156]  [A loss: 0.769187, acc: 0.320312]\n",
      "1536: [D loss: 0.708625, acc: 0.515625]  [A loss: 0.902635, acc: 0.144531]\n",
      "1537: [D loss: 0.675975, acc: 0.566406]  [A loss: 0.703921, acc: 0.480469]\n",
      "1538: [D loss: 0.722201, acc: 0.498047]  [A loss: 0.996199, acc: 0.074219]\n",
      "1539: [D loss: 0.712850, acc: 0.484375]  [A loss: 0.703848, acc: 0.496094]\n",
      "1540: [D loss: 0.715949, acc: 0.505859]  [A loss: 0.958277, acc: 0.089844]\n",
      "1541: [D loss: 0.683802, acc: 0.544922]  [A loss: 0.726451, acc: 0.484375]\n",
      "1542: [D loss: 0.719690, acc: 0.490234]  [A loss: 0.880105, acc: 0.167969]\n",
      "1543: [D loss: 0.689903, acc: 0.548828]  [A loss: 0.800845, acc: 0.273438]\n",
      "1544: [D loss: 0.716272, acc: 0.478516]  [A loss: 0.836585, acc: 0.187500]\n",
      "1545: [D loss: 0.696162, acc: 0.513672]  [A loss: 0.806553, acc: 0.261719]\n",
      "1546: [D loss: 0.700035, acc: 0.544922]  [A loss: 0.805345, acc: 0.281250]\n",
      "1547: [D loss: 0.708832, acc: 0.509766]  [A loss: 0.915747, acc: 0.128906]\n",
      "1548: [D loss: 0.701009, acc: 0.517578]  [A loss: 0.731997, acc: 0.460938]\n",
      "1549: [D loss: 0.707409, acc: 0.542969]  [A loss: 0.907548, acc: 0.148438]\n",
      "1550: [D loss: 0.683775, acc: 0.566406]  [A loss: 0.789471, acc: 0.261719]\n",
      "1551: [D loss: 0.704396, acc: 0.505859]  [A loss: 0.919695, acc: 0.128906]\n",
      "1552: [D loss: 0.683108, acc: 0.542969]  [A loss: 0.745602, acc: 0.425781]\n",
      "1553: [D loss: 0.699150, acc: 0.552734]  [A loss: 0.911369, acc: 0.140625]\n",
      "1554: [D loss: 0.694044, acc: 0.527344]  [A loss: 0.757202, acc: 0.386719]\n",
      "1555: [D loss: 0.694931, acc: 0.527344]  [A loss: 0.880962, acc: 0.187500]\n",
      "1556: [D loss: 0.689753, acc: 0.574219]  [A loss: 0.782082, acc: 0.347656]\n",
      "1557: [D loss: 0.688851, acc: 0.556641]  [A loss: 0.914389, acc: 0.167969]\n",
      "1558: [D loss: 0.703443, acc: 0.535156]  [A loss: 0.677408, acc: 0.558594]\n",
      "1559: [D loss: 0.704944, acc: 0.544922]  [A loss: 0.994893, acc: 0.074219]\n",
      "1560: [D loss: 0.701092, acc: 0.519531]  [A loss: 0.719934, acc: 0.449219]\n",
      "1561: [D loss: 0.720335, acc: 0.533203]  [A loss: 0.993968, acc: 0.082031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562: [D loss: 0.701383, acc: 0.523438]  [A loss: 0.685439, acc: 0.535156]\n",
      "1563: [D loss: 0.722842, acc: 0.519531]  [A loss: 0.925203, acc: 0.128906]\n",
      "1564: [D loss: 0.697235, acc: 0.503906]  [A loss: 0.781131, acc: 0.335938]\n",
      "1565: [D loss: 0.687431, acc: 0.558594]  [A loss: 0.810846, acc: 0.277344]\n",
      "1566: [D loss: 0.688563, acc: 0.552734]  [A loss: 0.813756, acc: 0.273438]\n",
      "1567: [D loss: 0.705485, acc: 0.511719]  [A loss: 0.887202, acc: 0.160156]\n",
      "1568: [D loss: 0.695063, acc: 0.517578]  [A loss: 0.785420, acc: 0.312500]\n",
      "1569: [D loss: 0.698685, acc: 0.531250]  [A loss: 0.966102, acc: 0.070312]\n",
      "1570: [D loss: 0.696854, acc: 0.509766]  [A loss: 0.829598, acc: 0.234375]\n",
      "1571: [D loss: 0.712948, acc: 0.525391]  [A loss: 0.888531, acc: 0.179688]\n",
      "1572: [D loss: 0.681822, acc: 0.552734]  [A loss: 0.757445, acc: 0.414062]\n",
      "1573: [D loss: 0.705442, acc: 0.513672]  [A loss: 0.940862, acc: 0.117188]\n",
      "1574: [D loss: 0.692205, acc: 0.531250]  [A loss: 0.696940, acc: 0.500000]\n",
      "1575: [D loss: 0.714628, acc: 0.500000]  [A loss: 0.947579, acc: 0.097656]\n",
      "1576: [D loss: 0.694587, acc: 0.542969]  [A loss: 0.662960, acc: 0.605469]\n",
      "1577: [D loss: 0.705316, acc: 0.525391]  [A loss: 0.932576, acc: 0.117188]\n",
      "1578: [D loss: 0.691496, acc: 0.542969]  [A loss: 0.683959, acc: 0.539062]\n",
      "1579: [D loss: 0.727396, acc: 0.507812]  [A loss: 0.939594, acc: 0.101562]\n",
      "1580: [D loss: 0.702391, acc: 0.529297]  [A loss: 0.759155, acc: 0.359375]\n",
      "1581: [D loss: 0.707260, acc: 0.527344]  [A loss: 0.911232, acc: 0.140625]\n",
      "1582: [D loss: 0.710273, acc: 0.494141]  [A loss: 0.729766, acc: 0.464844]\n",
      "1583: [D loss: 0.707151, acc: 0.507812]  [A loss: 0.859311, acc: 0.214844]\n",
      "1584: [D loss: 0.687302, acc: 0.539062]  [A loss: 0.793737, acc: 0.359375]\n",
      "1585: [D loss: 0.707588, acc: 0.492188]  [A loss: 0.853241, acc: 0.246094]\n",
      "1586: [D loss: 0.706107, acc: 0.505859]  [A loss: 0.775895, acc: 0.332031]\n",
      "1587: [D loss: 0.683197, acc: 0.572266]  [A loss: 0.898511, acc: 0.132812]\n",
      "1588: [D loss: 0.679664, acc: 0.562500]  [A loss: 0.724849, acc: 0.410156]\n",
      "1589: [D loss: 0.694514, acc: 0.562500]  [A loss: 0.922926, acc: 0.167969]\n",
      "1590: [D loss: 0.692207, acc: 0.541016]  [A loss: 0.700561, acc: 0.535156]\n",
      "1591: [D loss: 0.730042, acc: 0.501953]  [A loss: 1.039456, acc: 0.050781]\n",
      "1592: [D loss: 0.698852, acc: 0.527344]  [A loss: 0.647675, acc: 0.609375]\n",
      "1593: [D loss: 0.725191, acc: 0.539062]  [A loss: 1.034914, acc: 0.046875]\n",
      "1594: [D loss: 0.705488, acc: 0.517578]  [A loss: 0.710642, acc: 0.468750]\n",
      "1595: [D loss: 0.715241, acc: 0.513672]  [A loss: 0.907109, acc: 0.144531]\n",
      "1596: [D loss: 0.694366, acc: 0.527344]  [A loss: 0.716780, acc: 0.457031]\n",
      "1597: [D loss: 0.705191, acc: 0.525391]  [A loss: 0.878062, acc: 0.203125]\n",
      "1598: [D loss: 0.696440, acc: 0.527344]  [A loss: 0.769642, acc: 0.339844]\n",
      "1599: [D loss: 0.709345, acc: 0.541016]  [A loss: 0.869666, acc: 0.175781]\n",
      "1600: [D loss: 0.693197, acc: 0.544922]  [A loss: 0.857749, acc: 0.207031]\n",
      "1601: [D loss: 0.689364, acc: 0.544922]  [A loss: 0.835646, acc: 0.222656]\n",
      "1602: [D loss: 0.681734, acc: 0.578125]  [A loss: 0.811104, acc: 0.269531]\n",
      "1603: [D loss: 0.702591, acc: 0.515625]  [A loss: 0.830864, acc: 0.253906]\n",
      "1604: [D loss: 0.704505, acc: 0.501953]  [A loss: 0.778548, acc: 0.324219]\n",
      "1605: [D loss: 0.699031, acc: 0.552734]  [A loss: 0.876850, acc: 0.187500]\n",
      "1606: [D loss: 0.699822, acc: 0.519531]  [A loss: 0.863889, acc: 0.214844]\n",
      "1607: [D loss: 0.691863, acc: 0.546875]  [A loss: 0.792968, acc: 0.277344]\n",
      "1608: [D loss: 0.687915, acc: 0.564453]  [A loss: 0.880391, acc: 0.175781]\n",
      "1609: [D loss: 0.701700, acc: 0.529297]  [A loss: 0.801489, acc: 0.265625]\n",
      "1610: [D loss: 0.702003, acc: 0.525391]  [A loss: 0.924854, acc: 0.160156]\n",
      "1611: [D loss: 0.710663, acc: 0.509766]  [A loss: 0.807149, acc: 0.257812]\n",
      "1612: [D loss: 0.699595, acc: 0.515625]  [A loss: 0.893351, acc: 0.152344]\n",
      "1613: [D loss: 0.688201, acc: 0.548828]  [A loss: 0.809256, acc: 0.269531]\n",
      "1614: [D loss: 0.696145, acc: 0.535156]  [A loss: 0.889681, acc: 0.203125]\n",
      "1615: [D loss: 0.701043, acc: 0.498047]  [A loss: 0.764815, acc: 0.375000]\n",
      "1616: [D loss: 0.694615, acc: 0.511719]  [A loss: 0.938507, acc: 0.125000]\n",
      "1617: [D loss: 0.694230, acc: 0.535156]  [A loss: 0.677595, acc: 0.562500]\n",
      "1618: [D loss: 0.709213, acc: 0.513672]  [A loss: 1.059296, acc: 0.039062]\n",
      "1619: [D loss: 0.707165, acc: 0.486328]  [A loss: 0.636142, acc: 0.632812]\n",
      "1620: [D loss: 0.771009, acc: 0.500000]  [A loss: 1.068636, acc: 0.042969]\n",
      "1621: [D loss: 0.705776, acc: 0.505859]  [A loss: 0.682155, acc: 0.550781]\n",
      "1622: [D loss: 0.742771, acc: 0.486328]  [A loss: 0.839489, acc: 0.195312]\n",
      "1623: [D loss: 0.720062, acc: 0.498047]  [A loss: 0.867490, acc: 0.207031]\n",
      "1624: [D loss: 0.696019, acc: 0.533203]  [A loss: 0.763946, acc: 0.375000]\n",
      "1625: [D loss: 0.707132, acc: 0.523438]  [A loss: 0.869144, acc: 0.136719]\n",
      "1626: [D loss: 0.685091, acc: 0.552734]  [A loss: 0.775288, acc: 0.351562]\n",
      "1627: [D loss: 0.693441, acc: 0.531250]  [A loss: 0.814172, acc: 0.281250]\n",
      "1628: [D loss: 0.699159, acc: 0.525391]  [A loss: 0.851289, acc: 0.203125]\n",
      "1629: [D loss: 0.701053, acc: 0.548828]  [A loss: 0.846048, acc: 0.230469]\n",
      "1630: [D loss: 0.700776, acc: 0.519531]  [A loss: 0.841507, acc: 0.273438]\n",
      "1631: [D loss: 0.703417, acc: 0.525391]  [A loss: 0.825406, acc: 0.242188]\n",
      "1632: [D loss: 0.694189, acc: 0.517578]  [A loss: 0.799688, acc: 0.300781]\n",
      "1633: [D loss: 0.699285, acc: 0.517578]  [A loss: 0.819835, acc: 0.265625]\n",
      "1634: [D loss: 0.692307, acc: 0.558594]  [A loss: 0.740156, acc: 0.445312]\n",
      "1635: [D loss: 0.696324, acc: 0.542969]  [A loss: 0.929125, acc: 0.140625]\n",
      "1636: [D loss: 0.698793, acc: 0.521484]  [A loss: 0.707052, acc: 0.507812]\n",
      "1637: [D loss: 0.718120, acc: 0.523438]  [A loss: 0.969043, acc: 0.125000]\n",
      "1638: [D loss: 0.684508, acc: 0.541016]  [A loss: 0.714823, acc: 0.472656]\n",
      "1639: [D loss: 0.739075, acc: 0.498047]  [A loss: 0.964381, acc: 0.109375]\n",
      "1640: [D loss: 0.693616, acc: 0.535156]  [A loss: 0.719764, acc: 0.472656]\n",
      "1641: [D loss: 0.698408, acc: 0.515625]  [A loss: 0.879150, acc: 0.156250]\n",
      "1642: [D loss: 0.688003, acc: 0.527344]  [A loss: 0.781595, acc: 0.351562]\n",
      "1643: [D loss: 0.704202, acc: 0.527344]  [A loss: 0.948464, acc: 0.121094]\n",
      "1644: [D loss: 0.700744, acc: 0.527344]  [A loss: 0.713040, acc: 0.457031]\n",
      "1645: [D loss: 0.732051, acc: 0.511719]  [A loss: 1.031404, acc: 0.070312]\n",
      "1646: [D loss: 0.710570, acc: 0.503906]  [A loss: 0.716116, acc: 0.500000]\n",
      "1647: [D loss: 0.732176, acc: 0.500000]  [A loss: 0.936019, acc: 0.117188]\n",
      "1648: [D loss: 0.698171, acc: 0.511719]  [A loss: 0.736097, acc: 0.429688]\n",
      "1649: [D loss: 0.723894, acc: 0.503906]  [A loss: 0.988511, acc: 0.066406]\n",
      "1650: [D loss: 0.692498, acc: 0.517578]  [A loss: 0.695180, acc: 0.496094]\n",
      "1651: [D loss: 0.717338, acc: 0.490234]  [A loss: 0.954290, acc: 0.089844]\n",
      "1652: [D loss: 0.696053, acc: 0.564453]  [A loss: 0.751577, acc: 0.335938]\n",
      "1653: [D loss: 0.708010, acc: 0.529297]  [A loss: 0.826088, acc: 0.265625]\n",
      "1654: [D loss: 0.688106, acc: 0.535156]  [A loss: 0.757021, acc: 0.351562]\n",
      "1655: [D loss: 0.706505, acc: 0.507812]  [A loss: 0.923877, acc: 0.144531]\n",
      "1656: [D loss: 0.699003, acc: 0.519531]  [A loss: 0.733736, acc: 0.445312]\n",
      "1657: [D loss: 0.706979, acc: 0.521484]  [A loss: 0.884386, acc: 0.167969]\n",
      "1658: [D loss: 0.704663, acc: 0.507812]  [A loss: 0.755201, acc: 0.402344]\n",
      "1659: [D loss: 0.702351, acc: 0.496094]  [A loss: 0.886989, acc: 0.203125]\n",
      "1660: [D loss: 0.694623, acc: 0.507812]  [A loss: 0.767936, acc: 0.351562]\n",
      "1661: [D loss: 0.713273, acc: 0.523438]  [A loss: 0.903949, acc: 0.125000]\n",
      "1662: [D loss: 0.697047, acc: 0.531250]  [A loss: 0.722110, acc: 0.464844]\n",
      "1663: [D loss: 0.726611, acc: 0.496094]  [A loss: 0.937551, acc: 0.140625]\n",
      "1664: [D loss: 0.696912, acc: 0.552734]  [A loss: 0.708128, acc: 0.488281]\n",
      "1665: [D loss: 0.719810, acc: 0.488281]  [A loss: 0.958001, acc: 0.089844]\n",
      "1666: [D loss: 0.686010, acc: 0.566406]  [A loss: 0.692496, acc: 0.515625]\n",
      "1667: [D loss: 0.714456, acc: 0.515625]  [A loss: 0.918734, acc: 0.113281]\n",
      "1668: [D loss: 0.694657, acc: 0.544922]  [A loss: 0.734411, acc: 0.378906]\n",
      "1669: [D loss: 0.709190, acc: 0.548828]  [A loss: 0.862973, acc: 0.230469]\n",
      "1670: [D loss: 0.709496, acc: 0.509766]  [A loss: 0.736917, acc: 0.433594]\n",
      "1671: [D loss: 0.706485, acc: 0.494141]  [A loss: 0.998676, acc: 0.050781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672: [D loss: 0.684608, acc: 0.570312]  [A loss: 0.685761, acc: 0.539062]\n",
      "1673: [D loss: 0.713877, acc: 0.531250]  [A loss: 1.015275, acc: 0.046875]\n",
      "1674: [D loss: 0.700071, acc: 0.544922]  [A loss: 0.686670, acc: 0.535156]\n",
      "1675: [D loss: 0.725457, acc: 0.507812]  [A loss: 0.927671, acc: 0.160156]\n",
      "1676: [D loss: 0.703876, acc: 0.515625]  [A loss: 0.734733, acc: 0.433594]\n",
      "1677: [D loss: 0.716785, acc: 0.519531]  [A loss: 0.879035, acc: 0.203125]\n",
      "1678: [D loss: 0.698971, acc: 0.537109]  [A loss: 0.766438, acc: 0.386719]\n",
      "1679: [D loss: 0.707169, acc: 0.527344]  [A loss: 0.829070, acc: 0.210938]\n",
      "1680: [D loss: 0.701564, acc: 0.494141]  [A loss: 0.879254, acc: 0.171875]\n",
      "1681: [D loss: 0.712636, acc: 0.519531]  [A loss: 0.803922, acc: 0.250000]\n",
      "1682: [D loss: 0.694570, acc: 0.550781]  [A loss: 0.848105, acc: 0.214844]\n",
      "1683: [D loss: 0.690949, acc: 0.517578]  [A loss: 0.813465, acc: 0.257812]\n",
      "1684: [D loss: 0.713809, acc: 0.472656]  [A loss: 0.835829, acc: 0.207031]\n",
      "1685: [D loss: 0.712955, acc: 0.505859]  [A loss: 0.862215, acc: 0.187500]\n",
      "1686: [D loss: 0.711832, acc: 0.525391]  [A loss: 0.920477, acc: 0.109375]\n",
      "1687: [D loss: 0.702075, acc: 0.519531]  [A loss: 0.753819, acc: 0.378906]\n",
      "1688: [D loss: 0.705908, acc: 0.507812]  [A loss: 0.939463, acc: 0.105469]\n",
      "1689: [D loss: 0.715215, acc: 0.480469]  [A loss: 0.788536, acc: 0.312500]\n",
      "1690: [D loss: 0.699362, acc: 0.548828]  [A loss: 0.914311, acc: 0.128906]\n",
      "1691: [D loss: 0.683701, acc: 0.552734]  [A loss: 0.708298, acc: 0.476562]\n",
      "1692: [D loss: 0.718448, acc: 0.529297]  [A loss: 1.016396, acc: 0.046875]\n",
      "1693: [D loss: 0.703782, acc: 0.519531]  [A loss: 0.675531, acc: 0.550781]\n",
      "1694: [D loss: 0.722438, acc: 0.513672]  [A loss: 0.904626, acc: 0.132812]\n",
      "1695: [D loss: 0.700473, acc: 0.537109]  [A loss: 0.743064, acc: 0.445312]\n",
      "1696: [D loss: 0.689778, acc: 0.548828]  [A loss: 0.968931, acc: 0.070312]\n",
      "1697: [D loss: 0.692278, acc: 0.548828]  [A loss: 0.750640, acc: 0.421875]\n",
      "1698: [D loss: 0.693673, acc: 0.548828]  [A loss: 0.904228, acc: 0.148438]\n",
      "1699: [D loss: 0.709517, acc: 0.500000]  [A loss: 0.811656, acc: 0.250000]\n",
      "1700: [D loss: 0.693832, acc: 0.542969]  [A loss: 0.820703, acc: 0.226562]\n",
      "1701: [D loss: 0.696522, acc: 0.529297]  [A loss: 0.782690, acc: 0.335938]\n",
      "1702: [D loss: 0.705293, acc: 0.525391]  [A loss: 0.905294, acc: 0.128906]\n",
      "1703: [D loss: 0.687465, acc: 0.537109]  [A loss: 0.767120, acc: 0.328125]\n",
      "1704: [D loss: 0.695306, acc: 0.521484]  [A loss: 0.964800, acc: 0.093750]\n",
      "1705: [D loss: 0.688335, acc: 0.562500]  [A loss: 0.707797, acc: 0.472656]\n",
      "1706: [D loss: 0.725169, acc: 0.503906]  [A loss: 0.979190, acc: 0.070312]\n",
      "1707: [D loss: 0.697678, acc: 0.511719]  [A loss: 0.636669, acc: 0.675781]\n",
      "1708: [D loss: 0.738698, acc: 0.507812]  [A loss: 1.025013, acc: 0.066406]\n",
      "1709: [D loss: 0.696441, acc: 0.517578]  [A loss: 0.678549, acc: 0.550781]\n",
      "1710: [D loss: 0.713947, acc: 0.505859]  [A loss: 0.908358, acc: 0.136719]\n",
      "1711: [D loss: 0.695018, acc: 0.505859]  [A loss: 0.771106, acc: 0.351562]\n",
      "1712: [D loss: 0.706833, acc: 0.513672]  [A loss: 0.829324, acc: 0.230469]\n",
      "1713: [D loss: 0.693551, acc: 0.525391]  [A loss: 0.775358, acc: 0.378906]\n",
      "1714: [D loss: 0.703506, acc: 0.513672]  [A loss: 0.905935, acc: 0.156250]\n",
      "1715: [D loss: 0.665231, acc: 0.619141]  [A loss: 0.745331, acc: 0.410156]\n",
      "1716: [D loss: 0.704168, acc: 0.507812]  [A loss: 0.882179, acc: 0.183594]\n",
      "1717: [D loss: 0.687743, acc: 0.558594]  [A loss: 0.729730, acc: 0.433594]\n",
      "1718: [D loss: 0.694362, acc: 0.554688]  [A loss: 0.929327, acc: 0.140625]\n",
      "1719: [D loss: 0.690912, acc: 0.548828]  [A loss: 0.794032, acc: 0.273438]\n",
      "1720: [D loss: 0.715616, acc: 0.488281]  [A loss: 0.899572, acc: 0.175781]\n",
      "1721: [D loss: 0.701728, acc: 0.505859]  [A loss: 0.786976, acc: 0.308594]\n",
      "1722: [D loss: 0.708018, acc: 0.521484]  [A loss: 0.912094, acc: 0.171875]\n",
      "1723: [D loss: 0.714427, acc: 0.513672]  [A loss: 0.845940, acc: 0.234375]\n",
      "1724: [D loss: 0.683612, acc: 0.537109]  [A loss: 0.819480, acc: 0.230469]\n",
      "1725: [D loss: 0.698420, acc: 0.509766]  [A loss: 0.861487, acc: 0.183594]\n",
      "1726: [D loss: 0.695251, acc: 0.544922]  [A loss: 0.798948, acc: 0.304688]\n",
      "1727: [D loss: 0.709972, acc: 0.509766]  [A loss: 0.917474, acc: 0.121094]\n",
      "1728: [D loss: 0.695459, acc: 0.521484]  [A loss: 0.776445, acc: 0.347656]\n",
      "1729: [D loss: 0.700571, acc: 0.523438]  [A loss: 0.889933, acc: 0.160156]\n",
      "1730: [D loss: 0.693108, acc: 0.544922]  [A loss: 0.877200, acc: 0.171875]\n",
      "1731: [D loss: 0.691535, acc: 0.525391]  [A loss: 0.777009, acc: 0.378906]\n",
      "1732: [D loss: 0.704978, acc: 0.507812]  [A loss: 1.092350, acc: 0.042969]\n",
      "1733: [D loss: 0.699241, acc: 0.552734]  [A loss: 0.701254, acc: 0.472656]\n",
      "1734: [D loss: 0.713135, acc: 0.519531]  [A loss: 1.002053, acc: 0.070312]\n",
      "1735: [D loss: 0.704459, acc: 0.511719]  [A loss: 0.678724, acc: 0.554688]\n",
      "1736: [D loss: 0.735886, acc: 0.515625]  [A loss: 1.041503, acc: 0.058594]\n",
      "1737: [D loss: 0.710535, acc: 0.529297]  [A loss: 0.671057, acc: 0.566406]\n",
      "1738: [D loss: 0.722057, acc: 0.515625]  [A loss: 0.936513, acc: 0.125000]\n",
      "1739: [D loss: 0.708523, acc: 0.488281]  [A loss: 0.735719, acc: 0.433594]\n",
      "1740: [D loss: 0.724695, acc: 0.484375]  [A loss: 0.977666, acc: 0.066406]\n",
      "1741: [D loss: 0.696637, acc: 0.513672]  [A loss: 0.691135, acc: 0.507812]\n",
      "1742: [D loss: 0.704139, acc: 0.525391]  [A loss: 0.945972, acc: 0.105469]\n",
      "1743: [D loss: 0.699174, acc: 0.537109]  [A loss: 0.717809, acc: 0.507812]\n",
      "1744: [D loss: 0.714566, acc: 0.507812]  [A loss: 0.871687, acc: 0.175781]\n",
      "1745: [D loss: 0.692180, acc: 0.529297]  [A loss: 0.790037, acc: 0.332031]\n",
      "1746: [D loss: 0.694195, acc: 0.519531]  [A loss: 0.822164, acc: 0.273438]\n",
      "1747: [D loss: 0.712342, acc: 0.474609]  [A loss: 0.812763, acc: 0.281250]\n",
      "1748: [D loss: 0.714715, acc: 0.476562]  [A loss: 0.882357, acc: 0.195312]\n",
      "1749: [D loss: 0.700484, acc: 0.519531]  [A loss: 0.799131, acc: 0.324219]\n",
      "1750: [D loss: 0.690660, acc: 0.537109]  [A loss: 0.844264, acc: 0.210938]\n",
      "1751: [D loss: 0.681905, acc: 0.560547]  [A loss: 0.812041, acc: 0.246094]\n",
      "1752: [D loss: 0.699423, acc: 0.511719]  [A loss: 0.820613, acc: 0.300781]\n",
      "1753: [D loss: 0.694479, acc: 0.542969]  [A loss: 0.758600, acc: 0.429688]\n",
      "1754: [D loss: 0.708810, acc: 0.511719]  [A loss: 0.945519, acc: 0.144531]\n",
      "1755: [D loss: 0.697271, acc: 0.515625]  [A loss: 0.794149, acc: 0.335938]\n",
      "1756: [D loss: 0.717111, acc: 0.519531]  [A loss: 0.993922, acc: 0.058594]\n",
      "1757: [D loss: 0.685079, acc: 0.533203]  [A loss: 0.697044, acc: 0.527344]\n",
      "1758: [D loss: 0.715125, acc: 0.523438]  [A loss: 0.991919, acc: 0.074219]\n",
      "1759: [D loss: 0.702136, acc: 0.496094]  [A loss: 0.683140, acc: 0.550781]\n",
      "1760: [D loss: 0.716722, acc: 0.527344]  [A loss: 0.890661, acc: 0.156250]\n",
      "1761: [D loss: 0.680460, acc: 0.578125]  [A loss: 0.768287, acc: 0.355469]\n",
      "1762: [D loss: 0.708815, acc: 0.517578]  [A loss: 0.834261, acc: 0.230469]\n",
      "1763: [D loss: 0.691027, acc: 0.546875]  [A loss: 0.785055, acc: 0.324219]\n",
      "1764: [D loss: 0.698076, acc: 0.525391]  [A loss: 0.912114, acc: 0.156250]\n",
      "1765: [D loss: 0.705209, acc: 0.517578]  [A loss: 0.724402, acc: 0.464844]\n",
      "1766: [D loss: 0.707959, acc: 0.542969]  [A loss: 0.992071, acc: 0.089844]\n",
      "1767: [D loss: 0.715473, acc: 0.492188]  [A loss: 0.675253, acc: 0.574219]\n",
      "1768: [D loss: 0.729107, acc: 0.500000]  [A loss: 1.009487, acc: 0.058594]\n",
      "1769: [D loss: 0.689266, acc: 0.542969]  [A loss: 0.644898, acc: 0.648438]\n",
      "1770: [D loss: 0.737227, acc: 0.486328]  [A loss: 0.951987, acc: 0.101562]\n",
      "1771: [D loss: 0.691959, acc: 0.535156]  [A loss: 0.713450, acc: 0.453125]\n",
      "1772: [D loss: 0.710057, acc: 0.539062]  [A loss: 0.902362, acc: 0.140625]\n",
      "1773: [D loss: 0.684868, acc: 0.535156]  [A loss: 0.733054, acc: 0.406250]\n",
      "1774: [D loss: 0.713496, acc: 0.482422]  [A loss: 0.883673, acc: 0.171875]\n",
      "1775: [D loss: 0.690371, acc: 0.527344]  [A loss: 0.774855, acc: 0.335938]\n",
      "1776: [D loss: 0.694914, acc: 0.544922]  [A loss: 0.882795, acc: 0.183594]\n",
      "1777: [D loss: 0.704265, acc: 0.531250]  [A loss: 0.809817, acc: 0.277344]\n",
      "1778: [D loss: 0.691165, acc: 0.574219]  [A loss: 0.844010, acc: 0.187500]\n",
      "1779: [D loss: 0.700783, acc: 0.527344]  [A loss: 0.875069, acc: 0.214844]\n",
      "1780: [D loss: 0.687982, acc: 0.537109]  [A loss: 0.799873, acc: 0.289062]\n",
      "1781: [D loss: 0.706132, acc: 0.552734]  [A loss: 1.117402, acc: 0.031250]\n",
      "1782: [D loss: 0.712255, acc: 0.550781]  [A loss: 0.612688, acc: 0.691406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1783: [D loss: 0.751044, acc: 0.517578]  [A loss: 0.973238, acc: 0.109375]\n",
      "1784: [D loss: 0.701646, acc: 0.490234]  [A loss: 0.694144, acc: 0.546875]\n",
      "1785: [D loss: 0.724049, acc: 0.494141]  [A loss: 0.862494, acc: 0.207031]\n",
      "1786: [D loss: 0.693513, acc: 0.552734]  [A loss: 0.790268, acc: 0.292969]\n",
      "1787: [D loss: 0.686816, acc: 0.578125]  [A loss: 0.853024, acc: 0.203125]\n",
      "1788: [D loss: 0.686933, acc: 0.560547]  [A loss: 0.826194, acc: 0.242188]\n",
      "1789: [D loss: 0.710865, acc: 0.519531]  [A loss: 0.818490, acc: 0.273438]\n",
      "1790: [D loss: 0.709722, acc: 0.500000]  [A loss: 0.869894, acc: 0.214844]\n",
      "1791: [D loss: 0.720315, acc: 0.490234]  [A loss: 0.817041, acc: 0.277344]\n",
      "1792: [D loss: 0.701796, acc: 0.521484]  [A loss: 0.797275, acc: 0.324219]\n",
      "1793: [D loss: 0.699080, acc: 0.529297]  [A loss: 0.807741, acc: 0.296875]\n",
      "1794: [D loss: 0.690897, acc: 0.554688]  [A loss: 0.849414, acc: 0.210938]\n",
      "1795: [D loss: 0.695044, acc: 0.544922]  [A loss: 0.797930, acc: 0.285156]\n",
      "1796: [D loss: 0.711336, acc: 0.490234]  [A loss: 0.949494, acc: 0.117188]\n",
      "1797: [D loss: 0.698489, acc: 0.539062]  [A loss: 0.720831, acc: 0.484375]\n",
      "1798: [D loss: 0.721249, acc: 0.511719]  [A loss: 1.012460, acc: 0.070312]\n",
      "1799: [D loss: 0.707694, acc: 0.480469]  [A loss: 0.668977, acc: 0.550781]\n",
      "1800: [D loss: 0.713577, acc: 0.513672]  [A loss: 0.942132, acc: 0.121094]\n",
      "1801: [D loss: 0.688969, acc: 0.541016]  [A loss: 0.716826, acc: 0.500000]\n",
      "1802: [D loss: 0.705042, acc: 0.523438]  [A loss: 0.909833, acc: 0.187500]\n",
      "1803: [D loss: 0.696433, acc: 0.527344]  [A loss: 0.781717, acc: 0.324219]\n",
      "1804: [D loss: 0.693756, acc: 0.535156]  [A loss: 0.805753, acc: 0.261719]\n",
      "1805: [D loss: 0.694700, acc: 0.541016]  [A loss: 0.863373, acc: 0.183594]\n",
      "1806: [D loss: 0.692155, acc: 0.546875]  [A loss: 0.819635, acc: 0.269531]\n",
      "1807: [D loss: 0.695755, acc: 0.513672]  [A loss: 0.826496, acc: 0.250000]\n",
      "1808: [D loss: 0.696454, acc: 0.515625]  [A loss: 0.822904, acc: 0.226562]\n",
      "1809: [D loss: 0.699687, acc: 0.527344]  [A loss: 0.796169, acc: 0.328125]\n",
      "1810: [D loss: 0.692252, acc: 0.539062]  [A loss: 0.880698, acc: 0.187500]\n",
      "1811: [D loss: 0.696847, acc: 0.542969]  [A loss: 0.794232, acc: 0.296875]\n",
      "1812: [D loss: 0.688488, acc: 0.511719]  [A loss: 0.870739, acc: 0.199219]\n",
      "1813: [D loss: 0.714104, acc: 0.496094]  [A loss: 0.793980, acc: 0.277344]\n",
      "1814: [D loss: 0.704072, acc: 0.529297]  [A loss: 0.947453, acc: 0.136719]\n",
      "1815: [D loss: 0.692908, acc: 0.541016]  [A loss: 0.708970, acc: 0.460938]\n",
      "1816: [D loss: 0.711337, acc: 0.546875]  [A loss: 1.014898, acc: 0.101562]\n",
      "1817: [D loss: 0.708151, acc: 0.505859]  [A loss: 0.657835, acc: 0.628906]\n",
      "1818: [D loss: 0.736640, acc: 0.501953]  [A loss: 1.023159, acc: 0.066406]\n",
      "1819: [D loss: 0.705048, acc: 0.509766]  [A loss: 0.693112, acc: 0.500000]\n",
      "1820: [D loss: 0.731145, acc: 0.515625]  [A loss: 0.929322, acc: 0.156250]\n",
      "1821: [D loss: 0.696026, acc: 0.527344]  [A loss: 0.717348, acc: 0.464844]\n",
      "1822: [D loss: 0.695601, acc: 0.539062]  [A loss: 0.851115, acc: 0.234375]\n",
      "1823: [D loss: 0.695875, acc: 0.529297]  [A loss: 0.749917, acc: 0.414062]\n",
      "1824: [D loss: 0.704322, acc: 0.515625]  [A loss: 0.852519, acc: 0.234375]\n",
      "1825: [D loss: 0.695436, acc: 0.568359]  [A loss: 0.748977, acc: 0.390625]\n",
      "1826: [D loss: 0.720294, acc: 0.507812]  [A loss: 0.962296, acc: 0.078125]\n",
      "1827: [D loss: 0.680964, acc: 0.576172]  [A loss: 0.739081, acc: 0.402344]\n",
      "1828: [D loss: 0.701559, acc: 0.517578]  [A loss: 0.891943, acc: 0.160156]\n",
      "1829: [D loss: 0.695385, acc: 0.537109]  [A loss: 0.797768, acc: 0.300781]\n",
      "1830: [D loss: 0.700024, acc: 0.519531]  [A loss: 0.789519, acc: 0.296875]\n",
      "1831: [D loss: 0.710368, acc: 0.500000]  [A loss: 0.891006, acc: 0.167969]\n",
      "1832: [D loss: 0.694615, acc: 0.527344]  [A loss: 0.837245, acc: 0.257812]\n",
      "1833: [D loss: 0.697602, acc: 0.525391]  [A loss: 0.891786, acc: 0.156250]\n",
      "1834: [D loss: 0.702105, acc: 0.507812]  [A loss: 0.860248, acc: 0.207031]\n",
      "1835: [D loss: 0.693912, acc: 0.552734]  [A loss: 0.774322, acc: 0.339844]\n",
      "1836: [D loss: 0.706896, acc: 0.492188]  [A loss: 0.882115, acc: 0.179688]\n",
      "1837: [D loss: 0.685357, acc: 0.568359]  [A loss: 0.750634, acc: 0.410156]\n",
      "1838: [D loss: 0.723872, acc: 0.496094]  [A loss: 0.993887, acc: 0.101562]\n",
      "1839: [D loss: 0.715962, acc: 0.503906]  [A loss: 0.696346, acc: 0.511719]\n",
      "1840: [D loss: 0.723794, acc: 0.503906]  [A loss: 1.003083, acc: 0.070312]\n",
      "1841: [D loss: 0.703479, acc: 0.507812]  [A loss: 0.706441, acc: 0.515625]\n",
      "1842: [D loss: 0.719235, acc: 0.525391]  [A loss: 0.998593, acc: 0.121094]\n",
      "1843: [D loss: 0.698113, acc: 0.521484]  [A loss: 0.696102, acc: 0.535156]\n",
      "1844: [D loss: 0.705331, acc: 0.515625]  [A loss: 0.914865, acc: 0.167969]\n",
      "1845: [D loss: 0.698823, acc: 0.527344]  [A loss: 0.747275, acc: 0.410156]\n",
      "1846: [D loss: 0.703567, acc: 0.509766]  [A loss: 0.845207, acc: 0.222656]\n",
      "1847: [D loss: 0.684656, acc: 0.564453]  [A loss: 0.758667, acc: 0.386719]\n",
      "1848: [D loss: 0.709556, acc: 0.535156]  [A loss: 0.891063, acc: 0.203125]\n",
      "1849: [D loss: 0.695688, acc: 0.542969]  [A loss: 0.777610, acc: 0.343750]\n",
      "1850: [D loss: 0.690653, acc: 0.548828]  [A loss: 0.840850, acc: 0.226562]\n",
      "1851: [D loss: 0.693804, acc: 0.531250]  [A loss: 0.875830, acc: 0.175781]\n",
      "1852: [D loss: 0.698903, acc: 0.531250]  [A loss: 0.798963, acc: 0.308594]\n",
      "1853: [D loss: 0.700276, acc: 0.519531]  [A loss: 0.867426, acc: 0.199219]\n",
      "1854: [D loss: 0.687204, acc: 0.562500]  [A loss: 0.825141, acc: 0.269531]\n",
      "1855: [D loss: 0.707963, acc: 0.521484]  [A loss: 0.896795, acc: 0.167969]\n",
      "1856: [D loss: 0.707933, acc: 0.505859]  [A loss: 0.868394, acc: 0.171875]\n",
      "1857: [D loss: 0.699332, acc: 0.541016]  [A loss: 0.875965, acc: 0.179688]\n",
      "1858: [D loss: 0.692319, acc: 0.529297]  [A loss: 0.878064, acc: 0.199219]\n",
      "1859: [D loss: 0.683846, acc: 0.537109]  [A loss: 0.892026, acc: 0.187500]\n",
      "1860: [D loss: 0.705107, acc: 0.562500]  [A loss: 0.903327, acc: 0.132812]\n",
      "1861: [D loss: 0.713814, acc: 0.490234]  [A loss: 0.845534, acc: 0.230469]\n",
      "1862: [D loss: 0.699803, acc: 0.519531]  [A loss: 0.789652, acc: 0.339844]\n",
      "1863: [D loss: 0.695948, acc: 0.537109]  [A loss: 0.939445, acc: 0.167969]\n",
      "1864: [D loss: 0.692881, acc: 0.552734]  [A loss: 0.725206, acc: 0.437500]\n",
      "1865: [D loss: 0.709187, acc: 0.509766]  [A loss: 1.062263, acc: 0.046875]\n",
      "1866: [D loss: 0.702537, acc: 0.519531]  [A loss: 0.562724, acc: 0.800781]\n",
      "1867: [D loss: 0.765307, acc: 0.509766]  [A loss: 1.102041, acc: 0.062500]\n",
      "1868: [D loss: 0.707847, acc: 0.537109]  [A loss: 0.679300, acc: 0.566406]\n",
      "1869: [D loss: 0.720190, acc: 0.507812]  [A loss: 0.865416, acc: 0.218750]\n",
      "1870: [D loss: 0.707384, acc: 0.505859]  [A loss: 0.801463, acc: 0.285156]\n",
      "1871: [D loss: 0.710937, acc: 0.472656]  [A loss: 0.845456, acc: 0.250000]\n",
      "1872: [D loss: 0.682008, acc: 0.568359]  [A loss: 0.770535, acc: 0.339844]\n",
      "1873: [D loss: 0.716873, acc: 0.523438]  [A loss: 0.901894, acc: 0.179688]\n",
      "1874: [D loss: 0.680917, acc: 0.574219]  [A loss: 0.802841, acc: 0.296875]\n",
      "1875: [D loss: 0.700384, acc: 0.533203]  [A loss: 0.885114, acc: 0.214844]\n",
      "1876: [D loss: 0.699089, acc: 0.511719]  [A loss: 0.787112, acc: 0.343750]\n",
      "1877: [D loss: 0.704509, acc: 0.503906]  [A loss: 0.878724, acc: 0.195312]\n",
      "1878: [D loss: 0.691830, acc: 0.537109]  [A loss: 0.813277, acc: 0.320312]\n",
      "1879: [D loss: 0.713279, acc: 0.494141]  [A loss: 0.953671, acc: 0.117188]\n",
      "1880: [D loss: 0.699515, acc: 0.527344]  [A loss: 0.687181, acc: 0.558594]\n",
      "1881: [D loss: 0.735867, acc: 0.505859]  [A loss: 0.990572, acc: 0.097656]\n",
      "1882: [D loss: 0.696365, acc: 0.523438]  [A loss: 0.676758, acc: 0.542969]\n",
      "1883: [D loss: 0.741760, acc: 0.496094]  [A loss: 1.007660, acc: 0.078125]\n",
      "1884: [D loss: 0.691290, acc: 0.533203]  [A loss: 0.710746, acc: 0.453125]\n",
      "1885: [D loss: 0.725186, acc: 0.527344]  [A loss: 0.998221, acc: 0.078125]\n",
      "1886: [D loss: 0.706049, acc: 0.509766]  [A loss: 0.739355, acc: 0.410156]\n",
      "1887: [D loss: 0.723367, acc: 0.501953]  [A loss: 0.900929, acc: 0.171875]\n",
      "1888: [D loss: 0.694823, acc: 0.529297]  [A loss: 0.757107, acc: 0.410156]\n",
      "1889: [D loss: 0.711173, acc: 0.507812]  [A loss: 0.878361, acc: 0.203125]\n",
      "1890: [D loss: 0.710873, acc: 0.507812]  [A loss: 0.826016, acc: 0.253906]\n",
      "1891: [D loss: 0.695797, acc: 0.542969]  [A loss: 0.792454, acc: 0.343750]\n",
      "1892: [D loss: 0.704106, acc: 0.523438]  [A loss: 0.834403, acc: 0.265625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1893: [D loss: 0.697217, acc: 0.541016]  [A loss: 0.795991, acc: 0.304688]\n",
      "1894: [D loss: 0.711483, acc: 0.529297]  [A loss: 0.886300, acc: 0.175781]\n",
      "1895: [D loss: 0.701584, acc: 0.542969]  [A loss: 0.862261, acc: 0.218750]\n",
      "1896: [D loss: 0.702549, acc: 0.535156]  [A loss: 0.778061, acc: 0.375000]\n",
      "1897: [D loss: 0.705870, acc: 0.490234]  [A loss: 0.927968, acc: 0.156250]\n",
      "1898: [D loss: 0.699569, acc: 0.515625]  [A loss: 0.775148, acc: 0.367188]\n",
      "1899: [D loss: 0.704102, acc: 0.517578]  [A loss: 0.920981, acc: 0.152344]\n",
      "1900: [D loss: 0.711657, acc: 0.509766]  [A loss: 0.802379, acc: 0.312500]\n",
      "1901: [D loss: 0.696442, acc: 0.517578]  [A loss: 0.893804, acc: 0.164062]\n",
      "1902: [D loss: 0.687439, acc: 0.562500]  [A loss: 0.794887, acc: 0.335938]\n",
      "1903: [D loss: 0.705943, acc: 0.505859]  [A loss: 0.894863, acc: 0.187500]\n",
      "1904: [D loss: 0.676472, acc: 0.546875]  [A loss: 0.799974, acc: 0.320312]\n",
      "1905: [D loss: 0.700638, acc: 0.554688]  [A loss: 0.928632, acc: 0.160156]\n",
      "1906: [D loss: 0.682297, acc: 0.554688]  [A loss: 0.717301, acc: 0.496094]\n",
      "1907: [D loss: 0.710508, acc: 0.533203]  [A loss: 1.036811, acc: 0.066406]\n",
      "1908: [D loss: 0.692243, acc: 0.539062]  [A loss: 0.668167, acc: 0.609375]\n",
      "1909: [D loss: 0.739361, acc: 0.503906]  [A loss: 1.073927, acc: 0.035156]\n",
      "1910: [D loss: 0.713378, acc: 0.505859]  [A loss: 0.675073, acc: 0.531250]\n",
      "1911: [D loss: 0.714305, acc: 0.511719]  [A loss: 0.815370, acc: 0.308594]\n",
      "1912: [D loss: 0.686593, acc: 0.542969]  [A loss: 0.838225, acc: 0.214844]\n",
      "1913: [D loss: 0.702871, acc: 0.511719]  [A loss: 0.788147, acc: 0.308594]\n",
      "1914: [D loss: 0.713623, acc: 0.515625]  [A loss: 0.974236, acc: 0.082031]\n",
      "1915: [D loss: 0.695203, acc: 0.539062]  [A loss: 0.704031, acc: 0.500000]\n",
      "1916: [D loss: 0.724866, acc: 0.484375]  [A loss: 0.950197, acc: 0.113281]\n",
      "1917: [D loss: 0.700772, acc: 0.542969]  [A loss: 0.733935, acc: 0.457031]\n",
      "1918: [D loss: 0.709026, acc: 0.533203]  [A loss: 0.925640, acc: 0.132812]\n",
      "1919: [D loss: 0.697827, acc: 0.494141]  [A loss: 0.725374, acc: 0.445312]\n",
      "1920: [D loss: 0.701603, acc: 0.578125]  [A loss: 1.009925, acc: 0.070312]\n",
      "1921: [D loss: 0.718760, acc: 0.496094]  [A loss: 0.708051, acc: 0.496094]\n",
      "1922: [D loss: 0.714316, acc: 0.531250]  [A loss: 0.917296, acc: 0.128906]\n",
      "1923: [D loss: 0.690503, acc: 0.537109]  [A loss: 0.730584, acc: 0.417969]\n",
      "1924: [D loss: 0.710911, acc: 0.517578]  [A loss: 0.943256, acc: 0.113281]\n",
      "1925: [D loss: 0.703288, acc: 0.500000]  [A loss: 0.758774, acc: 0.378906]\n",
      "1926: [D loss: 0.713461, acc: 0.492188]  [A loss: 0.889821, acc: 0.171875]\n",
      "1927: [D loss: 0.691684, acc: 0.529297]  [A loss: 0.748634, acc: 0.394531]\n",
      "1928: [D loss: 0.691825, acc: 0.550781]  [A loss: 0.884686, acc: 0.234375]\n",
      "1929: [D loss: 0.699839, acc: 0.519531]  [A loss: 0.803477, acc: 0.285156]\n",
      "1930: [D loss: 0.701267, acc: 0.517578]  [A loss: 0.840271, acc: 0.273438]\n",
      "1931: [D loss: 0.708024, acc: 0.523438]  [A loss: 0.780890, acc: 0.355469]\n",
      "1932: [D loss: 0.715973, acc: 0.505859]  [A loss: 0.930558, acc: 0.113281]\n",
      "1933: [D loss: 0.693505, acc: 0.552734]  [A loss: 0.653795, acc: 0.609375]\n",
      "1934: [D loss: 0.698232, acc: 0.542969]  [A loss: 0.981643, acc: 0.105469]\n",
      "1935: [D loss: 0.698427, acc: 0.531250]  [A loss: 0.721299, acc: 0.464844]\n",
      "1936: [D loss: 0.721258, acc: 0.511719]  [A loss: 0.913035, acc: 0.156250]\n",
      "1937: [D loss: 0.683484, acc: 0.537109]  [A loss: 0.696571, acc: 0.511719]\n",
      "1938: [D loss: 0.713657, acc: 0.509766]  [A loss: 0.916649, acc: 0.156250]\n",
      "1939: [D loss: 0.689603, acc: 0.533203]  [A loss: 0.727073, acc: 0.417969]\n",
      "1940: [D loss: 0.721090, acc: 0.498047]  [A loss: 1.006353, acc: 0.089844]\n",
      "1941: [D loss: 0.693431, acc: 0.537109]  [A loss: 0.683338, acc: 0.539062]\n",
      "1942: [D loss: 0.710005, acc: 0.544922]  [A loss: 0.918988, acc: 0.179688]\n",
      "1943: [D loss: 0.698835, acc: 0.533203]  [A loss: 0.726391, acc: 0.464844]\n",
      "1944: [D loss: 0.714145, acc: 0.513672]  [A loss: 0.841651, acc: 0.199219]\n",
      "1945: [D loss: 0.691350, acc: 0.533203]  [A loss: 0.847176, acc: 0.218750]\n",
      "1946: [D loss: 0.698725, acc: 0.511719]  [A loss: 0.803571, acc: 0.316406]\n",
      "1947: [D loss: 0.692342, acc: 0.535156]  [A loss: 0.841097, acc: 0.210938]\n",
      "1948: [D loss: 0.688872, acc: 0.583984]  [A loss: 0.826167, acc: 0.265625]\n",
      "1949: [D loss: 0.703115, acc: 0.527344]  [A loss: 0.885366, acc: 0.164062]\n",
      "1950: [D loss: 0.703031, acc: 0.503906]  [A loss: 0.818781, acc: 0.273438]\n",
      "1951: [D loss: 0.695185, acc: 0.513672]  [A loss: 0.853014, acc: 0.218750]\n",
      "1952: [D loss: 0.696022, acc: 0.529297]  [A loss: 0.811884, acc: 0.242188]\n",
      "1953: [D loss: 0.688633, acc: 0.560547]  [A loss: 0.891288, acc: 0.195312]\n",
      "1954: [D loss: 0.698098, acc: 0.521484]  [A loss: 0.790233, acc: 0.320312]\n",
      "1955: [D loss: 0.703367, acc: 0.523438]  [A loss: 0.930785, acc: 0.156250]\n",
      "1956: [D loss: 0.708924, acc: 0.486328]  [A loss: 0.762220, acc: 0.351562]\n",
      "1957: [D loss: 0.694653, acc: 0.554688]  [A loss: 1.041193, acc: 0.074219]\n",
      "1958: [D loss: 0.694249, acc: 0.544922]  [A loss: 0.697401, acc: 0.503906]\n",
      "1959: [D loss: 0.714414, acc: 0.500000]  [A loss: 0.993130, acc: 0.132812]\n",
      "1960: [D loss: 0.694510, acc: 0.552734]  [A loss: 0.682527, acc: 0.519531]\n",
      "1961: [D loss: 0.699838, acc: 0.550781]  [A loss: 0.986430, acc: 0.070312]\n",
      "1962: [D loss: 0.689705, acc: 0.564453]  [A loss: 0.620071, acc: 0.664062]\n",
      "1963: [D loss: 0.746665, acc: 0.511719]  [A loss: 1.085138, acc: 0.046875]\n",
      "1964: [D loss: 0.710103, acc: 0.511719]  [A loss: 0.698286, acc: 0.484375]\n",
      "1965: [D loss: 0.716579, acc: 0.480469]  [A loss: 0.856305, acc: 0.214844]\n",
      "1966: [D loss: 0.708368, acc: 0.517578]  [A loss: 0.855281, acc: 0.222656]\n",
      "1967: [D loss: 0.701353, acc: 0.535156]  [A loss: 0.816979, acc: 0.292969]\n",
      "1968: [D loss: 0.712576, acc: 0.498047]  [A loss: 0.877877, acc: 0.183594]\n",
      "1969: [D loss: 0.716644, acc: 0.498047]  [A loss: 0.751018, acc: 0.390625]\n",
      "1970: [D loss: 0.713358, acc: 0.496094]  [A loss: 0.860236, acc: 0.238281]\n",
      "1971: [D loss: 0.699872, acc: 0.507812]  [A loss: 0.796107, acc: 0.304688]\n",
      "1972: [D loss: 0.707523, acc: 0.515625]  [A loss: 0.937051, acc: 0.164062]\n",
      "1973: [D loss: 0.693044, acc: 0.509766]  [A loss: 0.731457, acc: 0.449219]\n",
      "1974: [D loss: 0.719622, acc: 0.501953]  [A loss: 0.910197, acc: 0.148438]\n",
      "1975: [D loss: 0.690328, acc: 0.544922]  [A loss: 0.761990, acc: 0.339844]\n",
      "1976: [D loss: 0.717748, acc: 0.527344]  [A loss: 0.915249, acc: 0.125000]\n",
      "1977: [D loss: 0.702036, acc: 0.519531]  [A loss: 0.741458, acc: 0.425781]\n",
      "1978: [D loss: 0.715486, acc: 0.517578]  [A loss: 1.009549, acc: 0.093750]\n",
      "1979: [D loss: 0.702425, acc: 0.523438]  [A loss: 0.711824, acc: 0.484375]\n",
      "1980: [D loss: 0.716017, acc: 0.525391]  [A loss: 0.944641, acc: 0.121094]\n",
      "1981: [D loss: 0.676553, acc: 0.564453]  [A loss: 0.762657, acc: 0.390625]\n",
      "1982: [D loss: 0.705912, acc: 0.535156]  [A loss: 0.889715, acc: 0.175781]\n",
      "1983: [D loss: 0.696336, acc: 0.552734]  [A loss: 0.764817, acc: 0.335938]\n",
      "1984: [D loss: 0.704030, acc: 0.525391]  [A loss: 0.891573, acc: 0.160156]\n",
      "1985: [D loss: 0.691277, acc: 0.535156]  [A loss: 0.736240, acc: 0.425781]\n",
      "1986: [D loss: 0.718404, acc: 0.515625]  [A loss: 0.936065, acc: 0.121094]\n",
      "1987: [D loss: 0.695212, acc: 0.537109]  [A loss: 0.728376, acc: 0.445312]\n",
      "1988: [D loss: 0.699976, acc: 0.539062]  [A loss: 0.905972, acc: 0.171875]\n",
      "1989: [D loss: 0.701474, acc: 0.509766]  [A loss: 0.753958, acc: 0.386719]\n",
      "1990: [D loss: 0.712572, acc: 0.517578]  [A loss: 0.967272, acc: 0.113281]\n",
      "1991: [D loss: 0.682557, acc: 0.562500]  [A loss: 0.704211, acc: 0.503906]\n",
      "1992: [D loss: 0.722886, acc: 0.517578]  [A loss: 0.851355, acc: 0.207031]\n",
      "1993: [D loss: 0.688255, acc: 0.576172]  [A loss: 0.825833, acc: 0.250000]\n",
      "1994: [D loss: 0.703378, acc: 0.517578]  [A loss: 0.804132, acc: 0.300781]\n",
      "1995: [D loss: 0.711344, acc: 0.515625]  [A loss: 1.042775, acc: 0.062500]\n",
      "1996: [D loss: 0.682437, acc: 0.566406]  [A loss: 0.685550, acc: 0.531250]\n",
      "1997: [D loss: 0.710287, acc: 0.519531]  [A loss: 0.925894, acc: 0.171875]\n",
      "1998: [D loss: 0.701172, acc: 0.507812]  [A loss: 0.722424, acc: 0.437500]\n",
      "1999: [D loss: 0.715765, acc: 0.537109]  [A loss: 0.888433, acc: 0.203125]\n",
      "Elapsed: 23.47283943891525 min \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAALICAYAAACJnL11AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xe0XXW1P+x10itJSAIBQgi9NylSJNKLIALmcvXegRTFa0HERrkKAlHwqhcFRYoCoiIiSBERRKVKEVGxQBACiCEhkZBE0uv5/fEOh+9lzq+uk33aPud5/vyM1cL57rUna+y5Zktra2sFAABEfbr6AgAAoLtSLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAo6NeZJ2tpaTEukIa1tra2dNW5rWHaQ1eu4aqyjmkf7sU0u7pr2JNlAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgoF9XXwAANKJPn/jcp7W1Nd22lAOUeLIMAAAFimUAAChQLAMAQIFiGQAACjT4JQYPHhyyb37zmyGbNGlSyObMmZMe8/HHH6917g033DBkTzzxRMi+8Y1vpPtPnz49ZEuXLg1Z1uRSNwNoTy0tLWm+4447huzSSy8N2ahRo0J27733pse84IILQjZz5syQufcBf+fJMgAAFCiWAQCgQLEMAAAFimUAACho6cwmhpaWlqbtmBg2bFjI9ttvv5AdddRR6f4HH3xwyMaPH9/4hdXw7LPPhmyvvfYKWak5sbtpbW3Nu4E6QTOv4ayJqm/fviFbtWpVur+Gp/bTlWu4qrrfOi41+B166KEhu+SSS0I2bty4kA0cODA95ooVK0I2e/bskN18880hO/vss0OWNVBXVe/4vLgX0+zqrmFPlgEAoECxDAAABYplAAAoUCwDAECBBr8GDB06NGQ33XRTuu2b3/zmkPXv3z9kixcvDtnq1atDNnz48PQ8WcPWypUrQzZ58uSQ3XbbbekxuxtNJf9Qaoy66KKLQvaBD3wgZP36xSGeCxYsSI85derUkGWTKZ9++umQ3XXXXSHLpk1WVb5es89AM9Pg939l67Cq8ml7H/nIR2rv396y5sBSQ+yiRYtClt2fly1bFrJHHnkkZNm/u6ry6YPZdXbEd717Mc1Ogx8AADRIsQwAAAWKZQAAKFAsAwBAgQa/Bhx44IEhu/POO2vvf+utt4bsP//zP0OWNTdNnDgxPebPf/7zkG2wwQYhy6b67bDDDiHLGkW6mqaSfzj66KPT/Pvf/37IOqsJKrunzJgxI2Sf+9zn0v2zRtMlS5aELGuQnTdvXsiWL19e+zo7iwa//6t0P3vooYdCtv7663fw1XRPzzzzTJp/6EMfCtmjjz4astdee63dr8m9uGOVGrgffvjhkO24444hy77nTzzxxJD99re/Tc9jCuU/eLIMAAAFimUAAChQLAMAQIFiGQAAChTLAABQ0Dnt8T1A1pV68803h6z0xoFsJOopp5wSslLn/utNmzYtzTfddNOQ/cd//EfIzjvvvJBtu+22IXviiSdqXQ8dL1tb++23XxdcyT+XfVZGjhwZsp133jndf+7cuSE75JBDQrbVVluF7P777w/Zf//3f6fnKY0ppmONHj06ZNdcc0267XrrrdfRl9PlsjcOZKOy77333nT/V199NWQjRowIWUe8DYOO9dJLL6V59kaYlStXhix7y8xdd90Vsvnz56fn2WeffUI2e/bsdNuezpNlAAAoUCwDAECBYhkAAAoUywAAUGDcdU1Zw0TpR/GZz3zmMyE7++yzG7qmRmTjMjfeeOOQdccGm946YrVPn/j/tptsskm67Ve+8pWQ7b333iEbPHhwyDpiLHZ2nymNUs+aXAcNGhSyrJHwqaeeClmpkbArG/x6y7jrAQMGhOyqq64K2Tve8Y50/85ai9layJrsbrjhhpBdffXV6XnmzJkTsnXXXTdka621VshGjRoVsmy8e1VV1dprrx2yvn37huyiiy4KWaPf/731Xtyo7G/54x//OGQHHHBAuv9BBx0Usp///Oe1zn3ZZZeF7H3ve1+67dSpU0O23XbbhWz16tW1zt0dGXcNAAANUiwDAECBYhkAAAoUywAAUKDBr6Yvf/nLIfvwhz8csr/97W/p/lnDRmf+t3+9rBlg3333DdmYMWPS/efNm9fel1SbppJ/LWvcO/zww0N2zjnnhCxr9KyqvGkoy7LmlawZr1HLli0L2WGHHRay0uSzrtRbGvyyBqVbb701ZEOHDk33r7tusgajmTNnpttm57/44otD9sILL4Ss0abQ7PMyfvz4kO21114hO+KII9JjTpo0KWTZf48tttgiZNlnqC3ci/+1bA1/73vfC9nkyZNDVvqb33nnnWt8PQceeGDIfvrTn6bbZusoa1LNmlmbhQY/AABokGIZAAAKFMsAAFCgWAYAgAINfolsat2LL74Ysmy6VNasUVXlZpOucvTRR4fspptuCtmWW26Z7j9t2rR2v6a6NJW0n9GjR4esNDUqa/w79thjQ7bTTjuFLJs+WJLdk2bNmhWy/fffP2RPP/107fN0pZ7Y4JfdD++5556QZZMkG10fCxYsCFlpKtnNN98cskYb3TJZM9/HP/7xkJ133nkhy/5btuW/0cqVK0M2duzYkJUa0utyL/7Xsqbj66+/PmQPPfRQyLKm7EZNnDgxZFkza8kJJ5wQsmuvvbaBK+paGvwAAKBBimUAAChQLAMAQIFiGQAAChTLAABQEFtue5FSd/ETTzwRsmyE7/Tp00PW3d56UfKhD30oZNl/j9K46658Gwbt59VXXw3ZD3/4w3Tbgw8+OGRZ1302IjUb+VoaHZy90SJ7e4s12L0MGDAgZOuvv37I2jL6PHvzxZIlS0J29913h+yBBx5Ij5mt2UaMHDkyzbN1nI0Krqv05qrly5eHLHsTx3bbbRey7A0MtK9svHr29pXSaOv2NmPGjJBla6iq8s/09ttv3+7X1Aw8WQYAgALFMgAAFCiWAQCgQLEMAAAFvbrB713velear7POOrX2f8tb3tKel9Opdt5551rbTZ06tYOvhO6m1AD15je/OWSbbrpprWNmzUlZI2BVVdXixYtDNn/+/FrnoetkDZvPPfdcyDbccMPax8waUO+8886QZeN2Bw4cmB5zzz33DNlLL70UsqwJ64ILLgjZcccdl54na7Kra86cOSG75ppr0m2z0dhZA/fXv/71kG2zzTZrcHW0RfZygKeeeipkpQbO9pbdd3/2s5+l22ajuhtZ183Mk2UAAChQLAMAQIFiGQAAChTLAABQ0Ksb/M4+++za22ZTo5q5+S1rCsksWrSog6+E7iZrSKmqqjrmmGNCtvbaa4csm9CWNa+U1mDWKLZw4cJ0W7qPbArY5z//+ZBlU0FLU/0efPDBkGWNbq+88krIxo4dmx5zn332CVnW6JZNrKzb/F2SreN3vvOdIbv33ntDtnTp0vSYo0ePDtkHPvCBkI0fPz5kpf/undVs1lttvPHGnXKe7B6b3V/vu+++dP/sJQY33nhjw9fVjDxZBgCAAsUyAAAUKJYBAKBAsQwAAAW9usFvwoQJtbe9/vrrQ5b9UL5ZDBo0qNZ2zfxvZM0MGDAgzdvSmNWIoUOHhqw0VZDuI2sK++Uvfxmyz3zmMyHLpupVVb7mjj766JBl63DUqFHpMbOmpWyqYKnR9fVKkyj/+Mc/huzAAw8MWTatry0NdtnEyxUrVtTaV4Nfx8uaNffdd9+QbbfddiHL1lBJtl6z7+/sb/vud7+79nkee+yx2tv2JJ4sAwBAgWIZAAAKFMsAAFCgWAYAgIJe3eDXp0/+/wrZD+DbMu2vGWT/xroZPVupqbORtdCWRtGsYar0WaV7yybW/eQnPwnZ/Pnz0/2vvPLKkGWN2VlzU0c0n2afgWx6YFVV1fnnnx+ybBJsNmUt+wyUGg7PO++8kA0ePDhk8+bNq3Ue2lfW1JlNwcteIvCpT30qPeYdd9wRsrpNndm9dPPNN0+3zRqre2uztW8gAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAgl7zNoysA7TULZ11lf71r39t92vqStl/D2++oKqqaunSpWmejV59wxveELLs85N14q+zzjrpeUaMGBGytdZaK2TZmGC6v2x9ZW+JqKp8DHVpHHt7W7ZsWchmzZoVsgcffDDdf+LEiSE766yzQrbDDjuE7E9/+lPIdtttt/Q8WZ695aI0UpyOlX2vTp48OWSbbbZZyLbaaqv0mNmbUeq+pWLcuHEhK71taObMmbWO2Rt4sgwAAAWKZQAAKFAsAwBAgWIZAAAKek2DX9bwUGrw69u3b8iaecRj1hyV/aA/a2ih9ymNpp4yZUrIjjjiiJBlzbC77LJLyA466KD0PGPHjg3ZPvvsE7Jbbrkl3Z/uLfv7XnbZZem2AwcObPfzZ+v797//fcguvvjikP32t78NWXZ/raqqetvb3hayf/u3fwvZeuutF7KDDz44ZKXvq2yk+EknnRSyadOmpfvTPWR/n474m2UNfqXm/q985Svtfv5m5ckyAAAUKJYBAKBAsQwAAAWKZQAAKOg1DX6ZUtNe1uCXNcRlTYPd0ZFHHhmy7NpLk6joXUrTnJ555pmQXXfddSFbe+21Q5Y1ApYmsWXTqbKJVxr8ur+sKe3kk08OWWlSWSNKDcvPPvtsyLJ1nDXzvfbaayHLGharqqrGjx8fsmHDhoUs+x7K7s9PPfVUep5///d/D9mf//zndFu4/PLLa2971113deCVNBdPlgEAoECxDAAABYplAAAoUCwDAEBBr27wu//++9P8gAMOCFnWoPTDH/6w3a+prtI0p/XXXz9kl1xySciyKVaf/exnG78wmt7o0aPT/KyzzgrZFltsEbJsDW688cYhKzUSZnbbbbeQZZ+B0iQqukb2N37uuedCNnXq1HT/zTffPGTz588P2fPPPx+ymTNnpsfM1kjWeLftttuGLLtvZuu9qvLJfi+88ELI5s6dG7KHHnooZKUph3PmzElzyGTNtKX7Zukz1Bt5sgwAAAWKZQAAKFAsAwBAgWIZAAAKWjqzIaalpaVbdd+UGoyyiUpZttFGG4Xs5ZdfbuiasqalESNGhGy//fZL9//GN74RspEjR4ZsxowZIcuatZYuXZqepyu1trbm3Y2doLut4Y5www03pHk2CbJfv9gjnH2u2tLMl8mmB7alUaW76co1XFWdt46zaajbb799yDbbbLN0/6x5bsGCBSFbsWJFyA455JD0mHvvvXfIsqmR8+bNC1l235w1a1Z6nuy+u3z58pBlE9Uef/zxkC1cuDA9T1eueffi7i2772ZrsCSbxppNsWxmddewJ8sAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFvXrc9erVq9P8jjvuCNnhhx8eshdffDFkn/70p9Nj3nPPPSHbZpttQnbGGWeELOsUz7rMS7Lu8YMPPjhk3fHNF3Ss7C0A2dqoqqoaOHBgyEpj1xuRdfdno+mb5c0XvVl2j81GU2dvmaiqqho1alTIJk6cGLLDDjssZLvuumt6zNI499cbMGBAyIYOHVorq6r8zUjXXXddyB555JGQuRfTHuq+hSh721dV5W+Z6a08WQYAgALFMgAAFCiWAQCgQLEMAAAFvbrBr+Soo44K2aOPPhqyXXbZJWQXXHBBh1zT65WaE+fMmROyo48+OmTTpk1r92ui+WRNcqW11RHNfJms2eT222/vlHPTvrL1tWTJkpCtWrUq3T9rgt5xxx1D9qY3vSlkw4YNS4+ZjevNRltnsvvmk08+mW772GOPhey+++4LmWY+OkrW4Jfdx0svDBgyZEjIss9vb+DJMgAAFCiWAQCgQLEMAAAFimUAACjQ4JfIGpz233//kE2ZMiVkxx13XHrM7IfyixcvDtkNN9wQsvPPPz9kr7zySnqe7NpNOqMkawDp169rbwtZw1Q2AZPuL2smaktT6aJFi0K24YYbhmzMmDEhK63jrEEpa4x+8MEHQ/bTn/40ZM8991x6nvnz54dMMx+dKWucXb58echKn5Utt9wyZA8//HDjF9aEPFkGAIACxTIAABQolgEAoECxDAAABS2d2fzV0tKi04yGtba2ds4ouURvXsNZs9bAgQNDttVWW4Vst912C9lPfvKT9Dx/+ctf1uDqmktXruGq6t3rmPbjXtx8Tj311JB98pOfTLd929veFrJsmnEzq7uGPVkGAIACxTIAABQolgEAoECxDAAABYplAAAo8DYMmo4ObJqdt2HQE7gXN58+feIz0mxcfFXlY+BLo+mblbdhAABAgxTLAABQoFgGAIACxTIAABRo8KPpaCqh2WnwoydwL6bZafADAIAGKZYBAKBAsQwAAAWKZQAAKOjUBj8AAGgmniwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKCgX2eerKWlpbUzz0fP1Nra2tJV57aGaQ9duYaryjqmfbgX0+zqrmFPlgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgIJ+XX0BAAD0Ti0tLSFrbW3tgisp82QZAAAKFMsAAFCgWAYAgALFMgAAFGjwAzrc1KlTQ7bFFluk206ZMiVk5557bntfErTZKaecErLNN9883fbDH/5wR18OtIu+ffum+eDBg0O2bNmykB1yyCEh++Y3vxmyRx55JD3P5MmTa52nK3myDAAABYplAAAoUCwDAECBYhkAAApaOnNKSktLS/cayUJTam1tjeN+Ook1/K/tscceISs1dmR+8YtfhGyfffZp6Jq6m65cw1VlHddx5plnhuzCCy+svf+wYcNCtmjRooauqbtxL24+2bS8bK1WVVVttNFGIXv7298esrPPPjtkWW259tprp+dZsGBBmneGumvYk2UAAChQLAMAQIFiGQAAChTLAABQYIIfsMayZpHvfOc7DR1z7ty5De0PbTV06NCQnXPOOQ0dc/ny5Q3tDx0ha7wrNdg99dRTIdtvv/1C1qdPfO66cOHCkC1ZsqTOJXZLniwDAECBYhkAAAoUywAAUKBYBgCAAg1+naRv374hO/DAA0P25je/OWR33313yM4///z0PHvuuWfIZs2aFbIJEyaErDOnOdIzDBgwIGRjx46tte+KFSvS/KyzzmromuCfGThwYMieeOKJkA0aNKjW8UqNfKtWrWrbhUE3k9UtH/3oR0OWNXrPmzevQ66pq3iyDAAABYplAAAoUCwDAECBYhkAAAoUywAAUOBtGO0s6wqtqqq66qqrQnbooYeG7P777w/ZpEmTQrbXXnul58m6V8ePHx+yESNGhGz+/PnpMaFk+PDhIRsyZEitfWfPnp3m06ZNa+ia4J953/veF7LsHpnJ3hi0aNGi2ttCd1SqW04//fSQbbTRRiHL1npW86xcuXINrq578GQZAAAKFMsAAFCgWAYAgALFMgAAFGjwa2dZ015VVdXxxx8fsmuuuSZk73nPe0K2evXqkK299trpeT784Q//q0usqqqqXnvttVrbwT+z2267hSxrMs08++yzaV4agw1tkTWfVlVVfehDHwpZdo/Nskxp3DU0i3XXXTfNzzzzzJBlzYDZywEuvfTSxi+sG/FkGQAAChTLAABQoFgGAIACxTIAABRo8GvAxIkTQ3bHHXek2y5evDhkJ5100hqfO5usU1VVdcopp4Rs1apVIavbvAL/zK677hqyrAEkW2/33XdfekyTz2gPe+65Z5rPmDEjZNddd13ITj755JCts846Ias7sRK6g379Ytl32223pdsOGzas1jHPP//8kL366qttu7BuzpNlAAAoUCwDAECBYhkAAAoUywAAUKDBr6bsh+7ZBLJSc9LQoUPb/Zrqyn7QnzVhaazin8nWzLHHHltr32wq3913393wNUFVVdWYMWNCljU7V1VVnXPOOSH79a9/HbJRo0bVOmapwc89lq7Wp098Hvroo4+GbJdddql9zHnz5oXssssua9uFNSFPlgEAoECxDAAABYplAAAoUCwDAECBYhkAAAq8DSPRt2/fkP3iF78IWfaWie23375Drun1Vq5cmeZZt3XWEasrm7YaOHBgyDbffPNa+86ePTtkTz/9dMPXBFWVj6veZptt0m2zN18sXLgwZNdee23ISm/YyLjH0qjsjSpVlX+nT5o0KWQ33XRTyNZee+3a58/W8JQpU0K2bNmy2sdsVp4sAwBAgWIZAAAKFMsAAFCgWAYAgAINfoldd901ZFtuuWXIZs2aFbI//vGPHXJNr5f9wL+q8oaAUjMgtEXWQDJgwIBa+2bNfIsXL274muh9snvcvvvuG7IlS5ak+5fy18vum1nDU1uaraFk7733Dtm5556bbvvGN74xZMOHD1/jc69evTrN58+fH7LXXnttjc/TzDxZBgCAAsUyAAAUKJYBAKBAsQwAAAW9usGvNB3nve99b8iyqX6f+cxn2v2aMtl17rfffum2WePf73//+3a/Jnqu0uciW++lbV9v9OjRIevfv3+67fLly2sdE/4uu++VGp623nrrkD3//PMhu/7660OWrXeNqrTV4MGDQ/bVr341ZDvssEO6f6nBv47s/nr33Xen22YvNjj22GNDduutt4bs1VdfXYOr6748WQYAgALFMgAAFCiWAQCgQLEMAAAFvbrBr/Qj+alTp4Zs2bJlITv99NNDVmr2yM518sknhyz74f/xxx8fsnPOOSc9T9aAcswxx6TbQqZfv/y2kDV71LXWWmuFzIQz1kS2bubNmxeyMWPGpPvfc889IbvwwgtDNmTIkJBl3wN33HFHeh6oqnwdXX311SHbfvvtQ9ZII19VVdVLL70Usn322Sdkc+bMSfe/4oorQnb00UeH7IILLgjZf/3Xf9W5xKbhyTIAABQolgEAoECxDAAABYplAAAo6NUNfqXpY/fdd1/IRowYEbLJkyeH7Morr0yPmU0AzJpF3vjGN4bsySefDFk2Ea2qqmr16tUhmz59erotZIYOHZrmWaNKXU8//XTIli5dusbHg/+/rGnpscceS7fNJkfuvPPOIZs1a1bIsqlk1157bZ1LpIcrNeNl03ZHjhwZskWLFoWsNOV04cKFIfvEJz4Rsm9/+9shy2qEktmzZ4ds4MCBIctqofe///0Nnbu78WQZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgoKUzR862tLQ07Xzb0pszXi9760VV5SNaV61atcbXs2DBgjTPOlUHDBiwxufpjlpbW+v9MTpAM6/hunbcccc0/81vfhOyuuNYb7vttpBlY1OrqneMwe7KNVxVvWMdr7vuuml+3HHHhezFF18M2emnnx6y7C0GBxxwQHqev/zlL//qEpuee/E/7Lrrrmn+zne+M2TZG7ceeOCBkJW+59v7rRKl+3j2uRg/fnzIsrdzZG8Q645vw6i7hj1ZBgCAAsUyAAAUKJYBAKBAsQwAAAW9etx1W9RtOlq5cmW7nztrLiyNJM5+kA9tkY1cr6r6Ta7ZZ+WVV15p6JqgrbJRvVVVVV/60pdCtt5664Vs7bXXDtmYMWNCVmrCoucaO3ZsyK688sp02+9+97sh+8lPfhKy5cuXN35hNWQvAbj88svTbddff/1ax3zppZdC1tMatT1ZBgCAAsUyAAAUKJYBAKBAsQwAAAUa/JrA7rvvHrJSs1XWvAJt8Y53vCPN6zb4ZZMpf/e73zV0TdBesvW5dOnSkGVNf9k01J42IZV/7fvf/37Itt1223TbRYsWhWzFihXtfk39+sVy7m1ve1vIvvnNb4as9MKA7J6fNbS++93vDpkGPwAA6CUUywAAUKBYBgCAAsUyAAAUtHTmj7BbWlp61i++O8m0adNCtskmm6TbDho0KGSdNRmos7S2ttbrNOsAvWENv/DCC2k+ceLENT7ma6+9FrJx48al2y5ZsmSNz9MsunINV1XvWMdtMWLEiJBlU8kGDx4csmyqX1VV1fz58xu/sG6ut96Ls3tk6f54/fXXh+yEE04IWTb9N1tvVVVVhx56aMi++MUvhmzChAkh69On/jPSbPLqLrvsErJmnuBXdw17sgwAAAWKZQAAKFAsAwBAgWIZAAAKTPBrAiNHjgzZ3Llz0217WjMfne9Tn/pUmn/nO99Z42OutdZaIfv4xz+ebjtlypQ1Pg+siWwqWdaUmk3r64hpbHRvr776asg22mijdNuskTm7H2b7/+xnP0uPmTWk1p2wmnn55ZfT/MADDwzZ9OnT1/g8zcyTZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgAJvw+hm+vWLf5Isu/zyyzvjcuiFbrnlljS///77Q7b33nuHLFuvq1atCtnixYvX4Oqg/WWjefv37x+y7I0D2ZhierZLL700ZFdccUW67W677RayX/7ylyHLxmW3ZTR1XQ8++GDIJk+enG6bjbvurTxZBgCAAsUyAAAUKJYBAKBAsQwAAAUa/LqZHXfcMWSDBg0K2Y033tgZl0MvVGq823fffWvtnzWlZONZs3HC0F2MHDkyZH379g1ZNgK7qqpq2bJl7X5NdA9Zk1zp7z1s2LBaWaN++9vfhuyggw4K2dy5c0OWNbjyf3myDAAABYplAAAoUCwDAECBYhkAAApaOvOH3S0tLX5F/i8sWrQoZAMHDgxZ1jBV2r+naW1tjWO0Ook1THvoyjVcVdZxHQsWLKi13ejRo9N8+fLl7Xk53VJvvRdn0x2z5vyqqqopU6aEbN111w1Z1rT/ta99LT1m1hytSW/N1F3DniwDAECBYhkAAAoUywAAUKBYBgCAAg1+XailJf6znRX0AAAgAElEQVSufMmSJSG77rrrQvbud7+7Q66pGfTWphJ6Dg1+9ATuxTQ7DX4AANAgxTIAABQolgEAoECxDAAABYplAAAo6NfVF9CbZW8i2WijjUI2e/bszrgcAABex5NlAAAoUCwDAECBYhkAAAoUywAAUGDcNU3HiFWanXHX9ATuxTQ7464BAKBBimUAAChQLAMAQIFiGQAACjq1wQ8AAJqJJ8sAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQEG/zjxZS0tLa2eej56ptbW1pavObQ3THrpyDVeVdUz7cC+m2dVdw54sAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUNCvqy8AANpb375903z16tUha21t7ejLgabT0tISsjvuuCNkQ4YMCdl+++0Xsmb+nHmyDAAABYplAAAoUCwDAECBYhkAAAo0+DWBPn3i/9Osu+666bZHHnlkyGbNmhWyH/3oRyFbtWrVGlwddLx+/eKt6pJLLgnZZz/72ZDNmDGjQ66JNdO/f/+QjRkzJt12m222CdkJJ5wQsn333TdkK1asSI85fPjwkGWNRxdffHHIsvUFPdWmm24askMOOSRkkydPDlkzN/NlPFkGAIACxTIAABQolgEAoECxDAAABS2d+SPslpaWnvWL78SWW26Z5gMHDgzZs88+G7JsutSAAQNC9o53vCM9z5QpU0I2cuTIkP34xz8O2bHHHhuylStXpufpSq2trXGsUCfpDWu4q2Xr/Q9/+EPIJkyYELLjjjsuZDfddFP7XFg76so1XFWdt46zCWDDhg0L2bnnnpvuf/LJJ4csmxbWlu+xrFm0Edl9s6qq6sYbb2zX83RH7sU924UXXhiyM888M2TZZ3rRokUdck3tre4a9mQZAAAKFMsAAFCgWAYAgALFMgAAFJjgV1M2Re/2228P2XbbbZfunzUeZRPzsga/7NwlgwYNClnWXHjUUUeFbObMmSE78MAD0/NkDVc9bWJPb5A1YHXW3zFbl1VVVTfffHPINttss5AtWLAgZHfccUfjF0a7ydZSdo/bb7/90v2HDh0asuy+md27xo0blx6zb9++Ics+B3V9//vfT/NsSupb3/rWNT4P/F3W5LrWWmuFLJve2xZbbbVVyLLP75IlSxo6TzPwZBkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKDA2zAS2Xjohx56KGR1O0Wrqqpuu+22kE2bNi1kjz76aMiysdiHHnpoep6s0zvrSM+6v8eOHRuye+65Jz3PLrvsErIXX3wx3ZbuIXuzwL777huyrNO6qvI3oMyYMSNkK1asCFn2BoMNN9wwPc+b3vSmkGVr+C1veUvIekNXdrPL7pGlN1dkf/cnn3wyZJ/5zGdCttNOO6XHzO7v2VjfV155JWTZ24FuuOGG9DyHH354yJ577rmQbb755iErfY/Qu1xxxRVpfvzxx4fs0ksvDdnHPvaxhs5/5513hmzbbbcNWW9Yr54sAwBAgWIZAAAKFMsAAFCgWAYAgIKWzhxR3NLS0mXzkLPRujfddFO6bdaY0cg41M6U/T2z5qqsEbDu8aqqqu67776QZc0vHbG+Wltbu+yP0ZVruC2yv2+2rr/61a+GbNSoUekxs/HSN954Y8iyZqts329/+9vpeY455piQZY2EEydODFmzNJp05Rquqq5dx4MHDw5Z1mBXVVX1/PPPh+zrX/96yLqysXPChAlp/uc//zlk2ffIz3/+85Bl99LuyL24/Wy22WYhe/rpp9Nts+/VTTfdNGR/+ctfGrqm9dZbL2RnnHFGyE477bSGztOV6q5hT5YBAKBAsQwAAAWKZQAAKFAsAwBAQdNP8OvTJ9b7W2+9dcgeeOCBkK299todck3dzaJFi0K2bNmykGWNXf365Uska0bImspWrlxZ5xJZQ6VGzexv+f73vz9kG2ywQchKf7O//vWvIbvgggtClk0+y67zoIMOSs+TNUGdfPLJIWuWZj7+r6wZr5kbhGbNmpXmS5cuDVnW3JhNrMy+16z37q30EoABAwaEbPjw4SG75pprap/rRz/6UcgabebLzJ8/P2SlpsOezpNlAAAoUCwDAECBYhkAAAoUywAAUND0DX5ZA1r24/ms2aLUMJFNvJs5c2bIrrjiipBljW9VVVW77bZbyMaNGxey7NoXLlwYsquuuio9TzY9Lfv3vP3tbw/Z1772tZANHTo0PU9HNBPwz/Xv3z9k+++/f7rteeedF7Jdd901ZFlTygsvvJAec++99w5Z1jyayT6na621Vq19q6qq7r333trbQmcqTSnNmqOyBr/seyhriNXg172VJp9+4hOfCFk2kTS7R86dOzc9ZnZ/7whZo2k2jbU38GQZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgoOnfhrF8+fKQPfrooyHbeOONQ7bVVlulx8y6jqdPnx6y1157LWSlzuiulHW0HnXUUSHL3nyRvUmjqvK3bhht3X6yt1S85z3vCdmXv/zldP9sxGq2NrMR1jvssEN6zOyzVtepp54astJ42GzNNXLutsiuKfv8lD4X9GzZ+jjssMPSbUeOHFnrmM8991zIuuP3CP/cbbfdluZjx44N2e677x6yK6+8MmQTJkxIj5m9nasjbL/99iHbb7/9Qnbdddd1xuV0KU+WAQCgQLEMAAAFimUAAChQLAMAQEHTN/jVlTUI/f73v0+3zZo4mrnhImsweOtb3xqyrJFpxYoV6TGfeOKJxi+Mog033DBkn/70p0OWNfKVzJo1K2RbbrllyBptpss+PxdccEHt/bNxqh3xmczGh999990hy8bQTpo0KT1mM98n+Neypr2Pfexj6baDBg0KWdY8fu2114ZMA2n3ln1X7rHHHum23/rWt0KWvRwg8/zzz6d5trY6wtvf/vaQZffNnlYzZTxZBgCAAsUyAAAUKJYBAKBAsQwAAAW9psGvLXraD9M/8pGPhCz7kX72777//vvTY86ZM6fxC6Oqqrw5IpvclE1YbItswuKyZcsaOma2jubOnRuyrEmuZPjw4SG7+uqrQ/a5z30uZOPGjQvZCSeckJ5n8uTJIRs2bFjIss9F1jRbVflERHqOAw88MGR77bVXum32uf7b3/4WsmuuuSZkPe07qKc54ogjQla6xz3zzDNrfJ7SMcePHx+yZ599NmR119Guu+6a5lntMG/evJANGTIkZIsWLap17mbhyTIAABQolgEAoECxDAAABYplAAAo0ODXw2RNJVkjU/bD/+yH+yeddFJ6HhOm2k/2t3jsscdCdtZZZ4XsoosuSo+ZNd5lUwGzSVK/+tWv0mPuvvvuIWvLBMG6+vbtG7KsSa/UuNcZrrrqqjQ/8sgjQ6ZZqzllzVXZNLZSE1Y2re/b3/52yLKmP7q3bJLj0qVL021vu+22WsfMvruze3ZVVdWmm24assWLF4cs+54+9NBDQ3b55Zen58nuxdOnTw/Z6NGjQ7ZkyZKQZZ+JZuHJMgAAFCiWAQCgQLEMAAAFimUAACjQ4NfDnHLKKSHbZJNNQpb98P/iiy8O2cyZM9vnwmiT5cuXh+yyyy4LWdZoUlVVdf7554csayAZOHBgyN70pjfVucQ2WbFiRchKjVHZdXaErNnk5ptvDtlnP/vZkP35z39Oj6mZrzkNGjQoZC+//HKt7Up/8x/84AchO+ecc0KWTdake/vOd74Tst/97nfpttl3aNaAve6664bstNNOS4+ZTSrNGvy23377kK2//vohmz9/fnqexx9/PGQnn3xyyLLJpc3czJfxZBkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKCgpTO7t1taWrSKt5OtttoqzZ988smQ9ekT/5/oueeeC9nOO+8csgULFqzB1XWs1tbWznldQqK7reHSmyPOPPPMkGVvdWj0zRNPPfVUyL70pS+F7C1veUvIsrGrVZW/oSOTvTFkzpw5ITvuuOPS/R944IGQdVYHd1eu4arqfuu4M2VvtMhGTmej3LPvy09+8pPpebLR1tlbA7J13Czci/8hWy9VlY+sHjVqVMgOOuigkGVvsqqq/A0qy5YtC1n23T916tSQ/eUvf0nP8+yzz9bKmvktQHXXsCfLAABQoFgGAIACxTIAABQolgEAoKBXN/iVmpu624/VDz/88JDdfvvt6bbZvyn74f+WW24ZshdffHENrq7zaSpZM1kDStZoMnfu3HT/bGR1Zr311gvZb3/725CttdZa6f7Z2NiPfexjIfv1r38dsuwau+PYVQ1+HS9rbqqqqpo+fXrIshHA2fdAdi++55570vNsvfXWIRs6dGjIHnvssZDV/ax1NffifzjkkEPS/Atf+ELIsu/pVatWhSy7P1dVfi+fPXt2yP70pz+FLBvDnjUhVlX+ufj4xz8esu5WM7WFBj8AAGiQYhkAAAoUywAAUKBYBgCAgn5dfQGdZaeddgrZpZdemm77+OOPh+ycc84J2WuvvRaytvzQfciQISHLmpZK0/oy2flPO+20kDVLMx/tJ5sUljWFNOqmm24K2dixY0OWTdurqqqaPHlyyGbMmNH4hdGrnHrqqWmeNaBmTaCbb755yF544YWQlSZO/vCHP6x17uw6L7vssvSYdF/Dhg1L83nz5oUs+54ePHhwyLJpk1WVNwM+/PDDIfvWt74Vsmz63//8z/+k55kwYULIrrrqqpBlk1x7Gk+WAQCgQLEMAAAFimUAAChQLAMAQEGvmeA3ZcqUkJ111lm19585c2bIrr/++pCVmj2OPPLIkE2cODFkpamCdT3yyCMh22effUKWNQg0C1Ojuo+sSbVu4+sHP/jB9JhXXnll4xfWzZng17769Yu96lljVVXla/Yd73hHyG688cZa5x49enSa//Wvfw1ZNlXwwQcfDNmkSZNqnburuRf3DNlEwKqqqkWLFoVs1qxZIStNAGwGJvgBAECDFMsAAFCgWAYAgALFMgAAFPTICX5Zk9xjjz0WsmxqU1VVVf/+/UOW/YD9E5/4REPX1Ii77rorzY866qiQNXMzH93bhRdeGLJsrWcTI7NJULAmjj/++JANHTo03XbFihUhK91P69h4443TvG7z/DbbbBOy4cOHh2zBggVtuzCoKZvuWlVVdfbZZ4fsox/9aEdfTrfkyTIAABQolgEAoECxDAAABYplAAAoUCwDAEBBj3wbRmbp0qUhW7hwYbrtqFGjah2zvd9wUVX5myv+93//N2RZl2pVlbtaoSOcdNJJIcs+Fx/5yEdC5i0trIn1118/ZJdccknISvfnV199tdZ5sv379u0bskMPPTTdP3sbRpaNHDkyZNkI7N133z09j3s+HeXLX/5yyLL12ht4sgwAAAWKZQAAKFAsAwBAgWIZAAAKek2D3zPPPBOyL33pS+m2H/7wh0OWNf316VP//zVWrlwZsmeffTZk733ve0P2yCOPhExzFJ1t4MCBIRsyZEitfX/1q1+19+XQC4wZMyZkjz76aMjqrsOqqqo//elPIevfv3/IBg8eHLKsuWnChAnpeWbMmBGybNR2Ntp6gw02CNkBBxyQnufuu+8Ome8H2kP2YoRPfepTXXAlXc+TZQAAKFAsAwBAgWIZAAAKFMsAAFDQIxv8sslLc+fODdmNN96Y7j9o0KCQHXPMMSEbN25cyG655Zb0mKeffnqta1q9enW6P3S1rCG2bpNr1igCfzd06NA0v+eee0I2fvz4hs41adKkkGVNfy+//HLIsga9sWPHpufJmlqz75xs/+w77IUXXkjPk00V1OBHR8leVtAbeLIMAAAFimUAAChQLAMAQIFiGQAAClpaW1s772QtLZ13Mnqs1tbW2P3SSXrzGj7xxBNDdvXVV9faN5v+t3z58oavqVl15Rququ63jrP1UVVVtcsuu4Ts/PPPD9k+++wTsgEDBtQ+f7YWP//5z4fs3HPPDVlvbqZzL6bZ1V3DniwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUeBsGTUcHdveRjXe/4YYbQva9732vMy6naXgbRsfLRkaXdOb3YE/iXkyz8zYMAABokGIZAAAKFMsAAFCgWAYAgAINfjQdTSU0Ow1+9ATuxTQ7DX4AANAgxTIAABQolgEAoECxDAAABZ3a4AcAAM3Ek2UAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoKBfZ56spaWltTPPR8/U2tra0lXntoZpD125hqvKOqZ9uBfT7OquYU+WAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAgn5dfQFAz9fS0hKy1tbWLrgSAGgbT5YBAKBAsQwAAAWKZQAAKFAsAwBAgQa/LrTllluG7He/+13Ihg4dGrJVq1Z1yDVBW4wcOTJkM2bMCNngwYND9tnPfjY95tlnn934hUGD+vbtG7Lf/OY3Idt4443T/bfYYouQzZo1q/ELo0fq1y+WY2PGjEm3nTRpUsiOPvrokO24444hGz58eMgGDRqUnqd///4hW7ZsWcimTZsWsq985Sshu/XWW9PzLF26NM27E0+WAQCgQLEMAAAFimUAAChQLAMAQIEGvy505plnhmzAgAEhy35kr8GPzpZN4csanoYMGVLreFmTSlVVVZ8+8f/hV69eXeuY0F522223kG233XYhy9ZrVVXVbbfdFrI3vvGNjV8YTS9rqDv88MNDdsopp6T7Z417w4YNC1nWNJjdxxu1zjrrhGzPPfcM2V133ZXuf9RRR4Vs+fLljV9YO/JkGQAAChTLAABQoFgGAIACxTIAABRo8OtCdZuWOuIH+dBW2STJsWPHrvHxxo0bl+bZ5DQNfnSk7B47f/78ho656aab1jpPa2trQ+ehe8v+5tm9b//99w/ZhAkT0mNmE1E7q5mvruzcBx98cLrtiSeeGLKvf/3rIevK7wFPlgEAoECxDAAABYplAAAoUCwDAECBBr8utNZaa4Us+1F81vAEHaU0keyQQw4JWdZoklm5cmXInn766XRbDU90tuweu+GGGzZ0zL/97W8h0+DX+2R/85EjR4Zs2bJlDR2zGV4EUPpu+eIXvxiyN7zhDSF73/veF7LO+vx4sgwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFDgbRhdaOLEibW2GzZsWMgWLlzYzlcD/58RI0ak+Qc/+MGQ1X1Ty4IFC0L2ox/9KN3WaGu6g2xcdWbVqlVp/rWvfS1k1nbvk7316owzzgjZQQcdVGvfqqqq/v37N35hNaxYsSJkc+bMCdn06dNDltUtpfHd2b/nXe96V8g+//nPh+y5555Lj9nePFkGAIACxTIAABQolgEAoECxDAAABRr8ulDpx+6vN3r06JDNmjWrvS+HXigbkVpqbMrGj2ay8aOzZ88O2UsvvVTreNDRsmakD33oQyHLPi+ldXzttdc2fmE0jUGDBqX57bffHrI999wzZHWbpRu1cuXKkP3iF79It73oootC9sQTT4Rs8eLFIcs+U+9+97vT83zkIx8J2dChQ0N22WWXhezggw9Oj9nePFkGAIACxTIAABQolgEAoECxDAAABRr8utCAAQNqbTdu3LiQPfnkk+19OfRCWcPSlltumW6bNWxksga/V155JWR9+uT/r55dE3Skww47LGTZ5yBb25dcckl6zFdffbXxC6Nbyr67b7vttnTbvffeO2SN3uOydbho0aKQPfzwwyE74YQTQtboCwP69YulZNZIePPNN6f7v+997wtZ9n1Tt8m8I3iyDAAABYplAAAoUCwDAECBYhkAAAo0+HWhwYMH19pu1apVHXwl9FZZk90ee+xRe9u6soaUzTbbLN02m2TlM0B7ydbx5MmTQ5atw4ULF4bsu9/9bnqebM3TfLJmvA9+8IMhO+igg2rvX9fy5cvT/L777qt1Tc8991zIOmJdrlixImRZw+GLL76Y7l/3moYMGdK2C2tHniwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUeBtGF+rfv3+t7ebPn9/BV0JvNXDgwJAdcMAB6bZ1u7qz7dZbb72QHXjggen+3/rWt0JW6gqHtspG8+6777619n3wwQdDNmfOnEYviW5s5513DtkFF1wQskZHWK9evTpkl19+ebrteeedF7K5c+c2dP7OUHoD2IgRI2rtn312O4snywAAUKBYBgCAAsUyAAAUKJYBAKBAg18nyUas1m0IaIYf7tP9Zettxx13DNlGG23U7ufZYIMNQjZs2LB0/0022SRkv/71rxu6Jvi7ddZZJ2TDhw8PWTaCN2usykb90pyy7+mbb745ZIMGDWroPNnaeuWVV0J29tlnp/u/9tprDZ2/q2y++eZpXvdlB6tWrWrPy2kTT5YBAKBAsQwAAAWKZQAAKFAsAwBAgQa/TpJNnqnb4NesP+ane8nW22mnnRayRptXMtkxV65cmW676aabhkyDH+3l6KOPDlnd+3OjU9ro3rLm4vHjxzd0zOw+99JLL4XslltuCdmCBQsaOndXypol3/SmN9XeNvP88883dE2N8GQZAAAKFMsAAFCgWAYAgALFMgAAFGjw6yR9+/attV022Wfp0qXtfTlQVVVVrbfeep1ynqyBozS1afbs2R19OfQCgwcPTvOsqbVu496pp54askcffbRtF0a3dcopp4Ss7nf38uXL0/yBBx4I2Te+8Y2QPfTQQyHL6oFmkU1oPfjggxs65kknndTQ/o3wZBkAAAoUywAAUKBYBgCAAsUyAAAUaPDrJKVpZa+3evXqkK1YsaK9L4deKGsWmTNnTsgWL16c7j9kyJCQNTLlrNQ488orr9TaH/6ZbBJkVeWNR3VtvfXWa7wv3d+xxx5ba7vsXvqBD3wg3famm24KWTaZr5mb+bIJmNtvv33Itttuu9rHzGqhxx9/vG0X1o48WQYAgALFMgAAFCiWAQCgQLEMAAAFimUAACjwNoxOUvcNAQsXLgzZqlWr2vty6IWybusf/OAHISu9DWObbbaplQ0YMKDW9ZTehlE6P7TF5ptvnubZ6PW6XnzxxTXel+5v3XXXrbVd9saea6+9Nt227puwmln23+3EE08M2fDhw9P9s++mX//61yHrylrIk2UAAChQLAMAQIFiGQAAChTLAABQoMGvk2SjGzPz5s3r4CuBf5g6dWrI3vCGN6Tbjh8/PmSNNEuVdMQx6dmyptJTTjkl3Xb06NFrfJ5GRmXT/dVtxM++p+t+xze77LN2wgknhOytb31ryEpN3UuXLg3ZUUcd1faL60C+lQAAoECxDAAABYplAAAoUCwDAECBBr9O0r9//1rbLVu2rIOvBP5hnXXWCdkhhxySbjthwoSQ9eu35reQbGpTVdVvsoG/23PPPUO2zz77pNs2sr4GDhy4xvvS/WUT4rJ73AYbbBCy7F5aVVX117/+tda5604+rar8OrNJgaV77OuV7uN77LFHyN773veG7Oijjw7ZoEGDQlZqgswmyc6aNSvdtqt4sgwAAAWKZQAAKFAsAwBAgWIZAAAKNPh1M8uXL+/qS6AXGTlyZMgmTpyYbjt48OB2PfeKFSvS/JVXXmnX89CzZA16X/jCF0JWt6m6JGuOuuGGGxo6Jt3b9OnTQ7bxxhuHLGteu//++9NjPvPMMyEbMWJEyDbbbLOQle6FTz/9dMhuuummkC1evDhkO+20U8iOP/749DzZvz2bsJp9JrP7++9///v0PGeccUbIuttERE+WAQCgQLEMAAAFimUAAChQLAMAQEFL3Qkv7XKylpbOO1k307dv35BlE3eyaT/rrrtuh1xTs2ptbe2yEW89bQ1nzR6//OUv023bMmGqjpdeeinNN9poo5B1t2aPRnXlGq6q5l7H6623XsiytZQ1IpVkzUh/+tOfQjZp0qSQzZs3r/Z5epquXMd9+vQJa7gt9UzWlJY1mp122mkhGz58eMhK0x2z7/6eJqtlHnjggZCdeuqp6f5Tp04NWWfd8+uuYU+WAQCgQLEMAAAFimUAAChQLAMAQIFiGQAACoy77iR1u3RHjRrVwVcC/zBmzJiQteUtAo24/vrr07ynvfmC9pW9xSBbM6V1nL354uGHHw7ZZZddFrKFCxfWuUSa1K233hqy7C1A73rXu0KWvcWnJ8o+PzfeeGPIPv7xj4cse9tXVTXHPd+TZQAAKFAsAwBAgWIZAAAKFMsAAFBg3HUXWrVqVciy5pUtttgiZNOmTeuQa2oGxl23n6222ipkv/rVr9Jthw0btsbnyRo4jjjiiHTbO++8c43P0yyMu25f48ePD1k2uriqqqpfv9jXfsstt4TsD3/4Q8hmz54dsmZoTuooPW3cdTaaOmvw23DDDUO21157pec5+OCDQ7bnnnuGbOzYsbXOXVX5tWf/9mxtLl++PGRLly5NzzNz5syQ/fCHPwzZxRdfHLK5c+fWup6uZtw1AAA0SLEMAAAFimUAAChQLAMAQIEGvy6U/WipnzAAAAFWSURBVEh/3LhxIdNU8n9p8Gs//fv3D1nWaFJVVXXeeeeFbPfddw9ZNjnt7LPPDtntt9+enidrfO1pNPjRE7gXd6zSFMosz+qJTN3tqipvGszuz81cj2jwAwCABimWAQCgQLEMAAAFimUAACjQ4EfT0VRCs9PgR0/gXkyz0+AHAAANUiwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgoKW1tbWrrwEAALolT5YBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAgv8Hqn+vwXNIpSUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train for 2000 epochs and save at 200 intervals\n",
    "# print G images\n",
    "# save images\n",
    "mnist_dcgan.train(train_steps=2000, batch_size=256, save_interval=200)\n",
    "mnist_dcgan.plot_images(fake=True)\n",
    "mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.694200, acc: 0.486328]  [A loss: 1.032304, acc: 0.000000]\n",
      "1: [D loss: 0.665861, acc: 0.525391]  [A loss: 0.966823, acc: 0.000000]\n",
      "2: [D loss: 0.602871, acc: 0.978516]  [A loss: 1.148097, acc: 0.000000]\n",
      "3: [D loss: 0.519768, acc: 0.671875]  [A loss: 1.491733, acc: 0.000000]\n",
      "4: [D loss: 0.562950, acc: 0.970703]  [A loss: 0.746627, acc: 0.292969]\n",
      "5: [D loss: 0.952768, acc: 0.500000]  [A loss: 1.060897, acc: 0.000000]\n",
      "6: [D loss: 0.464746, acc: 0.996094]  [A loss: 1.009413, acc: 0.000000]\n",
      "7: [D loss: 0.417337, acc: 0.966797]  [A loss: 1.033287, acc: 0.000000]\n",
      "8: [D loss: 0.464833, acc: 0.552734]  [A loss: 1.274700, acc: 0.000000]\n",
      "9: [D loss: 0.420379, acc: 0.738281]  [A loss: 1.252419, acc: 0.000000]\n",
      "10: [D loss: 0.428633, acc: 0.623047]  [A loss: 1.365008, acc: 0.000000]\n",
      "11: [D loss: 0.388039, acc: 0.857422]  [A loss: 1.254597, acc: 0.000000]\n",
      "12: [D loss: 0.423267, acc: 0.605469]  [A loss: 1.648377, acc: 0.000000]\n",
      "13: [D loss: 0.338808, acc: 1.000000]  [A loss: 1.133146, acc: 0.000000]\n",
      "14: [D loss: 0.544424, acc: 0.503906]  [A loss: 1.975783, acc: 0.000000]\n",
      "15: [D loss: 0.400627, acc: 0.972656]  [A loss: 1.139391, acc: 0.000000]\n",
      "16: [D loss: 0.376420, acc: 0.736328]  [A loss: 1.447931, acc: 0.000000]\n",
      "17: [D loss: 0.288176, acc: 0.996094]  [A loss: 1.462482, acc: 0.000000]\n",
      "18: [D loss: 0.316851, acc: 0.916016]  [A loss: 1.741265, acc: 0.000000]\n",
      "19: [D loss: 0.254322, acc: 1.000000]  [A loss: 1.534269, acc: 0.000000]\n",
      "20: [D loss: 0.299180, acc: 0.914062]  [A loss: 2.077768, acc: 0.000000]\n",
      "21: [D loss: 0.233214, acc: 0.998047]  [A loss: 1.417787, acc: 0.000000]\n",
      "22: [D loss: 0.420057, acc: 0.650391]  [A loss: 2.475970, acc: 0.000000]\n",
      "23: [D loss: 0.299429, acc: 0.960938]  [A loss: 1.351000, acc: 0.003906]\n",
      "24: [D loss: 0.343128, acc: 0.798828]  [A loss: 1.939791, acc: 0.000000]\n",
      "25: [D loss: 0.189052, acc: 1.000000]  [A loss: 1.601070, acc: 0.000000]\n",
      "26: [D loss: 0.266040, acc: 0.919922]  [A loss: 2.093161, acc: 0.000000]\n",
      "27: [D loss: 0.183062, acc: 0.998047]  [A loss: 1.776334, acc: 0.000000]\n",
      "28: [D loss: 0.246367, acc: 0.937500]  [A loss: 2.276956, acc: 0.000000]\n",
      "29: [D loss: 0.172771, acc: 0.990234]  [A loss: 1.687536, acc: 0.000000]\n",
      "30: [D loss: 0.313863, acc: 0.832031]  [A loss: 2.730697, acc: 0.000000]\n",
      "31: [D loss: 0.239719, acc: 0.962891]  [A loss: 1.415520, acc: 0.003906]\n",
      "32: [D loss: 0.502578, acc: 0.638672]  [A loss: 2.589464, acc: 0.000000]\n",
      "33: [D loss: 0.230785, acc: 0.955078]  [A loss: 1.531991, acc: 0.000000]\n",
      "34: [D loss: 0.295999, acc: 0.863281]  [A loss: 2.066101, acc: 0.000000]\n",
      "35: [D loss: 0.142184, acc: 0.998047]  [A loss: 1.758636, acc: 0.003906]\n",
      "36: [D loss: 0.238474, acc: 0.937500]  [A loss: 2.193515, acc: 0.000000]\n",
      "37: [D loss: 0.152828, acc: 0.990234]  [A loss: 1.873016, acc: 0.000000]\n",
      "38: [D loss: 0.254393, acc: 0.906250]  [A loss: 2.537743, acc: 0.000000]\n",
      "39: [D loss: 0.151217, acc: 0.986328]  [A loss: 1.704497, acc: 0.003906]\n",
      "40: [D loss: 0.371059, acc: 0.783203]  [A loss: 2.955094, acc: 0.000000]\n",
      "41: [D loss: 0.236596, acc: 0.921875]  [A loss: 1.461635, acc: 0.027344]\n",
      "42: [D loss: 0.425703, acc: 0.738281]  [A loss: 2.546874, acc: 0.000000]\n",
      "43: [D loss: 0.151387, acc: 0.980469]  [A loss: 1.705090, acc: 0.000000]\n",
      "44: [D loss: 0.272152, acc: 0.875000]  [A loss: 2.342799, acc: 0.000000]\n",
      "45: [D loss: 0.133303, acc: 0.998047]  [A loss: 1.995947, acc: 0.000000]\n",
      "46: [D loss: 0.210627, acc: 0.951172]  [A loss: 2.382203, acc: 0.000000]\n",
      "47: [D loss: 0.133779, acc: 0.992188]  [A loss: 2.160319, acc: 0.000000]\n",
      "48: [D loss: 0.205858, acc: 0.953125]  [A loss: 2.620759, acc: 0.000000]\n",
      "49: [D loss: 0.126532, acc: 0.998047]  [A loss: 2.171293, acc: 0.000000]\n",
      "50: [D loss: 0.218835, acc: 0.941406]  [A loss: 2.855862, acc: 0.000000]\n",
      "51: [D loss: 0.126347, acc: 0.992188]  [A loss: 2.052037, acc: 0.000000]\n",
      "52: [D loss: 0.270372, acc: 0.882812]  [A loss: 3.253034, acc: 0.000000]\n",
      "53: [D loss: 0.130047, acc: 0.968750]  [A loss: 1.917676, acc: 0.003906]\n",
      "54: [D loss: 0.305572, acc: 0.830078]  [A loss: 3.122481, acc: 0.000000]\n",
      "55: [D loss: 0.110551, acc: 0.988281]  [A loss: 1.954443, acc: 0.000000]\n",
      "56: [D loss: 0.241806, acc: 0.904297]  [A loss: 2.747159, acc: 0.000000]\n",
      "57: [D loss: 0.107032, acc: 0.996094]  [A loss: 1.887031, acc: 0.003906]\n",
      "58: [D loss: 0.294131, acc: 0.839844]  [A loss: 2.390698, acc: 0.000000]\n",
      "59: [D loss: 0.181792, acc: 0.978516]  [A loss: 0.919154, acc: 0.261719]\n",
      "60: [D loss: 0.474008, acc: 0.685547]  [A loss: 2.687168, acc: 0.000000]\n",
      "61: [D loss: 0.380281, acc: 0.865234]  [A loss: 0.381651, acc: 0.925781]\n",
      "62: [D loss: 0.718120, acc: 0.529297]  [A loss: 1.827329, acc: 0.000000]\n",
      "63: [D loss: 0.301740, acc: 0.929688]  [A loss: 0.749722, acc: 0.429688]\n",
      "64: [D loss: 0.542753, acc: 0.582031]  [A loss: 1.859212, acc: 0.000000]\n",
      "65: [D loss: 0.275349, acc: 0.960938]  [A loss: 1.029692, acc: 0.085938]\n",
      "66: [D loss: 0.474861, acc: 0.615234]  [A loss: 2.000913, acc: 0.000000]\n",
      "67: [D loss: 0.327500, acc: 0.939453]  [A loss: 0.949923, acc: 0.160156]\n",
      "68: [D loss: 0.564257, acc: 0.556641]  [A loss: 2.196131, acc: 0.000000]\n",
      "69: [D loss: 0.339974, acc: 0.933594]  [A loss: 0.831438, acc: 0.242188]\n",
      "70: [D loss: 0.657355, acc: 0.513672]  [A loss: 2.163247, acc: 0.000000]\n",
      "71: [D loss: 0.340042, acc: 0.927734]  [A loss: 0.892265, acc: 0.156250]\n",
      "72: [D loss: 0.611199, acc: 0.513672]  [A loss: 2.020648, acc: 0.000000]\n",
      "73: [D loss: 0.362645, acc: 0.931641]  [A loss: 0.873003, acc: 0.171875]\n",
      "74: [D loss: 0.623347, acc: 0.507812]  [A loss: 2.071486, acc: 0.000000]\n",
      "75: [D loss: 0.351868, acc: 0.945312]  [A loss: 1.028897, acc: 0.054688]\n",
      "76: [D loss: 0.580654, acc: 0.537109]  [A loss: 2.058793, acc: 0.000000]\n",
      "77: [D loss: 0.387645, acc: 0.925781]  [A loss: 0.888195, acc: 0.207031]\n",
      "78: [D loss: 0.727143, acc: 0.501953]  [A loss: 2.236962, acc: 0.000000]\n",
      "79: [D loss: 0.433806, acc: 0.886719]  [A loss: 0.780314, acc: 0.371094]\n",
      "80: [D loss: 0.792556, acc: 0.501953]  [A loss: 1.949603, acc: 0.000000]\n",
      "81: [D loss: 0.412138, acc: 0.943359]  [A loss: 1.001259, acc: 0.082031]\n",
      "82: [D loss: 0.613512, acc: 0.523438]  [A loss: 1.879825, acc: 0.000000]\n",
      "83: [D loss: 0.411750, acc: 0.896484]  [A loss: 1.088616, acc: 0.046875]\n",
      "84: [D loss: 0.603903, acc: 0.539062]  [A loss: 2.060644, acc: 0.000000]\n",
      "85: [D loss: 0.436034, acc: 0.917969]  [A loss: 0.851506, acc: 0.253906]\n",
      "86: [D loss: 0.719095, acc: 0.501953]  [A loss: 2.120864, acc: 0.000000]\n",
      "87: [D loss: 0.457829, acc: 0.886719]  [A loss: 0.776467, acc: 0.355469]\n",
      "88: [D loss: 0.756271, acc: 0.500000]  [A loss: 1.967323, acc: 0.000000]\n",
      "89: [D loss: 0.441850, acc: 0.906250]  [A loss: 0.905933, acc: 0.167969]\n",
      "90: [D loss: 0.659564, acc: 0.505859]  [A loss: 1.865736, acc: 0.000000]\n",
      "91: [D loss: 0.448732, acc: 0.888672]  [A loss: 0.995647, acc: 0.101562]\n",
      "92: [D loss: 0.627393, acc: 0.525391]  [A loss: 1.983092, acc: 0.000000]\n",
      "93: [D loss: 0.434544, acc: 0.902344]  [A loss: 0.950851, acc: 0.132812]\n",
      "94: [D loss: 0.638342, acc: 0.523438]  [A loss: 1.992355, acc: 0.000000]\n",
      "95: [D loss: 0.436001, acc: 0.904297]  [A loss: 0.868460, acc: 0.203125]\n",
      "96: [D loss: 0.655656, acc: 0.517578]  [A loss: 2.003112, acc: 0.000000]\n",
      "97: [D loss: 0.441887, acc: 0.896484]  [A loss: 0.913273, acc: 0.203125]\n",
      "98: [D loss: 0.637169, acc: 0.527344]  [A loss: 1.991253, acc: 0.000000]\n",
      "99: [D loss: 0.453463, acc: 0.884766]  [A loss: 0.886276, acc: 0.195312]\n",
      "100: [D loss: 0.667630, acc: 0.519531]  [A loss: 2.031467, acc: 0.000000]\n",
      "101: [D loss: 0.423246, acc: 0.906250]  [A loss: 0.896221, acc: 0.167969]\n",
      "102: [D loss: 0.633482, acc: 0.527344]  [A loss: 1.975850, acc: 0.000000]\n",
      "103: [D loss: 0.441733, acc: 0.890625]  [A loss: 0.957543, acc: 0.148438]\n",
      "104: [D loss: 0.628698, acc: 0.523438]  [A loss: 1.975219, acc: 0.000000]\n",
      "105: [D loss: 0.437767, acc: 0.906250]  [A loss: 0.967258, acc: 0.164062]\n",
      "106: [D loss: 0.611813, acc: 0.533203]  [A loss: 1.963507, acc: 0.000000]\n",
      "107: [D loss: 0.446264, acc: 0.906250]  [A loss: 1.009740, acc: 0.113281]\n",
      "108: [D loss: 0.606660, acc: 0.541016]  [A loss: 2.028900, acc: 0.000000]\n",
      "109: [D loss: 0.452238, acc: 0.882812]  [A loss: 0.923479, acc: 0.175781]\n",
      "110: [D loss: 0.672738, acc: 0.513672]  [A loss: 2.175581, acc: 0.000000]\n",
      "111: [D loss: 0.490955, acc: 0.859375]  [A loss: 0.773901, acc: 0.390625]\n",
      "112: [D loss: 0.739487, acc: 0.501953]  [A loss: 1.978223, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113: [D loss: 0.494028, acc: 0.875000]  [A loss: 0.788664, acc: 0.343750]\n",
      "114: [D loss: 0.710518, acc: 0.500000]  [A loss: 1.889697, acc: 0.000000]\n",
      "115: [D loss: 0.469649, acc: 0.884766]  [A loss: 1.025986, acc: 0.132812]\n",
      "116: [D loss: 0.619871, acc: 0.544922]  [A loss: 1.839863, acc: 0.000000]\n",
      "117: [D loss: 0.499653, acc: 0.833984]  [A loss: 1.008168, acc: 0.074219]\n",
      "118: [D loss: 0.653431, acc: 0.521484]  [A loss: 2.117540, acc: 0.000000]\n",
      "119: [D loss: 0.494996, acc: 0.861328]  [A loss: 0.832081, acc: 0.304688]\n",
      "120: [D loss: 0.737476, acc: 0.505859]  [A loss: 2.150315, acc: 0.000000]\n",
      "121: [D loss: 0.545545, acc: 0.796875]  [A loss: 0.706036, acc: 0.496094]\n",
      "122: [D loss: 0.813466, acc: 0.500000]  [A loss: 1.974588, acc: 0.000000]\n",
      "123: [D loss: 0.532366, acc: 0.798828]  [A loss: 0.820588, acc: 0.304688]\n",
      "124: [D loss: 0.693681, acc: 0.509766]  [A loss: 1.898536, acc: 0.000000]\n",
      "125: [D loss: 0.528870, acc: 0.816406]  [A loss: 0.891347, acc: 0.183594]\n",
      "126: [D loss: 0.705535, acc: 0.517578]  [A loss: 1.985905, acc: 0.000000]\n",
      "127: [D loss: 0.520491, acc: 0.818359]  [A loss: 0.787056, acc: 0.363281]\n",
      "128: [D loss: 0.717767, acc: 0.503906]  [A loss: 2.029816, acc: 0.000000]\n",
      "129: [D loss: 0.516247, acc: 0.841797]  [A loss: 0.773421, acc: 0.375000]\n",
      "130: [D loss: 0.714428, acc: 0.503906]  [A loss: 1.986768, acc: 0.000000]\n",
      "131: [D loss: 0.536861, acc: 0.806641]  [A loss: 0.734044, acc: 0.457031]\n",
      "132: [D loss: 0.692703, acc: 0.517578]  [A loss: 1.848793, acc: 0.000000]\n",
      "133: [D loss: 0.522398, acc: 0.828125]  [A loss: 0.839057, acc: 0.281250]\n",
      "134: [D loss: 0.658661, acc: 0.527344]  [A loss: 1.916455, acc: 0.000000]\n",
      "135: [D loss: 0.508340, acc: 0.837891]  [A loss: 0.854030, acc: 0.242188]\n",
      "136: [D loss: 0.677669, acc: 0.507812]  [A loss: 1.989318, acc: 0.000000]\n",
      "137: [D loss: 0.522181, acc: 0.812500]  [A loss: 0.708987, acc: 0.500000]\n",
      "138: [D loss: 0.704334, acc: 0.517578]  [A loss: 1.924044, acc: 0.000000]\n",
      "139: [D loss: 0.511954, acc: 0.818359]  [A loss: 0.726815, acc: 0.480469]\n",
      "140: [D loss: 0.703648, acc: 0.519531]  [A loss: 1.868569, acc: 0.000000]\n",
      "141: [D loss: 0.514251, acc: 0.830078]  [A loss: 0.784729, acc: 0.378906]\n",
      "142: [D loss: 0.680049, acc: 0.529297]  [A loss: 1.812627, acc: 0.000000]\n",
      "143: [D loss: 0.510341, acc: 0.830078]  [A loss: 0.842607, acc: 0.277344]\n",
      "144: [D loss: 0.655530, acc: 0.535156]  [A loss: 1.900623, acc: 0.000000]\n",
      "145: [D loss: 0.479859, acc: 0.863281]  [A loss: 0.861196, acc: 0.253906]\n",
      "146: [D loss: 0.651162, acc: 0.525391]  [A loss: 2.110258, acc: 0.000000]\n",
      "147: [D loss: 0.504360, acc: 0.816406]  [A loss: 0.644511, acc: 0.613281]\n",
      "148: [D loss: 0.750767, acc: 0.509766]  [A loss: 2.024263, acc: 0.000000]\n",
      "149: [D loss: 0.513544, acc: 0.794922]  [A loss: 0.705790, acc: 0.511719]\n",
      "150: [D loss: 0.711182, acc: 0.501953]  [A loss: 1.894592, acc: 0.000000]\n",
      "151: [D loss: 0.505621, acc: 0.843750]  [A loss: 0.839399, acc: 0.273438]\n",
      "152: [D loss: 0.638232, acc: 0.537109]  [A loss: 1.881321, acc: 0.000000]\n",
      "153: [D loss: 0.485328, acc: 0.873047]  [A loss: 0.890102, acc: 0.238281]\n",
      "154: [D loss: 0.644981, acc: 0.517578]  [A loss: 2.077676, acc: 0.000000]\n",
      "155: [D loss: 0.518510, acc: 0.820312]  [A loss: 0.673827, acc: 0.570312]\n",
      "156: [D loss: 0.725702, acc: 0.503906]  [A loss: 2.084033, acc: 0.000000]\n",
      "157: [D loss: 0.535847, acc: 0.785156]  [A loss: 0.675429, acc: 0.550781]\n",
      "158: [D loss: 0.750217, acc: 0.507812]  [A loss: 1.819678, acc: 0.000000]\n",
      "159: [D loss: 0.542388, acc: 0.787109]  [A loss: 0.819750, acc: 0.316406]\n",
      "160: [D loss: 0.680020, acc: 0.521484]  [A loss: 1.826311, acc: 0.000000]\n",
      "161: [D loss: 0.532773, acc: 0.820312]  [A loss: 0.829357, acc: 0.328125]\n",
      "162: [D loss: 0.672612, acc: 0.511719]  [A loss: 1.845764, acc: 0.000000]\n",
      "163: [D loss: 0.536586, acc: 0.812500]  [A loss: 0.735770, acc: 0.460938]\n",
      "164: [D loss: 0.734354, acc: 0.507812]  [A loss: 1.885261, acc: 0.000000]\n",
      "165: [D loss: 0.539538, acc: 0.787109]  [A loss: 0.687370, acc: 0.535156]\n",
      "166: [D loss: 0.724888, acc: 0.517578]  [A loss: 1.718231, acc: 0.000000]\n",
      "167: [D loss: 0.547453, acc: 0.798828]  [A loss: 0.793842, acc: 0.339844]\n",
      "168: [D loss: 0.678730, acc: 0.513672]  [A loss: 1.719345, acc: 0.000000]\n",
      "169: [D loss: 0.522249, acc: 0.835938]  [A loss: 0.871558, acc: 0.238281]\n",
      "170: [D loss: 0.650908, acc: 0.531250]  [A loss: 1.783896, acc: 0.000000]\n",
      "171: [D loss: 0.531982, acc: 0.818359]  [A loss: 0.759300, acc: 0.417969]\n",
      "172: [D loss: 0.708960, acc: 0.515625]  [A loss: 1.961748, acc: 0.000000]\n",
      "173: [D loss: 0.536414, acc: 0.796875]  [A loss: 0.644620, acc: 0.648438]\n",
      "174: [D loss: 0.744506, acc: 0.503906]  [A loss: 1.830130, acc: 0.000000]\n",
      "175: [D loss: 0.538749, acc: 0.806641]  [A loss: 0.762470, acc: 0.378906]\n",
      "176: [D loss: 0.681466, acc: 0.517578]  [A loss: 1.782777, acc: 0.000000]\n",
      "177: [D loss: 0.554088, acc: 0.789062]  [A loss: 0.764008, acc: 0.390625]\n",
      "178: [D loss: 0.683640, acc: 0.513672]  [A loss: 1.877416, acc: 0.000000]\n",
      "179: [D loss: 0.561582, acc: 0.732422]  [A loss: 0.666864, acc: 0.605469]\n",
      "180: [D loss: 0.714193, acc: 0.507812]  [A loss: 1.840234, acc: 0.000000]\n",
      "181: [D loss: 0.544105, acc: 0.791016]  [A loss: 0.755596, acc: 0.382812]\n",
      "182: [D loss: 0.699046, acc: 0.511719]  [A loss: 1.818060, acc: 0.000000]\n",
      "183: [D loss: 0.543810, acc: 0.796875]  [A loss: 0.723032, acc: 0.468750]\n",
      "184: [D loss: 0.713275, acc: 0.521484]  [A loss: 1.724722, acc: 0.000000]\n",
      "185: [D loss: 0.561918, acc: 0.771484]  [A loss: 0.776908, acc: 0.378906]\n",
      "186: [D loss: 0.702045, acc: 0.513672]  [A loss: 1.779893, acc: 0.000000]\n",
      "187: [D loss: 0.570401, acc: 0.767578]  [A loss: 0.757042, acc: 0.410156]\n",
      "188: [D loss: 0.692588, acc: 0.527344]  [A loss: 1.765719, acc: 0.000000]\n",
      "189: [D loss: 0.584545, acc: 0.740234]  [A loss: 0.694619, acc: 0.515625]\n",
      "190: [D loss: 0.737859, acc: 0.503906]  [A loss: 1.842769, acc: 0.000000]\n",
      "191: [D loss: 0.579600, acc: 0.714844]  [A loss: 0.611013, acc: 0.667969]\n",
      "192: [D loss: 0.752481, acc: 0.505859]  [A loss: 1.728309, acc: 0.000000]\n",
      "193: [D loss: 0.581759, acc: 0.750000]  [A loss: 0.739700, acc: 0.433594]\n",
      "194: [D loss: 0.717880, acc: 0.507812]  [A loss: 1.642740, acc: 0.000000]\n",
      "195: [D loss: 0.577090, acc: 0.740234]  [A loss: 0.805229, acc: 0.328125]\n",
      "196: [D loss: 0.691707, acc: 0.519531]  [A loss: 1.657057, acc: 0.000000]\n",
      "197: [D loss: 0.578833, acc: 0.750000]  [A loss: 0.705113, acc: 0.539062]\n",
      "198: [D loss: 0.710810, acc: 0.511719]  [A loss: 1.792031, acc: 0.000000]\n",
      "199: [D loss: 0.586233, acc: 0.742188]  [A loss: 0.698194, acc: 0.480469]\n",
      "200: [D loss: 0.716203, acc: 0.513672]  [A loss: 1.751596, acc: 0.000000]\n",
      "201: [D loss: 0.590755, acc: 0.722656]  [A loss: 0.694525, acc: 0.519531]\n",
      "202: [D loss: 0.712862, acc: 0.521484]  [A loss: 1.694116, acc: 0.000000]\n",
      "203: [D loss: 0.590469, acc: 0.716797]  [A loss: 0.748534, acc: 0.406250]\n",
      "204: [D loss: 0.707778, acc: 0.523438]  [A loss: 1.722394, acc: 0.000000]\n",
      "205: [D loss: 0.579753, acc: 0.751953]  [A loss: 0.730989, acc: 0.441406]\n",
      "206: [D loss: 0.685848, acc: 0.539062]  [A loss: 1.564754, acc: 0.000000]\n",
      "207: [D loss: 0.596128, acc: 0.730469]  [A loss: 0.752200, acc: 0.402344]\n",
      "208: [D loss: 0.709186, acc: 0.525391]  [A loss: 1.760988, acc: 0.000000]\n",
      "209: [D loss: 0.585604, acc: 0.705078]  [A loss: 0.672445, acc: 0.570312]\n",
      "210: [D loss: 0.748399, acc: 0.517578]  [A loss: 1.748948, acc: 0.000000]\n",
      "211: [D loss: 0.619544, acc: 0.660156]  [A loss: 0.634331, acc: 0.625000]\n",
      "212: [D loss: 0.772100, acc: 0.507812]  [A loss: 1.606086, acc: 0.000000]\n",
      "213: [D loss: 0.609471, acc: 0.687500]  [A loss: 0.768459, acc: 0.351562]\n",
      "214: [D loss: 0.691802, acc: 0.531250]  [A loss: 1.363920, acc: 0.000000]\n",
      "215: [D loss: 0.579474, acc: 0.751953]  [A loss: 0.904209, acc: 0.191406]\n",
      "216: [D loss: 0.648503, acc: 0.574219]  [A loss: 1.358292, acc: 0.007812]\n",
      "217: [D loss: 0.590195, acc: 0.693359]  [A loss: 0.982457, acc: 0.113281]\n",
      "218: [D loss: 0.646121, acc: 0.558594]  [A loss: 1.648772, acc: 0.000000]\n",
      "219: [D loss: 0.578784, acc: 0.742188]  [A loss: 0.858199, acc: 0.257812]\n",
      "220: [D loss: 0.678689, acc: 0.527344]  [A loss: 1.905561, acc: 0.000000]\n",
      "221: [D loss: 0.611004, acc: 0.654297]  [A loss: 0.537098, acc: 0.796875]\n",
      "222: [D loss: 0.804582, acc: 0.507812]  [A loss: 1.916705, acc: 0.000000]\n",
      "223: [D loss: 0.629200, acc: 0.625000]  [A loss: 0.624416, acc: 0.656250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224: [D loss: 0.751613, acc: 0.500000]  [A loss: 1.439034, acc: 0.003906]\n",
      "225: [D loss: 0.585584, acc: 0.734375]  [A loss: 0.868643, acc: 0.238281]\n",
      "226: [D loss: 0.663381, acc: 0.564453]  [A loss: 1.463061, acc: 0.003906]\n",
      "227: [D loss: 0.600621, acc: 0.708984]  [A loss: 0.841085, acc: 0.277344]\n",
      "228: [D loss: 0.662377, acc: 0.544922]  [A loss: 1.490902, acc: 0.003906]\n",
      "229: [D loss: 0.596896, acc: 0.736328]  [A loss: 0.863168, acc: 0.246094]\n",
      "230: [D loss: 0.677751, acc: 0.539062]  [A loss: 1.645102, acc: 0.000000]\n",
      "231: [D loss: 0.607533, acc: 0.703125]  [A loss: 0.715926, acc: 0.460938]\n",
      "232: [D loss: 0.737065, acc: 0.513672]  [A loss: 1.692635, acc: 0.000000]\n",
      "233: [D loss: 0.608111, acc: 0.685547]  [A loss: 0.694119, acc: 0.542969]\n",
      "234: [D loss: 0.740162, acc: 0.507812]  [A loss: 1.598205, acc: 0.000000]\n",
      "235: [D loss: 0.620001, acc: 0.669922]  [A loss: 0.729496, acc: 0.433594]\n",
      "236: [D loss: 0.696854, acc: 0.527344]  [A loss: 1.419046, acc: 0.011719]\n",
      "237: [D loss: 0.604087, acc: 0.697266]  [A loss: 0.931942, acc: 0.164062]\n",
      "238: [D loss: 0.679398, acc: 0.548828]  [A loss: 1.412288, acc: 0.003906]\n",
      "239: [D loss: 0.612751, acc: 0.712891]  [A loss: 0.833266, acc: 0.250000]\n",
      "240: [D loss: 0.686415, acc: 0.529297]  [A loss: 1.546639, acc: 0.000000]\n",
      "241: [D loss: 0.621300, acc: 0.675781]  [A loss: 0.741742, acc: 0.445312]\n",
      "242: [D loss: 0.723090, acc: 0.523438]  [A loss: 1.576210, acc: 0.000000]\n",
      "243: [D loss: 0.624317, acc: 0.640625]  [A loss: 0.671642, acc: 0.593750]\n",
      "244: [D loss: 0.749299, acc: 0.507812]  [A loss: 1.555199, acc: 0.003906]\n",
      "245: [D loss: 0.646394, acc: 0.611328]  [A loss: 0.672541, acc: 0.613281]\n",
      "246: [D loss: 0.735873, acc: 0.505859]  [A loss: 1.468518, acc: 0.000000]\n",
      "247: [D loss: 0.645027, acc: 0.640625]  [A loss: 0.755817, acc: 0.375000]\n",
      "248: [D loss: 0.701833, acc: 0.523438]  [A loss: 1.301141, acc: 0.011719]\n",
      "249: [D loss: 0.636293, acc: 0.652344]  [A loss: 0.773199, acc: 0.371094]\n",
      "250: [D loss: 0.696375, acc: 0.523438]  [A loss: 1.340983, acc: 0.000000]\n",
      "251: [D loss: 0.631115, acc: 0.675781]  [A loss: 0.800429, acc: 0.304688]\n",
      "252: [D loss: 0.679296, acc: 0.544922]  [A loss: 1.341124, acc: 0.000000]\n",
      "253: [D loss: 0.638330, acc: 0.671875]  [A loss: 0.813102, acc: 0.277344]\n",
      "254: [D loss: 0.703827, acc: 0.515625]  [A loss: 1.348630, acc: 0.000000]\n",
      "255: [D loss: 0.650277, acc: 0.625000]  [A loss: 0.733931, acc: 0.437500]\n",
      "256: [D loss: 0.722379, acc: 0.515625]  [A loss: 1.493879, acc: 0.000000]\n",
      "257: [D loss: 0.645530, acc: 0.621094]  [A loss: 0.662912, acc: 0.574219]\n",
      "258: [D loss: 0.741153, acc: 0.501953]  [A loss: 1.422655, acc: 0.000000]\n",
      "259: [D loss: 0.651359, acc: 0.623047]  [A loss: 0.720736, acc: 0.441406]\n",
      "260: [D loss: 0.692560, acc: 0.521484]  [A loss: 1.301753, acc: 0.003906]\n",
      "261: [D loss: 0.637441, acc: 0.644531]  [A loss: 0.760244, acc: 0.371094]\n",
      "262: [D loss: 0.688606, acc: 0.546875]  [A loss: 1.316122, acc: 0.000000]\n",
      "263: [D loss: 0.644725, acc: 0.652344]  [A loss: 0.739233, acc: 0.425781]\n",
      "264: [D loss: 0.717031, acc: 0.513672]  [A loss: 1.390450, acc: 0.003906]\n",
      "265: [D loss: 0.637474, acc: 0.656250]  [A loss: 0.687067, acc: 0.558594]\n",
      "266: [D loss: 0.719015, acc: 0.529297]  [A loss: 1.396145, acc: 0.000000]\n",
      "267: [D loss: 0.652544, acc: 0.625000]  [A loss: 0.710076, acc: 0.472656]\n",
      "268: [D loss: 0.720868, acc: 0.523438]  [A loss: 1.424706, acc: 0.000000]\n",
      "269: [D loss: 0.644746, acc: 0.615234]  [A loss: 0.702501, acc: 0.511719]\n",
      "270: [D loss: 0.695199, acc: 0.525391]  [A loss: 1.278136, acc: 0.000000]\n",
      "271: [D loss: 0.650824, acc: 0.660156]  [A loss: 0.773562, acc: 0.347656]\n",
      "272: [D loss: 0.706173, acc: 0.523438]  [A loss: 1.223168, acc: 0.011719]\n",
      "273: [D loss: 0.642826, acc: 0.652344]  [A loss: 0.844791, acc: 0.207031]\n",
      "274: [D loss: 0.684620, acc: 0.533203]  [A loss: 1.288697, acc: 0.007812]\n",
      "275: [D loss: 0.654798, acc: 0.636719]  [A loss: 0.781753, acc: 0.339844]\n",
      "276: [D loss: 0.692776, acc: 0.533203]  [A loss: 1.283377, acc: 0.000000]\n",
      "277: [D loss: 0.642150, acc: 0.646484]  [A loss: 0.729972, acc: 0.441406]\n",
      "278: [D loss: 0.685834, acc: 0.525391]  [A loss: 1.362383, acc: 0.000000]\n",
      "279: [D loss: 0.637772, acc: 0.667969]  [A loss: 0.730120, acc: 0.414062]\n",
      "280: [D loss: 0.723460, acc: 0.513672]  [A loss: 1.424419, acc: 0.000000]\n",
      "281: [D loss: 0.659021, acc: 0.601562]  [A loss: 0.634236, acc: 0.683594]\n",
      "282: [D loss: 0.750105, acc: 0.509766]  [A loss: 1.349552, acc: 0.000000]\n",
      "283: [D loss: 0.653921, acc: 0.623047]  [A loss: 0.698887, acc: 0.511719]\n",
      "284: [D loss: 0.710351, acc: 0.507812]  [A loss: 1.206507, acc: 0.007812]\n",
      "285: [D loss: 0.637137, acc: 0.673828]  [A loss: 0.718931, acc: 0.484375]\n",
      "286: [D loss: 0.691655, acc: 0.542969]  [A loss: 1.216292, acc: 0.000000]\n",
      "287: [D loss: 0.653128, acc: 0.630859]  [A loss: 0.764920, acc: 0.335938]\n",
      "288: [D loss: 0.697022, acc: 0.519531]  [A loss: 1.212897, acc: 0.007812]\n",
      "289: [D loss: 0.648833, acc: 0.652344]  [A loss: 0.786323, acc: 0.292969]\n",
      "290: [D loss: 0.689832, acc: 0.529297]  [A loss: 1.242967, acc: 0.003906]\n",
      "291: [D loss: 0.655537, acc: 0.623047]  [A loss: 0.716693, acc: 0.488281]\n",
      "292: [D loss: 0.716228, acc: 0.515625]  [A loss: 1.260501, acc: 0.003906]\n",
      "293: [D loss: 0.648704, acc: 0.628906]  [A loss: 0.741167, acc: 0.421875]\n",
      "294: [D loss: 0.682697, acc: 0.525391]  [A loss: 1.287676, acc: 0.000000]\n",
      "295: [D loss: 0.639567, acc: 0.666016]  [A loss: 0.715412, acc: 0.500000]\n",
      "296: [D loss: 0.699528, acc: 0.531250]  [A loss: 1.366547, acc: 0.000000]\n",
      "297: [D loss: 0.657679, acc: 0.574219]  [A loss: 0.647725, acc: 0.632812]\n",
      "298: [D loss: 0.736661, acc: 0.507812]  [A loss: 1.354676, acc: 0.000000]\n",
      "299: [D loss: 0.664295, acc: 0.587891]  [A loss: 0.619710, acc: 0.718750]\n",
      "300: [D loss: 0.725314, acc: 0.500000]  [A loss: 1.210440, acc: 0.000000]\n",
      "301: [D loss: 0.665655, acc: 0.609375]  [A loss: 0.727149, acc: 0.433594]\n",
      "302: [D loss: 0.697469, acc: 0.519531]  [A loss: 1.122825, acc: 0.007812]\n",
      "303: [D loss: 0.656398, acc: 0.626953]  [A loss: 0.803553, acc: 0.242188]\n",
      "304: [D loss: 0.688623, acc: 0.527344]  [A loss: 1.129077, acc: 0.007812]\n",
      "305: [D loss: 0.652887, acc: 0.613281]  [A loss: 0.799835, acc: 0.250000]\n",
      "306: [D loss: 0.681641, acc: 0.513672]  [A loss: 1.227162, acc: 0.000000]\n",
      "307: [D loss: 0.649101, acc: 0.650391]  [A loss: 0.729887, acc: 0.410156]\n",
      "308: [D loss: 0.713883, acc: 0.507812]  [A loss: 1.337189, acc: 0.000000]\n",
      "309: [D loss: 0.656333, acc: 0.636719]  [A loss: 0.704625, acc: 0.515625]\n",
      "310: [D loss: 0.725855, acc: 0.513672]  [A loss: 1.311831, acc: 0.000000]\n",
      "311: [D loss: 0.662514, acc: 0.599609]  [A loss: 0.674886, acc: 0.582031]\n",
      "312: [D loss: 0.711889, acc: 0.511719]  [A loss: 1.189137, acc: 0.007812]\n",
      "313: [D loss: 0.661658, acc: 0.621094]  [A loss: 0.706512, acc: 0.460938]\n",
      "314: [D loss: 0.709593, acc: 0.513672]  [A loss: 1.218232, acc: 0.000000]\n",
      "315: [D loss: 0.668472, acc: 0.611328]  [A loss: 0.736301, acc: 0.414062]\n",
      "316: [D loss: 0.706190, acc: 0.515625]  [A loss: 1.084284, acc: 0.011719]\n",
      "317: [D loss: 0.667999, acc: 0.576172]  [A loss: 0.799887, acc: 0.273438]\n",
      "318: [D loss: 0.688354, acc: 0.535156]  [A loss: 1.126764, acc: 0.011719]\n",
      "319: [D loss: 0.652548, acc: 0.650391]  [A loss: 0.747210, acc: 0.347656]\n",
      "320: [D loss: 0.701255, acc: 0.529297]  [A loss: 1.178705, acc: 0.000000]\n",
      "321: [D loss: 0.643283, acc: 0.664062]  [A loss: 0.754017, acc: 0.398438]\n",
      "322: [D loss: 0.713718, acc: 0.521484]  [A loss: 1.256048, acc: 0.000000]\n",
      "323: [D loss: 0.664774, acc: 0.613281]  [A loss: 0.648028, acc: 0.683594]\n",
      "324: [D loss: 0.711234, acc: 0.511719]  [A loss: 1.197756, acc: 0.003906]\n",
      "325: [D loss: 0.650822, acc: 0.644531]  [A loss: 0.713734, acc: 0.503906]\n",
      "326: [D loss: 0.686539, acc: 0.537109]  [A loss: 1.189837, acc: 0.003906]\n",
      "327: [D loss: 0.660955, acc: 0.625000]  [A loss: 0.719222, acc: 0.441406]\n",
      "328: [D loss: 0.688868, acc: 0.529297]  [A loss: 1.163090, acc: 0.000000]\n",
      "329: [D loss: 0.651221, acc: 0.617188]  [A loss: 0.704842, acc: 0.496094]\n",
      "330: [D loss: 0.699062, acc: 0.509766]  [A loss: 1.205566, acc: 0.000000]\n",
      "331: [D loss: 0.655848, acc: 0.591797]  [A loss: 0.719879, acc: 0.441406]\n",
      "332: [D loss: 0.706135, acc: 0.509766]  [A loss: 1.202493, acc: 0.000000]\n",
      "333: [D loss: 0.653943, acc: 0.603516]  [A loss: 0.691723, acc: 0.511719]\n",
      "334: [D loss: 0.683216, acc: 0.542969]  [A loss: 1.169023, acc: 0.003906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335: [D loss: 0.651980, acc: 0.646484]  [A loss: 0.739527, acc: 0.406250]\n",
      "336: [D loss: 0.701151, acc: 0.529297]  [A loss: 1.154433, acc: 0.007812]\n",
      "337: [D loss: 0.672107, acc: 0.580078]  [A loss: 0.794970, acc: 0.265625]\n",
      "338: [D loss: 0.693705, acc: 0.527344]  [A loss: 1.079484, acc: 0.007812]\n",
      "339: [D loss: 0.657940, acc: 0.630859]  [A loss: 0.775278, acc: 0.304688]\n",
      "340: [D loss: 0.678773, acc: 0.535156]  [A loss: 1.129700, acc: 0.000000]\n",
      "341: [D loss: 0.655561, acc: 0.636719]  [A loss: 0.802178, acc: 0.273438]\n",
      "342: [D loss: 0.674229, acc: 0.550781]  [A loss: 1.124812, acc: 0.000000]\n",
      "343: [D loss: 0.644975, acc: 0.628906]  [A loss: 0.795511, acc: 0.296875]\n",
      "344: [D loss: 0.693886, acc: 0.525391]  [A loss: 1.180147, acc: 0.003906]\n",
      "345: [D loss: 0.645244, acc: 0.658203]  [A loss: 0.691247, acc: 0.531250]\n",
      "346: [D loss: 0.695883, acc: 0.535156]  [A loss: 1.277721, acc: 0.003906]\n",
      "347: [D loss: 0.665456, acc: 0.613281]  [A loss: 0.640741, acc: 0.664062]\n",
      "348: [D loss: 0.719706, acc: 0.517578]  [A loss: 1.291985, acc: 0.003906]\n",
      "349: [D loss: 0.665116, acc: 0.580078]  [A loss: 0.667145, acc: 0.613281]\n",
      "350: [D loss: 0.701072, acc: 0.511719]  [A loss: 1.118646, acc: 0.007812]\n",
      "351: [D loss: 0.652624, acc: 0.628906]  [A loss: 0.747467, acc: 0.417969]\n",
      "352: [D loss: 0.684757, acc: 0.533203]  [A loss: 1.045925, acc: 0.015625]\n",
      "353: [D loss: 0.654392, acc: 0.636719]  [A loss: 0.812334, acc: 0.273438]\n",
      "354: [D loss: 0.677493, acc: 0.537109]  [A loss: 1.009698, acc: 0.027344]\n",
      "355: [D loss: 0.648483, acc: 0.644531]  [A loss: 0.810507, acc: 0.257812]\n",
      "356: [D loss: 0.668043, acc: 0.562500]  [A loss: 1.132116, acc: 0.015625]\n",
      "357: [D loss: 0.639282, acc: 0.664062]  [A loss: 0.778314, acc: 0.296875]\n",
      "358: [D loss: 0.686276, acc: 0.523438]  [A loss: 1.245953, acc: 0.003906]\n",
      "359: [D loss: 0.670202, acc: 0.591797]  [A loss: 0.669464, acc: 0.582031]\n",
      "360: [D loss: 0.719603, acc: 0.513672]  [A loss: 1.270537, acc: 0.000000]\n",
      "361: [D loss: 0.671792, acc: 0.564453]  [A loss: 0.623665, acc: 0.722656]\n",
      "362: [D loss: 0.732194, acc: 0.503906]  [A loss: 1.130826, acc: 0.000000]\n",
      "363: [D loss: 0.659993, acc: 0.625000]  [A loss: 0.701777, acc: 0.484375]\n",
      "364: [D loss: 0.697710, acc: 0.513672]  [A loss: 1.008103, acc: 0.023438]\n",
      "365: [D loss: 0.647902, acc: 0.648438]  [A loss: 0.800495, acc: 0.265625]\n",
      "366: [D loss: 0.674379, acc: 0.548828]  [A loss: 1.026083, acc: 0.031250]\n",
      "367: [D loss: 0.662471, acc: 0.603516]  [A loss: 0.772042, acc: 0.335938]\n",
      "368: [D loss: 0.678404, acc: 0.539062]  [A loss: 1.142418, acc: 0.007812]\n",
      "369: [D loss: 0.660022, acc: 0.615234]  [A loss: 0.740231, acc: 0.402344]\n",
      "370: [D loss: 0.688146, acc: 0.527344]  [A loss: 1.164773, acc: 0.000000]\n",
      "371: [D loss: 0.667979, acc: 0.597656]  [A loss: 0.810712, acc: 0.277344]\n",
      "372: [D loss: 0.703409, acc: 0.501953]  [A loss: 1.050627, acc: 0.019531]\n",
      "373: [D loss: 0.660195, acc: 0.574219]  [A loss: 0.788484, acc: 0.308594]\n",
      "374: [D loss: 0.673068, acc: 0.574219]  [A loss: 1.160347, acc: 0.003906]\n",
      "375: [D loss: 0.654297, acc: 0.617188]  [A loss: 0.724005, acc: 0.460938]\n",
      "376: [D loss: 0.701835, acc: 0.511719]  [A loss: 1.235322, acc: 0.000000]\n",
      "377: [D loss: 0.654557, acc: 0.613281]  [A loss: 0.646402, acc: 0.648438]\n",
      "378: [D loss: 0.706435, acc: 0.521484]  [A loss: 1.178761, acc: 0.000000]\n",
      "379: [D loss: 0.659085, acc: 0.640625]  [A loss: 0.707024, acc: 0.519531]\n",
      "380: [D loss: 0.704265, acc: 0.523438]  [A loss: 1.071389, acc: 0.023438]\n",
      "381: [D loss: 0.662993, acc: 0.574219]  [A loss: 0.807483, acc: 0.257812]\n",
      "382: [D loss: 0.674375, acc: 0.566406]  [A loss: 0.992601, acc: 0.027344]\n",
      "383: [D loss: 0.670525, acc: 0.583984]  [A loss: 0.856622, acc: 0.164062]\n",
      "384: [D loss: 0.674014, acc: 0.546875]  [A loss: 0.975133, acc: 0.058594]\n",
      "385: [D loss: 0.653768, acc: 0.611328]  [A loss: 0.909937, acc: 0.132812]\n",
      "386: [D loss: 0.692015, acc: 0.548828]  [A loss: 1.045259, acc: 0.015625]\n",
      "387: [D loss: 0.657479, acc: 0.626953]  [A loss: 0.858460, acc: 0.203125]\n",
      "388: [D loss: 0.685971, acc: 0.566406]  [A loss: 1.095592, acc: 0.019531]\n",
      "389: [D loss: 0.669940, acc: 0.603516]  [A loss: 0.884782, acc: 0.171875]\n",
      "390: [D loss: 0.670586, acc: 0.560547]  [A loss: 1.132072, acc: 0.007812]\n",
      "391: [D loss: 0.670676, acc: 0.560547]  [A loss: 0.765513, acc: 0.328125]\n",
      "392: [D loss: 0.683698, acc: 0.548828]  [A loss: 1.355531, acc: 0.000000]\n",
      "393: [D loss: 0.673284, acc: 0.562500]  [A loss: 0.567369, acc: 0.808594]\n",
      "394: [D loss: 0.764578, acc: 0.498047]  [A loss: 1.304654, acc: 0.000000]\n",
      "395: [D loss: 0.675747, acc: 0.564453]  [A loss: 0.633393, acc: 0.664062]\n",
      "396: [D loss: 0.720854, acc: 0.505859]  [A loss: 1.083109, acc: 0.011719]\n",
      "397: [D loss: 0.664900, acc: 0.621094]  [A loss: 0.755827, acc: 0.386719]\n",
      "398: [D loss: 0.684751, acc: 0.564453]  [A loss: 0.966322, acc: 0.082031]\n",
      "399: [D loss: 0.671266, acc: 0.583984]  [A loss: 0.807800, acc: 0.210938]\n",
      "400: [D loss: 0.675758, acc: 0.541016]  [A loss: 0.928209, acc: 0.089844]\n",
      "401: [D loss: 0.663145, acc: 0.593750]  [A loss: 0.857061, acc: 0.175781]\n",
      "402: [D loss: 0.694479, acc: 0.535156]  [A loss: 1.009022, acc: 0.054688]\n",
      "403: [D loss: 0.669892, acc: 0.593750]  [A loss: 0.831302, acc: 0.175781]\n",
      "404: [D loss: 0.682540, acc: 0.550781]  [A loss: 1.064647, acc: 0.035156]\n",
      "405: [D loss: 0.669782, acc: 0.582031]  [A loss: 0.856545, acc: 0.187500]\n",
      "406: [D loss: 0.682229, acc: 0.556641]  [A loss: 0.997371, acc: 0.027344]\n",
      "407: [D loss: 0.657226, acc: 0.615234]  [A loss: 0.887064, acc: 0.152344]\n",
      "408: [D loss: 0.671243, acc: 0.568359]  [A loss: 1.076764, acc: 0.015625]\n",
      "409: [D loss: 0.672675, acc: 0.582031]  [A loss: 0.819372, acc: 0.210938]\n",
      "410: [D loss: 0.692397, acc: 0.552734]  [A loss: 1.190143, acc: 0.003906]\n",
      "411: [D loss: 0.678819, acc: 0.556641]  [A loss: 0.694016, acc: 0.539062]\n",
      "412: [D loss: 0.726385, acc: 0.523438]  [A loss: 1.332288, acc: 0.000000]\n",
      "413: [D loss: 0.690202, acc: 0.535156]  [A loss: 0.574470, acc: 0.796875]\n",
      "414: [D loss: 0.752550, acc: 0.498047]  [A loss: 1.123507, acc: 0.003906]\n",
      "415: [D loss: 0.671440, acc: 0.558594]  [A loss: 0.672560, acc: 0.589844]\n",
      "416: [D loss: 0.693716, acc: 0.537109]  [A loss: 1.028633, acc: 0.011719]\n",
      "417: [D loss: 0.656070, acc: 0.636719]  [A loss: 0.748412, acc: 0.371094]\n",
      "418: [D loss: 0.693945, acc: 0.533203]  [A loss: 1.020082, acc: 0.019531]\n",
      "419: [D loss: 0.672094, acc: 0.595703]  [A loss: 0.761846, acc: 0.332031]\n",
      "420: [D loss: 0.688889, acc: 0.542969]  [A loss: 0.997893, acc: 0.042969]\n",
      "421: [D loss: 0.667266, acc: 0.587891]  [A loss: 0.808184, acc: 0.218750]\n",
      "422: [D loss: 0.672171, acc: 0.566406]  [A loss: 0.964349, acc: 0.066406]\n",
      "423: [D loss: 0.681362, acc: 0.558594]  [A loss: 0.832552, acc: 0.218750]\n",
      "424: [D loss: 0.680911, acc: 0.552734]  [A loss: 0.964509, acc: 0.054688]\n",
      "425: [D loss: 0.673299, acc: 0.580078]  [A loss: 0.917168, acc: 0.082031]\n",
      "426: [D loss: 0.665938, acc: 0.595703]  [A loss: 0.953588, acc: 0.070312]\n",
      "427: [D loss: 0.667007, acc: 0.621094]  [A loss: 0.881994, acc: 0.156250]\n",
      "428: [D loss: 0.693727, acc: 0.537109]  [A loss: 1.012424, acc: 0.023438]\n",
      "429: [D loss: 0.677748, acc: 0.599609]  [A loss: 0.839249, acc: 0.218750]\n",
      "430: [D loss: 0.673878, acc: 0.560547]  [A loss: 1.082335, acc: 0.019531]\n",
      "431: [D loss: 0.673741, acc: 0.578125]  [A loss: 0.727153, acc: 0.445312]\n",
      "432: [D loss: 0.703567, acc: 0.511719]  [A loss: 1.235936, acc: 0.015625]\n",
      "433: [D loss: 0.674253, acc: 0.580078]  [A loss: 0.587233, acc: 0.812500]\n",
      "434: [D loss: 0.735614, acc: 0.509766]  [A loss: 1.219853, acc: 0.000000]\n",
      "435: [D loss: 0.679596, acc: 0.548828]  [A loss: 0.671877, acc: 0.554688]\n",
      "436: [D loss: 0.711926, acc: 0.503906]  [A loss: 1.026193, acc: 0.011719]\n",
      "437: [D loss: 0.655355, acc: 0.650391]  [A loss: 0.737812, acc: 0.398438]\n",
      "438: [D loss: 0.687414, acc: 0.537109]  [A loss: 1.048089, acc: 0.007812]\n",
      "439: [D loss: 0.682821, acc: 0.562500]  [A loss: 0.760212, acc: 0.343750]\n",
      "440: [D loss: 0.689546, acc: 0.529297]  [A loss: 1.049063, acc: 0.015625]\n",
      "441: [D loss: 0.664362, acc: 0.587891]  [A loss: 0.737898, acc: 0.425781]\n",
      "442: [D loss: 0.696185, acc: 0.527344]  [A loss: 1.031905, acc: 0.011719]\n",
      "443: [D loss: 0.669716, acc: 0.591797]  [A loss: 0.746590, acc: 0.378906]\n",
      "444: [D loss: 0.691348, acc: 0.525391]  [A loss: 1.045828, acc: 0.019531]\n",
      "445: [D loss: 0.666350, acc: 0.595703]  [A loss: 0.740139, acc: 0.386719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446: [D loss: 0.695868, acc: 0.521484]  [A loss: 1.095983, acc: 0.023438]\n",
      "447: [D loss: 0.663285, acc: 0.605469]  [A loss: 0.772273, acc: 0.328125]\n",
      "448: [D loss: 0.694941, acc: 0.548828]  [A loss: 1.105670, acc: 0.015625]\n",
      "449: [D loss: 0.669872, acc: 0.583984]  [A loss: 0.744164, acc: 0.410156]\n",
      "450: [D loss: 0.697098, acc: 0.513672]  [A loss: 1.080040, acc: 0.031250]\n",
      "451: [D loss: 0.665019, acc: 0.613281]  [A loss: 0.755995, acc: 0.324219]\n",
      "452: [D loss: 0.697696, acc: 0.537109]  [A loss: 1.081200, acc: 0.011719]\n",
      "453: [D loss: 0.684594, acc: 0.539062]  [A loss: 0.770299, acc: 0.324219]\n",
      "454: [D loss: 0.691547, acc: 0.533203]  [A loss: 1.099787, acc: 0.011719]\n",
      "455: [D loss: 0.666062, acc: 0.580078]  [A loss: 0.704376, acc: 0.468750]\n",
      "456: [D loss: 0.707070, acc: 0.533203]  [A loss: 1.114911, acc: 0.000000]\n",
      "457: [D loss: 0.671142, acc: 0.574219]  [A loss: 0.713688, acc: 0.453125]\n",
      "458: [D loss: 0.719233, acc: 0.505859]  [A loss: 1.083932, acc: 0.046875]\n",
      "459: [D loss: 0.684225, acc: 0.527344]  [A loss: 0.802471, acc: 0.265625]\n",
      "460: [D loss: 0.674223, acc: 0.558594]  [A loss: 0.932508, acc: 0.062500]\n",
      "461: [D loss: 0.664077, acc: 0.601562]  [A loss: 0.827914, acc: 0.230469]\n",
      "462: [D loss: 0.688107, acc: 0.556641]  [A loss: 1.012968, acc: 0.019531]\n",
      "463: [D loss: 0.667476, acc: 0.613281]  [A loss: 0.754910, acc: 0.355469]\n",
      "464: [D loss: 0.690268, acc: 0.539062]  [A loss: 0.975733, acc: 0.027344]\n",
      "465: [D loss: 0.680021, acc: 0.572266]  [A loss: 0.797122, acc: 0.253906]\n",
      "466: [D loss: 0.683199, acc: 0.558594]  [A loss: 1.025972, acc: 0.035156]\n",
      "467: [D loss: 0.667003, acc: 0.640625]  [A loss: 0.811070, acc: 0.261719]\n",
      "468: [D loss: 0.676985, acc: 0.562500]  [A loss: 1.020388, acc: 0.039062]\n",
      "469: [D loss: 0.659878, acc: 0.628906]  [A loss: 0.812329, acc: 0.265625]\n",
      "470: [D loss: 0.678299, acc: 0.546875]  [A loss: 1.060871, acc: 0.011719]\n",
      "471: [D loss: 0.669735, acc: 0.605469]  [A loss: 0.740246, acc: 0.402344]\n",
      "472: [D loss: 0.688367, acc: 0.548828]  [A loss: 1.159872, acc: 0.015625]\n",
      "473: [D loss: 0.673904, acc: 0.574219]  [A loss: 0.641886, acc: 0.656250]\n",
      "474: [D loss: 0.723530, acc: 0.509766]  [A loss: 1.198732, acc: 0.000000]\n",
      "475: [D loss: 0.675978, acc: 0.556641]  [A loss: 0.675164, acc: 0.570312]\n",
      "476: [D loss: 0.724542, acc: 0.501953]  [A loss: 1.076855, acc: 0.050781]\n",
      "477: [D loss: 0.680403, acc: 0.552734]  [A loss: 0.773927, acc: 0.343750]\n",
      "478: [D loss: 0.686899, acc: 0.541016]  [A loss: 0.949696, acc: 0.054688]\n",
      "479: [D loss: 0.669712, acc: 0.605469]  [A loss: 0.794950, acc: 0.273438]\n",
      "480: [D loss: 0.675066, acc: 0.560547]  [A loss: 0.949362, acc: 0.074219]\n",
      "481: [D loss: 0.673566, acc: 0.597656]  [A loss: 0.806130, acc: 0.246094]\n",
      "482: [D loss: 0.681561, acc: 0.550781]  [A loss: 0.981199, acc: 0.050781]\n",
      "483: [D loss: 0.662863, acc: 0.599609]  [A loss: 0.843752, acc: 0.195312]\n",
      "484: [D loss: 0.661748, acc: 0.593750]  [A loss: 0.947039, acc: 0.074219]\n",
      "485: [D loss: 0.662040, acc: 0.611328]  [A loss: 0.887709, acc: 0.136719]\n",
      "486: [D loss: 0.670657, acc: 0.578125]  [A loss: 0.938347, acc: 0.101562]\n",
      "487: [D loss: 0.669037, acc: 0.578125]  [A loss: 0.922413, acc: 0.097656]\n",
      "488: [D loss: 0.667770, acc: 0.576172]  [A loss: 0.932950, acc: 0.058594]\n",
      "489: [D loss: 0.665417, acc: 0.593750]  [A loss: 0.942356, acc: 0.105469]\n",
      "490: [D loss: 0.663580, acc: 0.605469]  [A loss: 0.962629, acc: 0.074219]\n",
      "491: [D loss: 0.673322, acc: 0.558594]  [A loss: 0.915509, acc: 0.140625]\n",
      "492: [D loss: 0.686596, acc: 0.566406]  [A loss: 1.040142, acc: 0.035156]\n",
      "493: [D loss: 0.667755, acc: 0.589844]  [A loss: 0.802566, acc: 0.285156]\n",
      "494: [D loss: 0.682499, acc: 0.574219]  [A loss: 1.187320, acc: 0.000000]\n",
      "495: [D loss: 0.681770, acc: 0.550781]  [A loss: 0.687821, acc: 0.562500]\n",
      "496: [D loss: 0.723291, acc: 0.511719]  [A loss: 1.245755, acc: 0.000000]\n",
      "497: [D loss: 0.680572, acc: 0.568359]  [A loss: 0.605979, acc: 0.746094]\n",
      "498: [D loss: 0.730429, acc: 0.507812]  [A loss: 1.142034, acc: 0.007812]\n",
      "499: [D loss: 0.680095, acc: 0.562500]  [A loss: 0.695150, acc: 0.515625]\n",
      "500: [D loss: 0.705965, acc: 0.515625]  [A loss: 0.989261, acc: 0.062500]\n",
      "501: [D loss: 0.679169, acc: 0.548828]  [A loss: 0.821676, acc: 0.273438]\n",
      "502: [D loss: 0.683685, acc: 0.572266]  [A loss: 0.898418, acc: 0.113281]\n",
      "503: [D loss: 0.683833, acc: 0.576172]  [A loss: 0.882560, acc: 0.132812]\n",
      "504: [D loss: 0.669964, acc: 0.595703]  [A loss: 0.878805, acc: 0.136719]\n",
      "505: [D loss: 0.675180, acc: 0.591797]  [A loss: 0.874754, acc: 0.160156]\n",
      "506: [D loss: 0.683893, acc: 0.562500]  [A loss: 0.938745, acc: 0.085938]\n",
      "507: [D loss: 0.671251, acc: 0.593750]  [A loss: 0.817393, acc: 0.222656]\n",
      "508: [D loss: 0.688625, acc: 0.529297]  [A loss: 0.993751, acc: 0.039062]\n",
      "509: [D loss: 0.664675, acc: 0.607422]  [A loss: 0.814381, acc: 0.234375]\n",
      "510: [D loss: 0.678309, acc: 0.558594]  [A loss: 0.978923, acc: 0.054688]\n",
      "511: [D loss: 0.674287, acc: 0.576172]  [A loss: 0.815980, acc: 0.226562]\n",
      "512: [D loss: 0.674186, acc: 0.572266]  [A loss: 1.037236, acc: 0.015625]\n",
      "513: [D loss: 0.658254, acc: 0.625000]  [A loss: 0.772397, acc: 0.359375]\n",
      "514: [D loss: 0.684849, acc: 0.535156]  [A loss: 1.097335, acc: 0.023438]\n",
      "515: [D loss: 0.670575, acc: 0.580078]  [A loss: 0.724222, acc: 0.457031]\n",
      "516: [D loss: 0.684381, acc: 0.519531]  [A loss: 1.232656, acc: 0.007812]\n",
      "517: [D loss: 0.664374, acc: 0.591797]  [A loss: 0.679868, acc: 0.585938]\n",
      "518: [D loss: 0.698886, acc: 0.492188]  [A loss: 1.072392, acc: 0.023438]\n",
      "519: [D loss: 0.665648, acc: 0.585938]  [A loss: 0.708759, acc: 0.488281]\n",
      "520: [D loss: 0.698252, acc: 0.513672]  [A loss: 1.050882, acc: 0.050781]\n",
      "521: [D loss: 0.669000, acc: 0.578125]  [A loss: 0.744767, acc: 0.378906]\n",
      "522: [D loss: 0.684493, acc: 0.550781]  [A loss: 0.996066, acc: 0.027344]\n",
      "523: [D loss: 0.669266, acc: 0.601562]  [A loss: 0.775573, acc: 0.359375]\n",
      "524: [D loss: 0.695781, acc: 0.529297]  [A loss: 1.001178, acc: 0.046875]\n",
      "525: [D loss: 0.665565, acc: 0.593750]  [A loss: 0.755975, acc: 0.375000]\n",
      "526: [D loss: 0.681946, acc: 0.552734]  [A loss: 1.005924, acc: 0.027344]\n",
      "527: [D loss: 0.661881, acc: 0.597656]  [A loss: 0.783069, acc: 0.265625]\n",
      "528: [D loss: 0.675169, acc: 0.558594]  [A loss: 0.976350, acc: 0.093750]\n",
      "529: [D loss: 0.678583, acc: 0.564453]  [A loss: 0.870486, acc: 0.156250]\n",
      "530: [D loss: 0.666208, acc: 0.589844]  [A loss: 0.979283, acc: 0.066406]\n",
      "531: [D loss: 0.660094, acc: 0.619141]  [A loss: 0.858900, acc: 0.144531]\n",
      "532: [D loss: 0.681109, acc: 0.574219]  [A loss: 0.990635, acc: 0.023438]\n",
      "533: [D loss: 0.674174, acc: 0.568359]  [A loss: 0.831150, acc: 0.238281]\n",
      "534: [D loss: 0.682227, acc: 0.558594]  [A loss: 1.061942, acc: 0.042969]\n",
      "535: [D loss: 0.673651, acc: 0.558594]  [A loss: 0.714285, acc: 0.417969]\n",
      "536: [D loss: 0.703757, acc: 0.531250]  [A loss: 1.156385, acc: 0.015625]\n",
      "537: [D loss: 0.685037, acc: 0.533203]  [A loss: 0.704559, acc: 0.511719]\n",
      "538: [D loss: 0.713336, acc: 0.515625]  [A loss: 1.025357, acc: 0.011719]\n",
      "539: [D loss: 0.676544, acc: 0.572266]  [A loss: 0.706399, acc: 0.488281]\n",
      "540: [D loss: 0.688532, acc: 0.552734]  [A loss: 1.098015, acc: 0.007812]\n",
      "541: [D loss: 0.673432, acc: 0.585938]  [A loss: 0.734438, acc: 0.406250]\n",
      "542: [D loss: 0.698065, acc: 0.542969]  [A loss: 1.004809, acc: 0.054688]\n",
      "543: [D loss: 0.657077, acc: 0.605469]  [A loss: 0.790415, acc: 0.292969]\n",
      "544: [D loss: 0.677305, acc: 0.574219]  [A loss: 0.969685, acc: 0.042969]\n",
      "545: [D loss: 0.670325, acc: 0.587891]  [A loss: 0.787327, acc: 0.324219]\n",
      "546: [D loss: 0.682159, acc: 0.560547]  [A loss: 0.928392, acc: 0.054688]\n",
      "547: [D loss: 0.670289, acc: 0.582031]  [A loss: 0.821017, acc: 0.226562]\n",
      "548: [D loss: 0.683614, acc: 0.542969]  [A loss: 0.953432, acc: 0.070312]\n",
      "549: [D loss: 0.675921, acc: 0.583984]  [A loss: 0.850186, acc: 0.187500]\n",
      "550: [D loss: 0.676722, acc: 0.552734]  [A loss: 0.947233, acc: 0.062500]\n",
      "551: [D loss: 0.672334, acc: 0.541016]  [A loss: 0.878566, acc: 0.152344]\n",
      "552: [D loss: 0.678001, acc: 0.564453]  [A loss: 0.919999, acc: 0.093750]\n",
      "553: [D loss: 0.668705, acc: 0.601562]  [A loss: 0.957783, acc: 0.101562]\n",
      "554: [D loss: 0.691256, acc: 0.556641]  [A loss: 0.864852, acc: 0.191406]\n",
      "555: [D loss: 0.677904, acc: 0.574219]  [A loss: 0.923293, acc: 0.109375]\n",
      "556: [D loss: 0.674397, acc: 0.591797]  [A loss: 0.846332, acc: 0.199219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557: [D loss: 0.686281, acc: 0.554688]  [A loss: 0.948373, acc: 0.082031]\n",
      "558: [D loss: 0.683907, acc: 0.556641]  [A loss: 0.868445, acc: 0.156250]\n",
      "559: [D loss: 0.671923, acc: 0.583984]  [A loss: 0.908478, acc: 0.117188]\n",
      "560: [D loss: 0.673320, acc: 0.576172]  [A loss: 0.894130, acc: 0.136719]\n",
      "561: [D loss: 0.676965, acc: 0.546875]  [A loss: 0.924234, acc: 0.078125]\n",
      "562: [D loss: 0.664125, acc: 0.595703]  [A loss: 0.942456, acc: 0.078125]\n",
      "563: [D loss: 0.659469, acc: 0.625000]  [A loss: 0.943787, acc: 0.089844]\n",
      "564: [D loss: 0.673313, acc: 0.595703]  [A loss: 0.943082, acc: 0.117188]\n",
      "565: [D loss: 0.673950, acc: 0.585938]  [A loss: 0.917672, acc: 0.101562]\n",
      "566: [D loss: 0.677601, acc: 0.568359]  [A loss: 0.862113, acc: 0.179688]\n",
      "567: [D loss: 0.684559, acc: 0.574219]  [A loss: 1.002528, acc: 0.070312]\n",
      "568: [D loss: 0.677909, acc: 0.582031]  [A loss: 0.775821, acc: 0.324219]\n",
      "569: [D loss: 0.703443, acc: 0.507812]  [A loss: 1.218689, acc: 0.000000]\n",
      "570: [D loss: 0.680672, acc: 0.546875]  [A loss: 0.610139, acc: 0.714844]\n",
      "571: [D loss: 0.726887, acc: 0.501953]  [A loss: 1.190897, acc: 0.011719]\n",
      "572: [D loss: 0.685551, acc: 0.531250]  [A loss: 0.703533, acc: 0.476562]\n",
      "573: [D loss: 0.704711, acc: 0.523438]  [A loss: 0.943083, acc: 0.062500]\n",
      "574: [D loss: 0.674127, acc: 0.582031]  [A loss: 0.818023, acc: 0.195312]\n",
      "575: [D loss: 0.675141, acc: 0.572266]  [A loss: 0.916130, acc: 0.121094]\n",
      "576: [D loss: 0.666644, acc: 0.597656]  [A loss: 0.821548, acc: 0.214844]\n",
      "577: [D loss: 0.668009, acc: 0.568359]  [A loss: 0.907612, acc: 0.113281]\n",
      "578: [D loss: 0.667195, acc: 0.613281]  [A loss: 0.850742, acc: 0.218750]\n",
      "579: [D loss: 0.680521, acc: 0.558594]  [A loss: 0.999638, acc: 0.062500]\n",
      "580: [D loss: 0.658803, acc: 0.626953]  [A loss: 0.762208, acc: 0.394531]\n",
      "581: [D loss: 0.685819, acc: 0.572266]  [A loss: 1.020279, acc: 0.039062]\n",
      "582: [D loss: 0.671777, acc: 0.589844]  [A loss: 0.804475, acc: 0.289062]\n",
      "583: [D loss: 0.669283, acc: 0.576172]  [A loss: 0.970419, acc: 0.066406]\n",
      "584: [D loss: 0.665293, acc: 0.597656]  [A loss: 0.855090, acc: 0.175781]\n",
      "585: [D loss: 0.688704, acc: 0.541016]  [A loss: 1.013946, acc: 0.058594]\n",
      "586: [D loss: 0.667437, acc: 0.601562]  [A loss: 0.838846, acc: 0.179688]\n",
      "587: [D loss: 0.677378, acc: 0.552734]  [A loss: 1.013889, acc: 0.050781]\n",
      "588: [D loss: 0.665561, acc: 0.617188]  [A loss: 0.795321, acc: 0.292969]\n",
      "589: [D loss: 0.685793, acc: 0.550781]  [A loss: 1.116511, acc: 0.023438]\n",
      "590: [D loss: 0.670385, acc: 0.580078]  [A loss: 0.713518, acc: 0.453125]\n",
      "591: [D loss: 0.689916, acc: 0.533203]  [A loss: 1.129586, acc: 0.011719]\n",
      "592: [D loss: 0.670292, acc: 0.578125]  [A loss: 0.688431, acc: 0.562500]\n",
      "593: [D loss: 0.694255, acc: 0.523438]  [A loss: 1.032197, acc: 0.023438]\n",
      "594: [D loss: 0.660476, acc: 0.625000]  [A loss: 0.727345, acc: 0.414062]\n",
      "595: [D loss: 0.692890, acc: 0.535156]  [A loss: 1.028998, acc: 0.050781]\n",
      "596: [D loss: 0.668606, acc: 0.595703]  [A loss: 0.760617, acc: 0.347656]\n",
      "597: [D loss: 0.681156, acc: 0.541016]  [A loss: 0.999022, acc: 0.031250]\n",
      "598: [D loss: 0.665044, acc: 0.603516]  [A loss: 0.776178, acc: 0.312500]\n",
      "599: [D loss: 0.692081, acc: 0.554688]  [A loss: 1.039946, acc: 0.035156]\n",
      "600: [D loss: 0.658098, acc: 0.626953]  [A loss: 0.771871, acc: 0.335938]\n",
      "601: [D loss: 0.696315, acc: 0.523438]  [A loss: 1.032004, acc: 0.054688]\n",
      "602: [D loss: 0.667688, acc: 0.595703]  [A loss: 0.765229, acc: 0.343750]\n",
      "603: [D loss: 0.678265, acc: 0.548828]  [A loss: 1.002966, acc: 0.066406]\n",
      "604: [D loss: 0.675218, acc: 0.566406]  [A loss: 0.822161, acc: 0.214844]\n",
      "605: [D loss: 0.670596, acc: 0.580078]  [A loss: 0.983488, acc: 0.062500]\n",
      "606: [D loss: 0.672333, acc: 0.578125]  [A loss: 0.849500, acc: 0.226562]\n",
      "607: [D loss: 0.670123, acc: 0.603516]  [A loss: 0.977621, acc: 0.109375]\n",
      "608: [D loss: 0.654200, acc: 0.607422]  [A loss: 0.845754, acc: 0.207031]\n",
      "609: [D loss: 0.675751, acc: 0.556641]  [A loss: 0.970539, acc: 0.074219]\n",
      "610: [D loss: 0.674712, acc: 0.576172]  [A loss: 0.832712, acc: 0.210938]\n",
      "611: [D loss: 0.696485, acc: 0.539062]  [A loss: 1.072307, acc: 0.035156]\n",
      "612: [D loss: 0.674380, acc: 0.544922]  [A loss: 0.787200, acc: 0.300781]\n",
      "613: [D loss: 0.680397, acc: 0.548828]  [A loss: 1.028146, acc: 0.019531]\n",
      "614: [D loss: 0.666664, acc: 0.576172]  [A loss: 0.761317, acc: 0.371094]\n",
      "615: [D loss: 0.701219, acc: 0.529297]  [A loss: 1.071569, acc: 0.019531]\n",
      "616: [D loss: 0.672956, acc: 0.582031]  [A loss: 0.727013, acc: 0.453125]\n",
      "617: [D loss: 0.679262, acc: 0.556641]  [A loss: 1.017249, acc: 0.039062]\n",
      "618: [D loss: 0.670952, acc: 0.585938]  [A loss: 0.786703, acc: 0.300781]\n",
      "619: [D loss: 0.685094, acc: 0.558594]  [A loss: 1.016612, acc: 0.054688]\n",
      "620: [D loss: 0.668609, acc: 0.587891]  [A loss: 0.817698, acc: 0.246094]\n",
      "621: [D loss: 0.657934, acc: 0.625000]  [A loss: 0.904767, acc: 0.132812]\n",
      "622: [D loss: 0.666187, acc: 0.578125]  [A loss: 0.867891, acc: 0.167969]\n",
      "623: [D loss: 0.655113, acc: 0.603516]  [A loss: 0.958534, acc: 0.113281]\n",
      "624: [D loss: 0.666858, acc: 0.599609]  [A loss: 0.889993, acc: 0.175781]\n",
      "625: [D loss: 0.671025, acc: 0.595703]  [A loss: 0.982090, acc: 0.097656]\n",
      "626: [D loss: 0.668094, acc: 0.580078]  [A loss: 0.859824, acc: 0.175781]\n",
      "627: [D loss: 0.679104, acc: 0.562500]  [A loss: 1.047448, acc: 0.039062]\n",
      "628: [D loss: 0.670707, acc: 0.587891]  [A loss: 0.763706, acc: 0.339844]\n",
      "629: [D loss: 0.696125, acc: 0.523438]  [A loss: 1.111347, acc: 0.019531]\n",
      "630: [D loss: 0.666316, acc: 0.607422]  [A loss: 0.687512, acc: 0.496094]\n",
      "631: [D loss: 0.722786, acc: 0.503906]  [A loss: 1.163543, acc: 0.000000]\n",
      "632: [D loss: 0.670121, acc: 0.570312]  [A loss: 0.700785, acc: 0.523438]\n",
      "633: [D loss: 0.704506, acc: 0.546875]  [A loss: 1.011583, acc: 0.031250]\n",
      "634: [D loss: 0.660261, acc: 0.621094]  [A loss: 0.778403, acc: 0.339844]\n",
      "635: [D loss: 0.676702, acc: 0.554688]  [A loss: 0.965181, acc: 0.085938]\n",
      "636: [D loss: 0.664355, acc: 0.587891]  [A loss: 0.869620, acc: 0.171875]\n",
      "637: [D loss: 0.683068, acc: 0.560547]  [A loss: 1.010383, acc: 0.062500]\n",
      "638: [D loss: 0.659584, acc: 0.587891]  [A loss: 0.833084, acc: 0.238281]\n",
      "639: [D loss: 0.682893, acc: 0.554688]  [A loss: 0.994025, acc: 0.074219]\n",
      "640: [D loss: 0.657178, acc: 0.595703]  [A loss: 0.763811, acc: 0.332031]\n",
      "641: [D loss: 0.695973, acc: 0.544922]  [A loss: 0.985329, acc: 0.039062]\n",
      "642: [D loss: 0.658279, acc: 0.611328]  [A loss: 0.800497, acc: 0.296875]\n",
      "643: [D loss: 0.667187, acc: 0.587891]  [A loss: 1.009412, acc: 0.062500]\n",
      "644: [D loss: 0.661253, acc: 0.646484]  [A loss: 0.800480, acc: 0.316406]\n",
      "645: [D loss: 0.696850, acc: 0.523438]  [A loss: 1.064627, acc: 0.019531]\n",
      "646: [D loss: 0.667887, acc: 0.599609]  [A loss: 0.777703, acc: 0.343750]\n",
      "647: [D loss: 0.683701, acc: 0.568359]  [A loss: 1.064447, acc: 0.023438]\n",
      "648: [D loss: 0.664254, acc: 0.601562]  [A loss: 0.746466, acc: 0.359375]\n",
      "649: [D loss: 0.684941, acc: 0.544922]  [A loss: 0.971879, acc: 0.054688]\n",
      "650: [D loss: 0.664459, acc: 0.611328]  [A loss: 0.785657, acc: 0.304688]\n",
      "651: [D loss: 0.673056, acc: 0.582031]  [A loss: 0.957288, acc: 0.109375]\n",
      "652: [D loss: 0.666296, acc: 0.613281]  [A loss: 0.853285, acc: 0.207031]\n",
      "653: [D loss: 0.669577, acc: 0.591797]  [A loss: 0.971511, acc: 0.093750]\n",
      "654: [D loss: 0.657599, acc: 0.613281]  [A loss: 0.853477, acc: 0.195312]\n",
      "655: [D loss: 0.674657, acc: 0.578125]  [A loss: 1.026230, acc: 0.058594]\n",
      "656: [D loss: 0.654678, acc: 0.617188]  [A loss: 0.823144, acc: 0.253906]\n",
      "657: [D loss: 0.698821, acc: 0.537109]  [A loss: 1.127051, acc: 0.023438]\n",
      "658: [D loss: 0.664414, acc: 0.628906]  [A loss: 0.714103, acc: 0.453125]\n",
      "659: [D loss: 0.679238, acc: 0.546875]  [A loss: 1.058862, acc: 0.039062]\n",
      "660: [D loss: 0.668686, acc: 0.591797]  [A loss: 0.772513, acc: 0.324219]\n",
      "661: [D loss: 0.669724, acc: 0.574219]  [A loss: 1.017921, acc: 0.035156]\n",
      "662: [D loss: 0.663217, acc: 0.621094]  [A loss: 0.744629, acc: 0.402344]\n",
      "663: [D loss: 0.676238, acc: 0.568359]  [A loss: 0.987045, acc: 0.066406]\n",
      "664: [D loss: 0.650372, acc: 0.619141]  [A loss: 0.780124, acc: 0.351562]\n",
      "665: [D loss: 0.686718, acc: 0.546875]  [A loss: 1.094867, acc: 0.015625]\n",
      "666: [D loss: 0.666128, acc: 0.605469]  [A loss: 0.754795, acc: 0.375000]\n",
      "667: [D loss: 0.693603, acc: 0.533203]  [A loss: 1.074131, acc: 0.042969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668: [D loss: 0.657805, acc: 0.603516]  [A loss: 0.749349, acc: 0.371094]\n",
      "669: [D loss: 0.685451, acc: 0.556641]  [A loss: 0.966708, acc: 0.089844]\n",
      "670: [D loss: 0.666681, acc: 0.582031]  [A loss: 0.812413, acc: 0.316406]\n",
      "671: [D loss: 0.675448, acc: 0.552734]  [A loss: 0.996126, acc: 0.066406]\n",
      "672: [D loss: 0.651440, acc: 0.648438]  [A loss: 0.822841, acc: 0.285156]\n",
      "673: [D loss: 0.676460, acc: 0.560547]  [A loss: 0.992800, acc: 0.066406]\n",
      "674: [D loss: 0.660225, acc: 0.623047]  [A loss: 0.839319, acc: 0.246094]\n",
      "675: [D loss: 0.696724, acc: 0.529297]  [A loss: 1.064316, acc: 0.027344]\n",
      "676: [D loss: 0.669199, acc: 0.597656]  [A loss: 0.767405, acc: 0.339844]\n",
      "677: [D loss: 0.696256, acc: 0.539062]  [A loss: 1.087916, acc: 0.027344]\n",
      "678: [D loss: 0.666481, acc: 0.603516]  [A loss: 0.771740, acc: 0.355469]\n",
      "679: [D loss: 0.681460, acc: 0.554688]  [A loss: 1.044684, acc: 0.046875]\n",
      "680: [D loss: 0.653768, acc: 0.640625]  [A loss: 0.814191, acc: 0.265625]\n",
      "681: [D loss: 0.669018, acc: 0.585938]  [A loss: 0.928607, acc: 0.140625]\n",
      "682: [D loss: 0.651304, acc: 0.601562]  [A loss: 0.851561, acc: 0.203125]\n",
      "683: [D loss: 0.666124, acc: 0.570312]  [A loss: 0.989543, acc: 0.070312]\n",
      "684: [D loss: 0.653621, acc: 0.617188]  [A loss: 0.874054, acc: 0.195312]\n",
      "685: [D loss: 0.664556, acc: 0.585938]  [A loss: 0.942767, acc: 0.132812]\n",
      "686: [D loss: 0.660044, acc: 0.597656]  [A loss: 0.968684, acc: 0.062500]\n",
      "687: [D loss: 0.666601, acc: 0.576172]  [A loss: 0.960834, acc: 0.121094]\n",
      "688: [D loss: 0.651263, acc: 0.621094]  [A loss: 0.867544, acc: 0.183594]\n",
      "689: [D loss: 0.677627, acc: 0.564453]  [A loss: 1.120693, acc: 0.042969]\n",
      "690: [D loss: 0.647526, acc: 0.658203]  [A loss: 0.769638, acc: 0.371094]\n",
      "691: [D loss: 0.697352, acc: 0.542969]  [A loss: 1.122421, acc: 0.027344]\n",
      "692: [D loss: 0.666919, acc: 0.601562]  [A loss: 0.696086, acc: 0.519531]\n",
      "693: [D loss: 0.700092, acc: 0.537109]  [A loss: 1.098811, acc: 0.039062]\n",
      "694: [D loss: 0.653175, acc: 0.636719]  [A loss: 0.735465, acc: 0.421875]\n",
      "695: [D loss: 0.680292, acc: 0.556641]  [A loss: 1.083084, acc: 0.042969]\n",
      "696: [D loss: 0.655012, acc: 0.625000]  [A loss: 0.746866, acc: 0.425781]\n",
      "697: [D loss: 0.708222, acc: 0.535156]  [A loss: 1.069672, acc: 0.027344]\n",
      "698: [D loss: 0.661031, acc: 0.613281]  [A loss: 0.770183, acc: 0.339844]\n",
      "699: [D loss: 0.677569, acc: 0.560547]  [A loss: 0.933781, acc: 0.101562]\n",
      "700: [D loss: 0.660145, acc: 0.617188]  [A loss: 0.833145, acc: 0.261719]\n",
      "701: [D loss: 0.684709, acc: 0.556641]  [A loss: 0.973342, acc: 0.097656]\n",
      "702: [D loss: 0.660405, acc: 0.617188]  [A loss: 0.778209, acc: 0.332031]\n",
      "703: [D loss: 0.673197, acc: 0.542969]  [A loss: 0.986153, acc: 0.058594]\n",
      "704: [D loss: 0.664403, acc: 0.611328]  [A loss: 0.817467, acc: 0.316406]\n",
      "705: [D loss: 0.675013, acc: 0.570312]  [A loss: 0.965934, acc: 0.097656]\n",
      "706: [D loss: 0.671510, acc: 0.574219]  [A loss: 0.854077, acc: 0.230469]\n",
      "707: [D loss: 0.661610, acc: 0.589844]  [A loss: 0.949922, acc: 0.136719]\n",
      "708: [D loss: 0.644667, acc: 0.619141]  [A loss: 0.910623, acc: 0.167969]\n",
      "709: [D loss: 0.665995, acc: 0.597656]  [A loss: 0.897362, acc: 0.148438]\n",
      "710: [D loss: 0.661654, acc: 0.591797]  [A loss: 0.941720, acc: 0.128906]\n",
      "711: [D loss: 0.658813, acc: 0.595703]  [A loss: 0.929424, acc: 0.156250]\n",
      "712: [D loss: 0.689459, acc: 0.564453]  [A loss: 0.962131, acc: 0.089844]\n",
      "713: [D loss: 0.675111, acc: 0.578125]  [A loss: 0.916420, acc: 0.152344]\n",
      "714: [D loss: 0.674776, acc: 0.583984]  [A loss: 0.972323, acc: 0.085938]\n",
      "715: [D loss: 0.650146, acc: 0.648438]  [A loss: 0.874668, acc: 0.187500]\n",
      "716: [D loss: 0.674589, acc: 0.578125]  [A loss: 1.040258, acc: 0.085938]\n",
      "717: [D loss: 0.652182, acc: 0.605469]  [A loss: 0.781865, acc: 0.351562]\n",
      "718: [D loss: 0.683549, acc: 0.570312]  [A loss: 1.118927, acc: 0.019531]\n",
      "719: [D loss: 0.669706, acc: 0.574219]  [A loss: 0.736311, acc: 0.457031]\n",
      "720: [D loss: 0.702692, acc: 0.542969]  [A loss: 1.153450, acc: 0.031250]\n",
      "721: [D loss: 0.675115, acc: 0.568359]  [A loss: 0.720168, acc: 0.476562]\n",
      "722: [D loss: 0.687684, acc: 0.560547]  [A loss: 1.008808, acc: 0.089844]\n",
      "723: [D loss: 0.662343, acc: 0.611328]  [A loss: 0.813054, acc: 0.332031]\n",
      "724: [D loss: 0.703844, acc: 0.552734]  [A loss: 0.959131, acc: 0.082031]\n",
      "725: [D loss: 0.670184, acc: 0.566406]  [A loss: 0.815722, acc: 0.300781]\n",
      "726: [D loss: 0.655422, acc: 0.626953]  [A loss: 0.953880, acc: 0.128906]\n",
      "727: [D loss: 0.658203, acc: 0.595703]  [A loss: 0.822730, acc: 0.261719]\n",
      "728: [D loss: 0.673281, acc: 0.587891]  [A loss: 1.013283, acc: 0.062500]\n",
      "729: [D loss: 0.663412, acc: 0.597656]  [A loss: 0.806211, acc: 0.296875]\n",
      "730: [D loss: 0.692918, acc: 0.552734]  [A loss: 1.063887, acc: 0.031250]\n",
      "731: [D loss: 0.658187, acc: 0.611328]  [A loss: 0.746502, acc: 0.414062]\n",
      "732: [D loss: 0.696301, acc: 0.552734]  [A loss: 1.037216, acc: 0.046875]\n",
      "733: [D loss: 0.672097, acc: 0.585938]  [A loss: 0.782986, acc: 0.324219]\n",
      "734: [D loss: 0.681718, acc: 0.568359]  [A loss: 1.027050, acc: 0.050781]\n",
      "735: [D loss: 0.665654, acc: 0.574219]  [A loss: 0.797750, acc: 0.312500]\n",
      "736: [D loss: 0.685490, acc: 0.562500]  [A loss: 0.947976, acc: 0.105469]\n",
      "737: [D loss: 0.665594, acc: 0.617188]  [A loss: 0.833550, acc: 0.277344]\n",
      "738: [D loss: 0.688307, acc: 0.560547]  [A loss: 1.060434, acc: 0.050781]\n",
      "739: [D loss: 0.663248, acc: 0.599609]  [A loss: 0.759942, acc: 0.394531]\n",
      "740: [D loss: 0.696546, acc: 0.548828]  [A loss: 1.010796, acc: 0.050781]\n",
      "741: [D loss: 0.660173, acc: 0.623047]  [A loss: 0.785249, acc: 0.312500]\n",
      "742: [D loss: 0.684726, acc: 0.574219]  [A loss: 0.984021, acc: 0.078125]\n",
      "743: [D loss: 0.654358, acc: 0.607422]  [A loss: 0.823204, acc: 0.226562]\n",
      "744: [D loss: 0.673005, acc: 0.548828]  [A loss: 1.011540, acc: 0.054688]\n",
      "745: [D loss: 0.653590, acc: 0.615234]  [A loss: 0.784064, acc: 0.343750]\n",
      "746: [D loss: 0.680532, acc: 0.562500]  [A loss: 0.997335, acc: 0.050781]\n",
      "747: [D loss: 0.657591, acc: 0.619141]  [A loss: 0.782908, acc: 0.328125]\n",
      "748: [D loss: 0.685814, acc: 0.554688]  [A loss: 0.993367, acc: 0.066406]\n",
      "749: [D loss: 0.667640, acc: 0.597656]  [A loss: 0.782811, acc: 0.320312]\n",
      "750: [D loss: 0.674745, acc: 0.568359]  [A loss: 1.020769, acc: 0.078125]\n",
      "751: [D loss: 0.661585, acc: 0.599609]  [A loss: 0.852243, acc: 0.222656]\n",
      "752: [D loss: 0.665496, acc: 0.597656]  [A loss: 1.004388, acc: 0.066406]\n",
      "753: [D loss: 0.657786, acc: 0.609375]  [A loss: 0.833106, acc: 0.265625]\n",
      "754: [D loss: 0.663746, acc: 0.566406]  [A loss: 0.969857, acc: 0.109375]\n",
      "755: [D loss: 0.650270, acc: 0.619141]  [A loss: 0.893081, acc: 0.179688]\n",
      "756: [D loss: 0.678112, acc: 0.572266]  [A loss: 1.080401, acc: 0.046875]\n",
      "757: [D loss: 0.651021, acc: 0.638672]  [A loss: 0.739238, acc: 0.425781]\n",
      "758: [D loss: 0.686737, acc: 0.558594]  [A loss: 1.098611, acc: 0.046875]\n",
      "759: [D loss: 0.662647, acc: 0.576172]  [A loss: 0.726909, acc: 0.441406]\n",
      "760: [D loss: 0.696365, acc: 0.523438]  [A loss: 1.054592, acc: 0.062500]\n",
      "761: [D loss: 0.659593, acc: 0.609375]  [A loss: 0.742776, acc: 0.433594]\n",
      "762: [D loss: 0.680386, acc: 0.552734]  [A loss: 1.026947, acc: 0.078125]\n",
      "763: [D loss: 0.665821, acc: 0.585938]  [A loss: 0.808034, acc: 0.296875]\n",
      "764: [D loss: 0.671358, acc: 0.593750]  [A loss: 0.975418, acc: 0.105469]\n",
      "765: [D loss: 0.674244, acc: 0.564453]  [A loss: 0.847207, acc: 0.234375]\n",
      "766: [D loss: 0.673030, acc: 0.583984]  [A loss: 0.957990, acc: 0.093750]\n",
      "767: [D loss: 0.662213, acc: 0.585938]  [A loss: 0.884416, acc: 0.179688]\n",
      "768: [D loss: 0.680332, acc: 0.560547]  [A loss: 0.992925, acc: 0.070312]\n",
      "769: [D loss: 0.668343, acc: 0.595703]  [A loss: 0.838401, acc: 0.257812]\n",
      "770: [D loss: 0.680334, acc: 0.548828]  [A loss: 1.067037, acc: 0.054688]\n",
      "771: [D loss: 0.672132, acc: 0.583984]  [A loss: 0.792170, acc: 0.324219]\n",
      "772: [D loss: 0.688905, acc: 0.533203]  [A loss: 1.034456, acc: 0.066406]\n",
      "773: [D loss: 0.659589, acc: 0.599609]  [A loss: 0.822476, acc: 0.308594]\n",
      "774: [D loss: 0.681468, acc: 0.550781]  [A loss: 0.999723, acc: 0.089844]\n",
      "775: [D loss: 0.659859, acc: 0.605469]  [A loss: 0.779444, acc: 0.382812]\n",
      "776: [D loss: 0.699428, acc: 0.541016]  [A loss: 1.007435, acc: 0.070312]\n",
      "777: [D loss: 0.665548, acc: 0.609375]  [A loss: 0.794645, acc: 0.320312]\n",
      "778: [D loss: 0.693474, acc: 0.539062]  [A loss: 1.065659, acc: 0.082031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779: [D loss: 0.649354, acc: 0.644531]  [A loss: 0.856061, acc: 0.214844]\n",
      "780: [D loss: 0.678660, acc: 0.544922]  [A loss: 1.003192, acc: 0.070312]\n",
      "781: [D loss: 0.644289, acc: 0.632812]  [A loss: 0.834380, acc: 0.230469]\n",
      "782: [D loss: 0.706291, acc: 0.541016]  [A loss: 1.073040, acc: 0.039062]\n",
      "783: [D loss: 0.651709, acc: 0.603516]  [A loss: 0.823086, acc: 0.292969]\n",
      "784: [D loss: 0.678026, acc: 0.548828]  [A loss: 1.009800, acc: 0.035156]\n",
      "785: [D loss: 0.660728, acc: 0.623047]  [A loss: 0.825507, acc: 0.269531]\n",
      "786: [D loss: 0.680481, acc: 0.576172]  [A loss: 0.953780, acc: 0.144531]\n",
      "787: [D loss: 0.677624, acc: 0.580078]  [A loss: 0.928211, acc: 0.140625]\n",
      "788: [D loss: 0.684026, acc: 0.566406]  [A loss: 0.960967, acc: 0.117188]\n",
      "789: [D loss: 0.657052, acc: 0.609375]  [A loss: 0.848748, acc: 0.230469]\n",
      "790: [D loss: 0.674399, acc: 0.570312]  [A loss: 1.080035, acc: 0.046875]\n",
      "791: [D loss: 0.659646, acc: 0.621094]  [A loss: 0.792107, acc: 0.332031]\n",
      "792: [D loss: 0.688831, acc: 0.546875]  [A loss: 1.075165, acc: 0.046875]\n",
      "793: [D loss: 0.674208, acc: 0.572266]  [A loss: 0.732182, acc: 0.445312]\n",
      "794: [D loss: 0.688602, acc: 0.558594]  [A loss: 1.026427, acc: 0.113281]\n",
      "795: [D loss: 0.682842, acc: 0.582031]  [A loss: 0.835762, acc: 0.253906]\n",
      "796: [D loss: 0.702585, acc: 0.525391]  [A loss: 1.015742, acc: 0.058594]\n",
      "797: [D loss: 0.665690, acc: 0.578125]  [A loss: 0.787887, acc: 0.347656]\n",
      "798: [D loss: 0.685918, acc: 0.570312]  [A loss: 1.041548, acc: 0.058594]\n",
      "799: [D loss: 0.666863, acc: 0.601562]  [A loss: 0.764477, acc: 0.367188]\n",
      "800: [D loss: 0.689520, acc: 0.529297]  [A loss: 1.025615, acc: 0.039062]\n",
      "801: [D loss: 0.652639, acc: 0.626953]  [A loss: 0.786050, acc: 0.347656]\n",
      "802: [D loss: 0.699323, acc: 0.552734]  [A loss: 1.041365, acc: 0.039062]\n",
      "803: [D loss: 0.655160, acc: 0.617188]  [A loss: 0.755172, acc: 0.417969]\n",
      "804: [D loss: 0.679621, acc: 0.564453]  [A loss: 1.037134, acc: 0.062500]\n",
      "805: [D loss: 0.658336, acc: 0.621094]  [A loss: 0.826888, acc: 0.261719]\n",
      "806: [D loss: 0.667112, acc: 0.609375]  [A loss: 0.977712, acc: 0.117188]\n",
      "807: [D loss: 0.659421, acc: 0.605469]  [A loss: 0.854084, acc: 0.226562]\n",
      "808: [D loss: 0.689607, acc: 0.556641]  [A loss: 1.002437, acc: 0.093750]\n",
      "809: [D loss: 0.658907, acc: 0.607422]  [A loss: 0.821608, acc: 0.304688]\n",
      "810: [D loss: 0.674637, acc: 0.556641]  [A loss: 0.979208, acc: 0.109375]\n",
      "811: [D loss: 0.674643, acc: 0.560547]  [A loss: 0.876429, acc: 0.203125]\n",
      "812: [D loss: 0.665757, acc: 0.597656]  [A loss: 0.921469, acc: 0.160156]\n",
      "813: [D loss: 0.668162, acc: 0.568359]  [A loss: 0.920114, acc: 0.128906]\n",
      "814: [D loss: 0.651519, acc: 0.628906]  [A loss: 0.883533, acc: 0.156250]\n",
      "815: [D loss: 0.677530, acc: 0.580078]  [A loss: 0.991597, acc: 0.078125]\n",
      "816: [D loss: 0.673269, acc: 0.558594]  [A loss: 0.913127, acc: 0.179688]\n",
      "817: [D loss: 0.668394, acc: 0.597656]  [A loss: 0.884422, acc: 0.218750]\n",
      "818: [D loss: 0.653592, acc: 0.628906]  [A loss: 0.938496, acc: 0.171875]\n",
      "819: [D loss: 0.679576, acc: 0.552734]  [A loss: 0.955059, acc: 0.101562]\n",
      "820: [D loss: 0.666099, acc: 0.597656]  [A loss: 0.868376, acc: 0.207031]\n",
      "821: [D loss: 0.673765, acc: 0.562500]  [A loss: 0.962092, acc: 0.109375]\n",
      "822: [D loss: 0.672544, acc: 0.595703]  [A loss: 0.849689, acc: 0.214844]\n",
      "823: [D loss: 0.690824, acc: 0.558594]  [A loss: 1.104886, acc: 0.058594]\n",
      "824: [D loss: 0.672541, acc: 0.607422]  [A loss: 0.750555, acc: 0.425781]\n",
      "825: [D loss: 0.711012, acc: 0.515625]  [A loss: 1.084558, acc: 0.042969]\n",
      "826: [D loss: 0.679496, acc: 0.578125]  [A loss: 0.728344, acc: 0.449219]\n",
      "827: [D loss: 0.685431, acc: 0.539062]  [A loss: 1.074123, acc: 0.054688]\n",
      "828: [D loss: 0.664518, acc: 0.615234]  [A loss: 0.742067, acc: 0.425781]\n",
      "829: [D loss: 0.691822, acc: 0.560547]  [A loss: 1.030972, acc: 0.082031]\n",
      "830: [D loss: 0.678072, acc: 0.564453]  [A loss: 0.778577, acc: 0.367188]\n",
      "831: [D loss: 0.669823, acc: 0.578125]  [A loss: 0.978961, acc: 0.093750]\n",
      "832: [D loss: 0.676453, acc: 0.556641]  [A loss: 0.827773, acc: 0.242188]\n",
      "833: [D loss: 0.676562, acc: 0.572266]  [A loss: 0.908153, acc: 0.144531]\n",
      "834: [D loss: 0.689874, acc: 0.566406]  [A loss: 0.870811, acc: 0.203125]\n",
      "835: [D loss: 0.667902, acc: 0.587891]  [A loss: 0.923606, acc: 0.152344]\n",
      "836: [D loss: 0.671800, acc: 0.582031]  [A loss: 0.876473, acc: 0.195312]\n",
      "837: [D loss: 0.653793, acc: 0.632812]  [A loss: 0.958600, acc: 0.085938]\n",
      "838: [D loss: 0.676831, acc: 0.558594]  [A loss: 0.870175, acc: 0.191406]\n",
      "839: [D loss: 0.665308, acc: 0.589844]  [A loss: 0.975856, acc: 0.089844]\n",
      "840: [D loss: 0.664268, acc: 0.597656]  [A loss: 0.818804, acc: 0.250000]\n",
      "841: [D loss: 0.668136, acc: 0.576172]  [A loss: 1.066786, acc: 0.066406]\n",
      "842: [D loss: 0.670643, acc: 0.589844]  [A loss: 0.838252, acc: 0.273438]\n",
      "843: [D loss: 0.683289, acc: 0.564453]  [A loss: 1.114902, acc: 0.027344]\n",
      "844: [D loss: 0.670463, acc: 0.583984]  [A loss: 0.743676, acc: 0.417969]\n",
      "845: [D loss: 0.686418, acc: 0.537109]  [A loss: 1.025614, acc: 0.070312]\n",
      "846: [D loss: 0.665507, acc: 0.593750]  [A loss: 0.801488, acc: 0.289062]\n",
      "847: [D loss: 0.714532, acc: 0.525391]  [A loss: 1.085805, acc: 0.046875]\n",
      "848: [D loss: 0.662167, acc: 0.609375]  [A loss: 0.732232, acc: 0.429688]\n",
      "849: [D loss: 0.697178, acc: 0.550781]  [A loss: 0.975019, acc: 0.074219]\n",
      "850: [D loss: 0.671275, acc: 0.572266]  [A loss: 0.772131, acc: 0.363281]\n",
      "851: [D loss: 0.676167, acc: 0.597656]  [A loss: 0.908709, acc: 0.140625]\n",
      "852: [D loss: 0.660295, acc: 0.605469]  [A loss: 0.862781, acc: 0.199219]\n",
      "853: [D loss: 0.676991, acc: 0.587891]  [A loss: 0.945887, acc: 0.105469]\n",
      "854: [D loss: 0.661469, acc: 0.589844]  [A loss: 0.865041, acc: 0.179688]\n",
      "855: [D loss: 0.671005, acc: 0.564453]  [A loss: 0.896403, acc: 0.144531]\n",
      "856: [D loss: 0.665320, acc: 0.593750]  [A loss: 0.867536, acc: 0.203125]\n",
      "857: [D loss: 0.683319, acc: 0.548828]  [A loss: 1.019728, acc: 0.082031]\n",
      "858: [D loss: 0.672505, acc: 0.589844]  [A loss: 0.781683, acc: 0.343750]\n",
      "859: [D loss: 0.687180, acc: 0.560547]  [A loss: 1.091911, acc: 0.035156]\n",
      "860: [D loss: 0.668379, acc: 0.589844]  [A loss: 0.747112, acc: 0.382812]\n",
      "861: [D loss: 0.698914, acc: 0.548828]  [A loss: 1.055536, acc: 0.039062]\n",
      "862: [D loss: 0.670089, acc: 0.591797]  [A loss: 0.811106, acc: 0.320312]\n",
      "863: [D loss: 0.690217, acc: 0.554688]  [A loss: 1.035078, acc: 0.054688]\n",
      "864: [D loss: 0.670228, acc: 0.578125]  [A loss: 0.816134, acc: 0.285156]\n",
      "865: [D loss: 0.684665, acc: 0.554688]  [A loss: 0.916168, acc: 0.136719]\n",
      "866: [D loss: 0.654348, acc: 0.609375]  [A loss: 0.856547, acc: 0.234375]\n",
      "867: [D loss: 0.654991, acc: 0.585938]  [A loss: 0.906388, acc: 0.140625]\n",
      "868: [D loss: 0.678419, acc: 0.552734]  [A loss: 0.883312, acc: 0.195312]\n",
      "869: [D loss: 0.666219, acc: 0.601562]  [A loss: 0.933493, acc: 0.152344]\n",
      "870: [D loss: 0.675841, acc: 0.576172]  [A loss: 0.864151, acc: 0.207031]\n",
      "871: [D loss: 0.679346, acc: 0.583984]  [A loss: 0.915795, acc: 0.171875]\n",
      "872: [D loss: 0.680812, acc: 0.554688]  [A loss: 0.918251, acc: 0.136719]\n",
      "873: [D loss: 0.670167, acc: 0.570312]  [A loss: 0.929380, acc: 0.121094]\n",
      "874: [D loss: 0.672187, acc: 0.572266]  [A loss: 0.926110, acc: 0.144531]\n",
      "875: [D loss: 0.676997, acc: 0.558594]  [A loss: 0.897897, acc: 0.175781]\n",
      "876: [D loss: 0.670553, acc: 0.589844]  [A loss: 0.940634, acc: 0.121094]\n",
      "877: [D loss: 0.668790, acc: 0.587891]  [A loss: 0.861045, acc: 0.199219]\n",
      "878: [D loss: 0.658588, acc: 0.564453]  [A loss: 0.993600, acc: 0.101562]\n",
      "879: [D loss: 0.662800, acc: 0.593750]  [A loss: 0.827296, acc: 0.261719]\n",
      "880: [D loss: 0.673718, acc: 0.611328]  [A loss: 1.045653, acc: 0.054688]\n",
      "881: [D loss: 0.657851, acc: 0.611328]  [A loss: 0.771269, acc: 0.355469]\n",
      "882: [D loss: 0.688510, acc: 0.562500]  [A loss: 1.135888, acc: 0.003906]\n",
      "883: [D loss: 0.666938, acc: 0.576172]  [A loss: 0.694772, acc: 0.503906]\n",
      "884: [D loss: 0.713327, acc: 0.525391]  [A loss: 1.155502, acc: 0.058594]\n",
      "885: [D loss: 0.656895, acc: 0.595703]  [A loss: 0.710639, acc: 0.500000]\n",
      "886: [D loss: 0.749427, acc: 0.527344]  [A loss: 1.097354, acc: 0.027344]\n",
      "887: [D loss: 0.658370, acc: 0.603516]  [A loss: 0.763798, acc: 0.371094]\n",
      "888: [D loss: 0.698544, acc: 0.511719]  [A loss: 0.945740, acc: 0.105469]\n",
      "889: [D loss: 0.653380, acc: 0.611328]  [A loss: 0.828891, acc: 0.277344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890: [D loss: 0.682241, acc: 0.554688]  [A loss: 0.925907, acc: 0.125000]\n",
      "891: [D loss: 0.664953, acc: 0.615234]  [A loss: 0.886053, acc: 0.175781]\n",
      "892: [D loss: 0.661025, acc: 0.607422]  [A loss: 0.909748, acc: 0.132812]\n",
      "893: [D loss: 0.665999, acc: 0.599609]  [A loss: 0.909896, acc: 0.144531]\n",
      "894: [D loss: 0.653121, acc: 0.611328]  [A loss: 0.934701, acc: 0.125000]\n",
      "895: [D loss: 0.680661, acc: 0.574219]  [A loss: 0.891031, acc: 0.117188]\n",
      "896: [D loss: 0.658723, acc: 0.582031]  [A loss: 0.914990, acc: 0.152344]\n",
      "897: [D loss: 0.683039, acc: 0.578125]  [A loss: 0.964974, acc: 0.093750]\n",
      "898: [D loss: 0.681542, acc: 0.570312]  [A loss: 0.896276, acc: 0.144531]\n",
      "899: [D loss: 0.665017, acc: 0.611328]  [A loss: 0.946750, acc: 0.121094]\n",
      "900: [D loss: 0.668047, acc: 0.582031]  [A loss: 0.986173, acc: 0.078125]\n",
      "901: [D loss: 0.680727, acc: 0.554688]  [A loss: 0.795905, acc: 0.335938]\n",
      "902: [D loss: 0.695163, acc: 0.548828]  [A loss: 1.048768, acc: 0.058594]\n",
      "903: [D loss: 0.650990, acc: 0.617188]  [A loss: 0.739049, acc: 0.441406]\n",
      "904: [D loss: 0.696420, acc: 0.558594]  [A loss: 1.091531, acc: 0.050781]\n",
      "905: [D loss: 0.656625, acc: 0.595703]  [A loss: 0.710028, acc: 0.488281]\n",
      "906: [D loss: 0.724605, acc: 0.513672]  [A loss: 1.098171, acc: 0.050781]\n",
      "907: [D loss: 0.672463, acc: 0.556641]  [A loss: 0.742468, acc: 0.433594]\n",
      "908: [D loss: 0.675903, acc: 0.556641]  [A loss: 0.998607, acc: 0.078125]\n",
      "909: [D loss: 0.651673, acc: 0.654297]  [A loss: 0.788164, acc: 0.359375]\n",
      "910: [D loss: 0.686021, acc: 0.560547]  [A loss: 1.004078, acc: 0.074219]\n",
      "911: [D loss: 0.673525, acc: 0.582031]  [A loss: 0.787359, acc: 0.351562]\n",
      "912: [D loss: 0.675911, acc: 0.587891]  [A loss: 0.901339, acc: 0.148438]\n",
      "913: [D loss: 0.673166, acc: 0.585938]  [A loss: 0.855982, acc: 0.183594]\n",
      "914: [D loss: 0.659310, acc: 0.621094]  [A loss: 0.938412, acc: 0.109375]\n",
      "915: [D loss: 0.690707, acc: 0.556641]  [A loss: 0.977274, acc: 0.109375]\n",
      "916: [D loss: 0.653720, acc: 0.634766]  [A loss: 0.805054, acc: 0.332031]\n",
      "917: [D loss: 0.658960, acc: 0.587891]  [A loss: 0.952520, acc: 0.117188]\n",
      "918: [D loss: 0.657263, acc: 0.617188]  [A loss: 0.860281, acc: 0.238281]\n",
      "919: [D loss: 0.674959, acc: 0.580078]  [A loss: 0.976756, acc: 0.078125]\n",
      "920: [D loss: 0.664947, acc: 0.609375]  [A loss: 0.837721, acc: 0.261719]\n",
      "921: [D loss: 0.673319, acc: 0.550781]  [A loss: 1.079305, acc: 0.046875]\n",
      "922: [D loss: 0.665926, acc: 0.564453]  [A loss: 0.767216, acc: 0.390625]\n",
      "923: [D loss: 0.708066, acc: 0.527344]  [A loss: 1.066754, acc: 0.023438]\n",
      "924: [D loss: 0.682575, acc: 0.546875]  [A loss: 0.812678, acc: 0.296875]\n",
      "925: [D loss: 0.684587, acc: 0.554688]  [A loss: 0.977717, acc: 0.117188]\n",
      "926: [D loss: 0.669231, acc: 0.574219]  [A loss: 0.845874, acc: 0.261719]\n",
      "927: [D loss: 0.660634, acc: 0.587891]  [A loss: 0.973300, acc: 0.097656]\n",
      "928: [D loss: 0.673434, acc: 0.568359]  [A loss: 0.877591, acc: 0.222656]\n",
      "929: [D loss: 0.680781, acc: 0.558594]  [A loss: 0.942591, acc: 0.132812]\n",
      "930: [D loss: 0.676947, acc: 0.574219]  [A loss: 0.852614, acc: 0.222656]\n",
      "931: [D loss: 0.674779, acc: 0.566406]  [A loss: 0.981617, acc: 0.101562]\n",
      "932: [D loss: 0.648795, acc: 0.630859]  [A loss: 0.786824, acc: 0.324219]\n",
      "933: [D loss: 0.682285, acc: 0.546875]  [A loss: 1.003211, acc: 0.058594]\n",
      "934: [D loss: 0.662404, acc: 0.619141]  [A loss: 0.802863, acc: 0.343750]\n",
      "935: [D loss: 0.698879, acc: 0.550781]  [A loss: 1.065237, acc: 0.062500]\n",
      "936: [D loss: 0.667285, acc: 0.585938]  [A loss: 0.752892, acc: 0.406250]\n",
      "937: [D loss: 0.693732, acc: 0.531250]  [A loss: 1.056571, acc: 0.050781]\n",
      "938: [D loss: 0.668200, acc: 0.597656]  [A loss: 0.750585, acc: 0.414062]\n",
      "939: [D loss: 0.686032, acc: 0.558594]  [A loss: 1.036162, acc: 0.042969]\n",
      "940: [D loss: 0.676079, acc: 0.558594]  [A loss: 0.824620, acc: 0.296875]\n",
      "941: [D loss: 0.676245, acc: 0.582031]  [A loss: 1.001349, acc: 0.066406]\n",
      "942: [D loss: 0.662008, acc: 0.595703]  [A loss: 0.787556, acc: 0.347656]\n",
      "943: [D loss: 0.695089, acc: 0.560547]  [A loss: 0.965984, acc: 0.097656]\n",
      "944: [D loss: 0.651021, acc: 0.640625]  [A loss: 0.769406, acc: 0.398438]\n",
      "945: [D loss: 0.691315, acc: 0.548828]  [A loss: 1.073062, acc: 0.054688]\n",
      "946: [D loss: 0.658958, acc: 0.630859]  [A loss: 0.753125, acc: 0.406250]\n",
      "947: [D loss: 0.698242, acc: 0.548828]  [A loss: 1.021747, acc: 0.093750]\n",
      "948: [D loss: 0.667422, acc: 0.589844]  [A loss: 0.782541, acc: 0.347656]\n",
      "949: [D loss: 0.686464, acc: 0.562500]  [A loss: 0.951104, acc: 0.128906]\n",
      "950: [D loss: 0.660243, acc: 0.625000]  [A loss: 0.815369, acc: 0.296875]\n",
      "951: [D loss: 0.692648, acc: 0.529297]  [A loss: 0.950841, acc: 0.121094]\n",
      "952: [D loss: 0.661018, acc: 0.603516]  [A loss: 0.889703, acc: 0.191406]\n",
      "953: [D loss: 0.662036, acc: 0.611328]  [A loss: 0.946781, acc: 0.078125]\n",
      "954: [D loss: 0.668218, acc: 0.583984]  [A loss: 0.860794, acc: 0.207031]\n",
      "955: [D loss: 0.665227, acc: 0.595703]  [A loss: 0.929807, acc: 0.160156]\n",
      "956: [D loss: 0.663077, acc: 0.623047]  [A loss: 0.835097, acc: 0.242188]\n",
      "957: [D loss: 0.668887, acc: 0.562500]  [A loss: 1.045235, acc: 0.046875]\n",
      "958: [D loss: 0.674419, acc: 0.564453]  [A loss: 0.819414, acc: 0.285156]\n",
      "959: [D loss: 0.673598, acc: 0.570312]  [A loss: 1.042868, acc: 0.054688]\n",
      "960: [D loss: 0.665658, acc: 0.601562]  [A loss: 0.762052, acc: 0.390625]\n",
      "961: [D loss: 0.693769, acc: 0.562500]  [A loss: 1.077841, acc: 0.074219]\n",
      "962: [D loss: 0.667404, acc: 0.572266]  [A loss: 0.767045, acc: 0.398438]\n",
      "963: [D loss: 0.682121, acc: 0.570312]  [A loss: 0.983652, acc: 0.089844]\n",
      "964: [D loss: 0.679779, acc: 0.570312]  [A loss: 0.816575, acc: 0.281250]\n",
      "965: [D loss: 0.675241, acc: 0.576172]  [A loss: 0.972552, acc: 0.085938]\n",
      "966: [D loss: 0.668914, acc: 0.603516]  [A loss: 0.844867, acc: 0.242188]\n",
      "967: [D loss: 0.656533, acc: 0.593750]  [A loss: 0.902548, acc: 0.195312]\n",
      "968: [D loss: 0.667877, acc: 0.580078]  [A loss: 0.902837, acc: 0.167969]\n",
      "969: [D loss: 0.659519, acc: 0.597656]  [A loss: 0.970202, acc: 0.097656]\n",
      "970: [D loss: 0.649182, acc: 0.636719]  [A loss: 0.819848, acc: 0.273438]\n",
      "971: [D loss: 0.692057, acc: 0.544922]  [A loss: 1.126202, acc: 0.031250]\n",
      "972: [D loss: 0.681456, acc: 0.554688]  [A loss: 0.708512, acc: 0.488281]\n",
      "973: [D loss: 0.698779, acc: 0.541016]  [A loss: 1.121187, acc: 0.035156]\n",
      "974: [D loss: 0.670782, acc: 0.589844]  [A loss: 0.758750, acc: 0.375000]\n",
      "975: [D loss: 0.710583, acc: 0.529297]  [A loss: 1.000229, acc: 0.082031]\n",
      "976: [D loss: 0.671768, acc: 0.568359]  [A loss: 0.793710, acc: 0.335938]\n",
      "977: [D loss: 0.663891, acc: 0.595703]  [A loss: 0.939927, acc: 0.152344]\n",
      "978: [D loss: 0.664452, acc: 0.613281]  [A loss: 0.838528, acc: 0.261719]\n",
      "979: [D loss: 0.679642, acc: 0.568359]  [A loss: 0.949891, acc: 0.101562]\n",
      "980: [D loss: 0.667331, acc: 0.599609]  [A loss: 0.867720, acc: 0.191406]\n",
      "981: [D loss: 0.679383, acc: 0.566406]  [A loss: 0.923624, acc: 0.109375]\n",
      "982: [D loss: 0.660125, acc: 0.621094]  [A loss: 0.832371, acc: 0.246094]\n",
      "983: [D loss: 0.679459, acc: 0.580078]  [A loss: 0.909868, acc: 0.148438]\n",
      "984: [D loss: 0.664724, acc: 0.609375]  [A loss: 0.810077, acc: 0.285156]\n",
      "985: [D loss: 0.674445, acc: 0.607422]  [A loss: 0.999173, acc: 0.097656]\n",
      "986: [D loss: 0.677460, acc: 0.552734]  [A loss: 0.906351, acc: 0.140625]\n",
      "987: [D loss: 0.659520, acc: 0.591797]  [A loss: 0.900162, acc: 0.195312]\n",
      "988: [D loss: 0.674243, acc: 0.587891]  [A loss: 0.922918, acc: 0.128906]\n",
      "989: [D loss: 0.660193, acc: 0.599609]  [A loss: 0.885803, acc: 0.175781]\n",
      "990: [D loss: 0.698603, acc: 0.552734]  [A loss: 1.017230, acc: 0.058594]\n",
      "991: [D loss: 0.661477, acc: 0.603516]  [A loss: 0.844924, acc: 0.230469]\n",
      "992: [D loss: 0.705674, acc: 0.562500]  [A loss: 1.073366, acc: 0.031250]\n",
      "993: [D loss: 0.669192, acc: 0.609375]  [A loss: 0.810769, acc: 0.292969]\n",
      "994: [D loss: 0.687321, acc: 0.537109]  [A loss: 1.025642, acc: 0.093750]\n",
      "995: [D loss: 0.654743, acc: 0.625000]  [A loss: 0.769818, acc: 0.394531]\n",
      "996: [D loss: 0.693705, acc: 0.548828]  [A loss: 1.064045, acc: 0.050781]\n",
      "997: [D loss: 0.665610, acc: 0.605469]  [A loss: 0.759736, acc: 0.386719]\n",
      "998: [D loss: 0.708812, acc: 0.544922]  [A loss: 1.032447, acc: 0.062500]\n",
      "999: [D loss: 0.669353, acc: 0.582031]  [A loss: 0.731937, acc: 0.421875]\n",
      "1000: [D loss: 0.704213, acc: 0.537109]  [A loss: 1.100661, acc: 0.027344]\n",
      "1001: [D loss: 0.664568, acc: 0.595703]  [A loss: 0.739320, acc: 0.445312]\n",
      "1002: [D loss: 0.691735, acc: 0.554688]  [A loss: 0.988796, acc: 0.085938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003: [D loss: 0.661041, acc: 0.583984]  [A loss: 0.803825, acc: 0.316406]\n",
      "1004: [D loss: 0.678530, acc: 0.564453]  [A loss: 0.957038, acc: 0.144531]\n",
      "1005: [D loss: 0.663567, acc: 0.587891]  [A loss: 0.848907, acc: 0.222656]\n",
      "1006: [D loss: 0.666694, acc: 0.583984]  [A loss: 0.902692, acc: 0.195312]\n",
      "1007: [D loss: 0.688201, acc: 0.542969]  [A loss: 0.938418, acc: 0.167969]\n",
      "1008: [D loss: 0.680986, acc: 0.535156]  [A loss: 0.890825, acc: 0.171875]\n",
      "1009: [D loss: 0.670293, acc: 0.593750]  [A loss: 0.901332, acc: 0.156250]\n",
      "1010: [D loss: 0.667116, acc: 0.576172]  [A loss: 0.905675, acc: 0.171875]\n",
      "1011: [D loss: 0.681284, acc: 0.582031]  [A loss: 1.027736, acc: 0.085938]\n",
      "1012: [D loss: 0.685420, acc: 0.566406]  [A loss: 0.740609, acc: 0.441406]\n",
      "1013: [D loss: 0.690468, acc: 0.541016]  [A loss: 1.036239, acc: 0.093750]\n",
      "1014: [D loss: 0.654285, acc: 0.619141]  [A loss: 0.780096, acc: 0.355469]\n",
      "1015: [D loss: 0.726552, acc: 0.533203]  [A loss: 1.008122, acc: 0.054688]\n",
      "1016: [D loss: 0.681858, acc: 0.554688]  [A loss: 0.803672, acc: 0.332031]\n",
      "1017: [D loss: 0.694312, acc: 0.531250]  [A loss: 0.982001, acc: 0.085938]\n",
      "1018: [D loss: 0.658674, acc: 0.625000]  [A loss: 0.778149, acc: 0.371094]\n",
      "1019: [D loss: 0.693861, acc: 0.537109]  [A loss: 1.052289, acc: 0.082031]\n",
      "1020: [D loss: 0.666001, acc: 0.578125]  [A loss: 0.814540, acc: 0.292969]\n",
      "1021: [D loss: 0.678410, acc: 0.576172]  [A loss: 0.964084, acc: 0.132812]\n",
      "1022: [D loss: 0.656698, acc: 0.619141]  [A loss: 0.820204, acc: 0.292969]\n",
      "1023: [D loss: 0.669953, acc: 0.583984]  [A loss: 0.944348, acc: 0.105469]\n",
      "1024: [D loss: 0.657472, acc: 0.603516]  [A loss: 0.871074, acc: 0.210938]\n",
      "1025: [D loss: 0.692797, acc: 0.548828]  [A loss: 1.005791, acc: 0.054688]\n",
      "1026: [D loss: 0.683509, acc: 0.548828]  [A loss: 0.819661, acc: 0.269531]\n",
      "1027: [D loss: 0.677956, acc: 0.583984]  [A loss: 0.946872, acc: 0.089844]\n",
      "1028: [D loss: 0.671536, acc: 0.583984]  [A loss: 0.877267, acc: 0.191406]\n",
      "1029: [D loss: 0.678468, acc: 0.560547]  [A loss: 0.949625, acc: 0.125000]\n",
      "1030: [D loss: 0.675285, acc: 0.572266]  [A loss: 0.999269, acc: 0.074219]\n",
      "1031: [D loss: 0.663882, acc: 0.613281]  [A loss: 0.819070, acc: 0.308594]\n",
      "1032: [D loss: 0.676621, acc: 0.605469]  [A loss: 1.033872, acc: 0.050781]\n",
      "1033: [D loss: 0.663936, acc: 0.623047]  [A loss: 0.769113, acc: 0.355469]\n",
      "1034: [D loss: 0.685328, acc: 0.582031]  [A loss: 1.003380, acc: 0.066406]\n",
      "1035: [D loss: 0.675580, acc: 0.587891]  [A loss: 0.757949, acc: 0.417969]\n",
      "1036: [D loss: 0.691041, acc: 0.568359]  [A loss: 1.035371, acc: 0.082031]\n",
      "1037: [D loss: 0.661589, acc: 0.587891]  [A loss: 0.742145, acc: 0.445312]\n",
      "1038: [D loss: 0.698983, acc: 0.541016]  [A loss: 1.083283, acc: 0.039062]\n",
      "1039: [D loss: 0.673768, acc: 0.568359]  [A loss: 0.692516, acc: 0.554688]\n",
      "1040: [D loss: 0.704966, acc: 0.564453]  [A loss: 0.988012, acc: 0.078125]\n",
      "1041: [D loss: 0.672827, acc: 0.595703]  [A loss: 0.760444, acc: 0.390625]\n",
      "1042: [D loss: 0.693629, acc: 0.550781]  [A loss: 0.956780, acc: 0.105469]\n",
      "1043: [D loss: 0.659812, acc: 0.605469]  [A loss: 0.777841, acc: 0.351562]\n",
      "1044: [D loss: 0.681092, acc: 0.570312]  [A loss: 0.925502, acc: 0.140625]\n",
      "1045: [D loss: 0.676006, acc: 0.566406]  [A loss: 0.807224, acc: 0.285156]\n",
      "1046: [D loss: 0.683705, acc: 0.570312]  [A loss: 0.916046, acc: 0.164062]\n",
      "1047: [D loss: 0.665110, acc: 0.585938]  [A loss: 0.861040, acc: 0.218750]\n",
      "1048: [D loss: 0.667067, acc: 0.617188]  [A loss: 0.882885, acc: 0.199219]\n",
      "1049: [D loss: 0.689322, acc: 0.546875]  [A loss: 0.999371, acc: 0.050781]\n",
      "1050: [D loss: 0.682177, acc: 0.566406]  [A loss: 0.749719, acc: 0.402344]\n",
      "1051: [D loss: 0.686037, acc: 0.578125]  [A loss: 0.985891, acc: 0.093750]\n",
      "1052: [D loss: 0.654284, acc: 0.603516]  [A loss: 0.804360, acc: 0.312500]\n",
      "1053: [D loss: 0.693232, acc: 0.546875]  [A loss: 0.968658, acc: 0.093750]\n",
      "1054: [D loss: 0.670954, acc: 0.550781]  [A loss: 0.782922, acc: 0.332031]\n",
      "1055: [D loss: 0.680644, acc: 0.560547]  [A loss: 0.912042, acc: 0.160156]\n",
      "1056: [D loss: 0.682375, acc: 0.556641]  [A loss: 0.850213, acc: 0.207031]\n",
      "1057: [D loss: 0.684386, acc: 0.582031]  [A loss: 0.921426, acc: 0.128906]\n",
      "1058: [D loss: 0.666144, acc: 0.603516]  [A loss: 0.799368, acc: 0.328125]\n",
      "1059: [D loss: 0.684411, acc: 0.562500]  [A loss: 0.932560, acc: 0.128906]\n",
      "1060: [D loss: 0.679265, acc: 0.572266]  [A loss: 0.909482, acc: 0.152344]\n",
      "1061: [D loss: 0.678051, acc: 0.570312]  [A loss: 0.852635, acc: 0.203125]\n",
      "1062: [D loss: 0.684001, acc: 0.556641]  [A loss: 0.864937, acc: 0.207031]\n",
      "1063: [D loss: 0.654489, acc: 0.621094]  [A loss: 0.825389, acc: 0.273438]\n",
      "1064: [D loss: 0.697584, acc: 0.533203]  [A loss: 0.952373, acc: 0.117188]\n",
      "1065: [D loss: 0.675093, acc: 0.576172]  [A loss: 0.885902, acc: 0.187500]\n",
      "1066: [D loss: 0.671174, acc: 0.580078]  [A loss: 0.935769, acc: 0.128906]\n",
      "1067: [D loss: 0.671149, acc: 0.587891]  [A loss: 0.877013, acc: 0.175781]\n",
      "1068: [D loss: 0.694846, acc: 0.552734]  [A loss: 1.057712, acc: 0.035156]\n",
      "1069: [D loss: 0.675602, acc: 0.554688]  [A loss: 0.736381, acc: 0.429688]\n",
      "1070: [D loss: 0.712875, acc: 0.531250]  [A loss: 1.102594, acc: 0.042969]\n",
      "1071: [D loss: 0.679089, acc: 0.560547]  [A loss: 0.715150, acc: 0.460938]\n",
      "1072: [D loss: 0.700720, acc: 0.548828]  [A loss: 1.045155, acc: 0.054688]\n",
      "1073: [D loss: 0.661609, acc: 0.607422]  [A loss: 0.776360, acc: 0.375000]\n",
      "1074: [D loss: 0.708098, acc: 0.523438]  [A loss: 0.977952, acc: 0.089844]\n",
      "1075: [D loss: 0.679477, acc: 0.578125]  [A loss: 0.806608, acc: 0.285156]\n",
      "1076: [D loss: 0.668103, acc: 0.589844]  [A loss: 0.891464, acc: 0.140625]\n",
      "1077: [D loss: 0.665481, acc: 0.597656]  [A loss: 0.938600, acc: 0.152344]\n",
      "1078: [D loss: 0.674818, acc: 0.578125]  [A loss: 0.859125, acc: 0.218750]\n",
      "1079: [D loss: 0.678322, acc: 0.580078]  [A loss: 0.827766, acc: 0.250000]\n",
      "1080: [D loss: 0.671301, acc: 0.587891]  [A loss: 0.926001, acc: 0.128906]\n",
      "1081: [D loss: 0.667631, acc: 0.595703]  [A loss: 0.796651, acc: 0.304688]\n",
      "1082: [D loss: 0.685516, acc: 0.564453]  [A loss: 1.051320, acc: 0.054688]\n",
      "1083: [D loss: 0.677401, acc: 0.562500]  [A loss: 0.722423, acc: 0.507812]\n",
      "1084: [D loss: 0.703369, acc: 0.560547]  [A loss: 1.024473, acc: 0.058594]\n",
      "1085: [D loss: 0.696836, acc: 0.523438]  [A loss: 0.750269, acc: 0.421875]\n",
      "1086: [D loss: 0.669831, acc: 0.587891]  [A loss: 0.938176, acc: 0.132812]\n",
      "1087: [D loss: 0.661086, acc: 0.599609]  [A loss: 0.818519, acc: 0.246094]\n",
      "1088: [D loss: 0.691656, acc: 0.554688]  [A loss: 0.949175, acc: 0.128906]\n",
      "1089: [D loss: 0.668537, acc: 0.589844]  [A loss: 0.789812, acc: 0.355469]\n",
      "1090: [D loss: 0.679148, acc: 0.552734]  [A loss: 0.961733, acc: 0.117188]\n",
      "1091: [D loss: 0.670222, acc: 0.587891]  [A loss: 0.796947, acc: 0.347656]\n",
      "1092: [D loss: 0.688949, acc: 0.560547]  [A loss: 0.980053, acc: 0.109375]\n",
      "1093: [D loss: 0.669108, acc: 0.585938]  [A loss: 0.821725, acc: 0.289062]\n",
      "1094: [D loss: 0.684683, acc: 0.564453]  [A loss: 0.964532, acc: 0.089844]\n",
      "1095: [D loss: 0.672925, acc: 0.583984]  [A loss: 0.825079, acc: 0.257812]\n",
      "1096: [D loss: 0.676217, acc: 0.615234]  [A loss: 0.983192, acc: 0.101562]\n",
      "1097: [D loss: 0.680128, acc: 0.568359]  [A loss: 0.827475, acc: 0.285156]\n",
      "1098: [D loss: 0.677624, acc: 0.587891]  [A loss: 0.927987, acc: 0.132812]\n",
      "1099: [D loss: 0.678224, acc: 0.572266]  [A loss: 0.810236, acc: 0.285156]\n",
      "1100: [D loss: 0.668187, acc: 0.578125]  [A loss: 0.989025, acc: 0.148438]\n",
      "1101: [D loss: 0.682825, acc: 0.556641]  [A loss: 0.821807, acc: 0.277344]\n",
      "1102: [D loss: 0.671117, acc: 0.572266]  [A loss: 0.919005, acc: 0.148438]\n",
      "1103: [D loss: 0.665822, acc: 0.599609]  [A loss: 0.833383, acc: 0.261719]\n",
      "1104: [D loss: 0.666292, acc: 0.603516]  [A loss: 0.982167, acc: 0.128906]\n",
      "1105: [D loss: 0.664726, acc: 0.589844]  [A loss: 0.840907, acc: 0.257812]\n",
      "1106: [D loss: 0.690290, acc: 0.562500]  [A loss: 1.070707, acc: 0.062500]\n",
      "1107: [D loss: 0.661469, acc: 0.607422]  [A loss: 0.735117, acc: 0.417969]\n",
      "1108: [D loss: 0.685465, acc: 0.544922]  [A loss: 1.025144, acc: 0.089844]\n",
      "1109: [D loss: 0.676553, acc: 0.578125]  [A loss: 0.765215, acc: 0.406250]\n",
      "1110: [D loss: 0.695090, acc: 0.564453]  [A loss: 1.047710, acc: 0.042969]\n",
      "1111: [D loss: 0.677553, acc: 0.576172]  [A loss: 0.771663, acc: 0.363281]\n",
      "1112: [D loss: 0.694393, acc: 0.533203]  [A loss: 0.977373, acc: 0.136719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113: [D loss: 0.680012, acc: 0.556641]  [A loss: 0.787923, acc: 0.316406]\n",
      "1114: [D loss: 0.691817, acc: 0.531250]  [A loss: 0.934056, acc: 0.117188]\n",
      "1115: [D loss: 0.677620, acc: 0.585938]  [A loss: 0.848876, acc: 0.246094]\n",
      "1116: [D loss: 0.665551, acc: 0.572266]  [A loss: 0.913668, acc: 0.183594]\n",
      "1117: [D loss: 0.675416, acc: 0.574219]  [A loss: 0.910979, acc: 0.148438]\n",
      "1118: [D loss: 0.682997, acc: 0.560547]  [A loss: 0.908549, acc: 0.183594]\n",
      "1119: [D loss: 0.667094, acc: 0.574219]  [A loss: 0.892134, acc: 0.187500]\n",
      "1120: [D loss: 0.667833, acc: 0.605469]  [A loss: 0.848743, acc: 0.242188]\n",
      "1121: [D loss: 0.666757, acc: 0.583984]  [A loss: 0.969759, acc: 0.093750]\n",
      "1122: [D loss: 0.663381, acc: 0.595703]  [A loss: 0.882621, acc: 0.179688]\n",
      "1123: [D loss: 0.665258, acc: 0.603516]  [A loss: 0.948515, acc: 0.132812]\n",
      "1124: [D loss: 0.668065, acc: 0.576172]  [A loss: 0.926505, acc: 0.132812]\n",
      "1125: [D loss: 0.675066, acc: 0.582031]  [A loss: 0.914274, acc: 0.164062]\n",
      "1126: [D loss: 0.685048, acc: 0.566406]  [A loss: 0.900578, acc: 0.144531]\n",
      "1127: [D loss: 0.677942, acc: 0.550781]  [A loss: 0.859984, acc: 0.230469]\n",
      "1128: [D loss: 0.671368, acc: 0.583984]  [A loss: 0.939707, acc: 0.140625]\n",
      "1129: [D loss: 0.660669, acc: 0.613281]  [A loss: 0.828575, acc: 0.300781]\n",
      "1130: [D loss: 0.685147, acc: 0.554688]  [A loss: 1.064180, acc: 0.062500]\n",
      "1131: [D loss: 0.673575, acc: 0.542969]  [A loss: 0.744514, acc: 0.421875]\n",
      "1132: [D loss: 0.708620, acc: 0.521484]  [A loss: 1.008754, acc: 0.074219]\n",
      "1133: [D loss: 0.676298, acc: 0.589844]  [A loss: 0.821629, acc: 0.261719]\n",
      "1134: [D loss: 0.713257, acc: 0.515625]  [A loss: 1.099805, acc: 0.058594]\n",
      "1135: [D loss: 0.672650, acc: 0.580078]  [A loss: 0.682114, acc: 0.546875]\n",
      "1136: [D loss: 0.707875, acc: 0.533203]  [A loss: 1.003504, acc: 0.097656]\n",
      "1137: [D loss: 0.674844, acc: 0.599609]  [A loss: 0.760968, acc: 0.367188]\n",
      "1138: [D loss: 0.700746, acc: 0.558594]  [A loss: 0.927492, acc: 0.132812]\n",
      "1139: [D loss: 0.682636, acc: 0.580078]  [A loss: 0.830927, acc: 0.246094]\n",
      "1140: [D loss: 0.681072, acc: 0.562500]  [A loss: 0.927493, acc: 0.148438]\n",
      "1141: [D loss: 0.669446, acc: 0.611328]  [A loss: 0.883477, acc: 0.199219]\n",
      "1142: [D loss: 0.663672, acc: 0.583984]  [A loss: 0.820745, acc: 0.273438]\n",
      "1143: [D loss: 0.689487, acc: 0.568359]  [A loss: 0.976324, acc: 0.113281]\n",
      "1144: [D loss: 0.683601, acc: 0.574219]  [A loss: 0.785584, acc: 0.289062]\n",
      "1145: [D loss: 0.690005, acc: 0.527344]  [A loss: 1.020219, acc: 0.078125]\n",
      "1146: [D loss: 0.673680, acc: 0.580078]  [A loss: 0.734586, acc: 0.460938]\n",
      "1147: [D loss: 0.716142, acc: 0.519531]  [A loss: 0.974944, acc: 0.085938]\n",
      "1148: [D loss: 0.668759, acc: 0.582031]  [A loss: 0.822663, acc: 0.250000]\n",
      "1149: [D loss: 0.679366, acc: 0.566406]  [A loss: 0.890044, acc: 0.136719]\n",
      "1150: [D loss: 0.673632, acc: 0.562500]  [A loss: 0.816026, acc: 0.285156]\n",
      "1151: [D loss: 0.681782, acc: 0.572266]  [A loss: 0.974345, acc: 0.109375]\n",
      "1152: [D loss: 0.666887, acc: 0.613281]  [A loss: 0.776148, acc: 0.343750]\n",
      "1153: [D loss: 0.687557, acc: 0.570312]  [A loss: 0.990452, acc: 0.066406]\n",
      "1154: [D loss: 0.667415, acc: 0.587891]  [A loss: 0.843702, acc: 0.230469]\n",
      "1155: [D loss: 0.690182, acc: 0.535156]  [A loss: 0.944793, acc: 0.101562]\n",
      "1156: [D loss: 0.664560, acc: 0.619141]  [A loss: 0.781481, acc: 0.355469]\n",
      "1157: [D loss: 0.709684, acc: 0.542969]  [A loss: 0.996371, acc: 0.062500]\n",
      "1158: [D loss: 0.679312, acc: 0.562500]  [A loss: 0.749864, acc: 0.402344]\n",
      "1159: [D loss: 0.700279, acc: 0.529297]  [A loss: 1.007327, acc: 0.066406]\n",
      "1160: [D loss: 0.647657, acc: 0.648438]  [A loss: 0.778567, acc: 0.355469]\n",
      "1161: [D loss: 0.700907, acc: 0.541016]  [A loss: 0.975477, acc: 0.097656]\n",
      "1162: [D loss: 0.679546, acc: 0.572266]  [A loss: 0.764365, acc: 0.375000]\n",
      "1163: [D loss: 0.684304, acc: 0.541016]  [A loss: 0.926120, acc: 0.105469]\n",
      "1164: [D loss: 0.687994, acc: 0.570312]  [A loss: 0.881298, acc: 0.140625]\n",
      "1165: [D loss: 0.678984, acc: 0.576172]  [A loss: 0.851660, acc: 0.199219]\n",
      "1166: [D loss: 0.681350, acc: 0.582031]  [A loss: 0.931748, acc: 0.121094]\n",
      "1167: [D loss: 0.684459, acc: 0.554688]  [A loss: 0.856574, acc: 0.195312]\n",
      "1168: [D loss: 0.686487, acc: 0.550781]  [A loss: 0.876119, acc: 0.179688]\n",
      "1169: [D loss: 0.686149, acc: 0.550781]  [A loss: 0.814153, acc: 0.312500]\n",
      "1170: [D loss: 0.679186, acc: 0.585938]  [A loss: 0.883240, acc: 0.175781]\n",
      "1171: [D loss: 0.665996, acc: 0.603516]  [A loss: 0.909389, acc: 0.128906]\n",
      "1172: [D loss: 0.679510, acc: 0.570312]  [A loss: 0.813651, acc: 0.265625]\n",
      "1173: [D loss: 0.678524, acc: 0.562500]  [A loss: 0.906275, acc: 0.175781]\n",
      "1174: [D loss: 0.674630, acc: 0.611328]  [A loss: 0.826370, acc: 0.277344]\n",
      "1175: [D loss: 0.671911, acc: 0.599609]  [A loss: 0.956596, acc: 0.121094]\n",
      "1176: [D loss: 0.666989, acc: 0.587891]  [A loss: 0.770234, acc: 0.339844]\n",
      "1177: [D loss: 0.669234, acc: 0.572266]  [A loss: 0.984246, acc: 0.097656]\n",
      "1178: [D loss: 0.674301, acc: 0.552734]  [A loss: 0.844252, acc: 0.250000]\n",
      "1179: [D loss: 0.679863, acc: 0.568359]  [A loss: 0.968480, acc: 0.128906]\n",
      "1180: [D loss: 0.662500, acc: 0.585938]  [A loss: 0.828669, acc: 0.261719]\n",
      "1181: [D loss: 0.679729, acc: 0.578125]  [A loss: 0.948660, acc: 0.121094]\n",
      "1182: [D loss: 0.673882, acc: 0.572266]  [A loss: 0.876304, acc: 0.218750]\n",
      "1183: [D loss: 0.679738, acc: 0.544922]  [A loss: 0.892533, acc: 0.214844]\n",
      "1184: [D loss: 0.688731, acc: 0.552734]  [A loss: 0.919465, acc: 0.128906]\n",
      "1185: [D loss: 0.674238, acc: 0.595703]  [A loss: 0.867796, acc: 0.199219]\n",
      "1186: [D loss: 0.678219, acc: 0.576172]  [A loss: 0.907184, acc: 0.160156]\n",
      "1187: [D loss: 0.693860, acc: 0.541016]  [A loss: 0.851620, acc: 0.230469]\n",
      "1188: [D loss: 0.680290, acc: 0.570312]  [A loss: 0.980049, acc: 0.128906]\n",
      "1189: [D loss: 0.657104, acc: 0.613281]  [A loss: 0.792663, acc: 0.332031]\n",
      "1190: [D loss: 0.693835, acc: 0.542969]  [A loss: 0.980699, acc: 0.074219]\n",
      "1191: [D loss: 0.673515, acc: 0.576172]  [A loss: 0.818428, acc: 0.296875]\n",
      "1192: [D loss: 0.711249, acc: 0.537109]  [A loss: 1.066285, acc: 0.046875]\n",
      "1193: [D loss: 0.670490, acc: 0.599609]  [A loss: 0.694664, acc: 0.515625]\n",
      "1194: [D loss: 0.706230, acc: 0.548828]  [A loss: 1.020420, acc: 0.062500]\n",
      "1195: [D loss: 0.680223, acc: 0.554688]  [A loss: 0.755185, acc: 0.410156]\n",
      "1196: [D loss: 0.696778, acc: 0.558594]  [A loss: 0.976994, acc: 0.089844]\n",
      "1197: [D loss: 0.668388, acc: 0.603516]  [A loss: 0.820413, acc: 0.250000]\n",
      "1198: [D loss: 0.675965, acc: 0.578125]  [A loss: 0.988600, acc: 0.097656]\n",
      "1199: [D loss: 0.685982, acc: 0.568359]  [A loss: 0.788946, acc: 0.328125]\n",
      "1200: [D loss: 0.685940, acc: 0.554688]  [A loss: 0.937109, acc: 0.144531]\n",
      "1201: [D loss: 0.664875, acc: 0.621094]  [A loss: 0.844900, acc: 0.234375]\n",
      "1202: [D loss: 0.676893, acc: 0.589844]  [A loss: 0.890576, acc: 0.171875]\n",
      "1203: [D loss: 0.664462, acc: 0.595703]  [A loss: 0.905025, acc: 0.183594]\n",
      "1204: [D loss: 0.691380, acc: 0.556641]  [A loss: 0.958743, acc: 0.109375]\n",
      "1205: [D loss: 0.677350, acc: 0.562500]  [A loss: 0.796820, acc: 0.289062]\n",
      "1206: [D loss: 0.674811, acc: 0.587891]  [A loss: 0.980848, acc: 0.070312]\n",
      "1207: [D loss: 0.681585, acc: 0.548828]  [A loss: 0.800500, acc: 0.265625]\n",
      "1208: [D loss: 0.684466, acc: 0.578125]  [A loss: 0.999274, acc: 0.093750]\n",
      "1209: [D loss: 0.675112, acc: 0.570312]  [A loss: 0.739759, acc: 0.441406]\n",
      "1210: [D loss: 0.691060, acc: 0.554688]  [A loss: 1.022578, acc: 0.070312]\n",
      "1211: [D loss: 0.667481, acc: 0.619141]  [A loss: 0.810541, acc: 0.296875]\n",
      "1212: [D loss: 0.676395, acc: 0.572266]  [A loss: 1.045825, acc: 0.054688]\n",
      "1213: [D loss: 0.681158, acc: 0.558594]  [A loss: 0.721382, acc: 0.488281]\n",
      "1214: [D loss: 0.696871, acc: 0.541016]  [A loss: 1.010400, acc: 0.085938]\n",
      "1215: [D loss: 0.685069, acc: 0.542969]  [A loss: 0.736007, acc: 0.449219]\n",
      "1216: [D loss: 0.693175, acc: 0.570312]  [A loss: 0.959920, acc: 0.085938]\n",
      "1217: [D loss: 0.673436, acc: 0.560547]  [A loss: 0.856402, acc: 0.207031]\n",
      "1218: [D loss: 0.683351, acc: 0.576172]  [A loss: 0.894559, acc: 0.156250]\n",
      "1219: [D loss: 0.668491, acc: 0.570312]  [A loss: 0.797002, acc: 0.308594]\n",
      "1220: [D loss: 0.686569, acc: 0.564453]  [A loss: 0.868029, acc: 0.218750]\n",
      "1221: [D loss: 0.679519, acc: 0.582031]  [A loss: 0.857641, acc: 0.230469]\n",
      "1222: [D loss: 0.674451, acc: 0.560547]  [A loss: 0.924767, acc: 0.140625]\n",
      "1223: [D loss: 0.672767, acc: 0.585938]  [A loss: 0.904897, acc: 0.160156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224: [D loss: 0.683184, acc: 0.539062]  [A loss: 0.885613, acc: 0.144531]\n",
      "1225: [D loss: 0.684008, acc: 0.560547]  [A loss: 0.879113, acc: 0.187500]\n",
      "1226: [D loss: 0.674342, acc: 0.583984]  [A loss: 0.846523, acc: 0.257812]\n",
      "1227: [D loss: 0.692954, acc: 0.560547]  [A loss: 0.884692, acc: 0.207031]\n",
      "1228: [D loss: 0.672267, acc: 0.597656]  [A loss: 0.926516, acc: 0.125000]\n",
      "1229: [D loss: 0.666722, acc: 0.591797]  [A loss: 0.895880, acc: 0.183594]\n",
      "1230: [D loss: 0.677584, acc: 0.580078]  [A loss: 0.824168, acc: 0.265625]\n",
      "1231: [D loss: 0.679388, acc: 0.591797]  [A loss: 0.995623, acc: 0.101562]\n",
      "1232: [D loss: 0.665599, acc: 0.591797]  [A loss: 0.780409, acc: 0.343750]\n",
      "1233: [D loss: 0.686266, acc: 0.558594]  [A loss: 1.014348, acc: 0.093750]\n",
      "1234: [D loss: 0.688867, acc: 0.558594]  [A loss: 0.771578, acc: 0.367188]\n",
      "1235: [D loss: 0.683300, acc: 0.560547]  [A loss: 1.063742, acc: 0.062500]\n",
      "1236: [D loss: 0.677868, acc: 0.560547]  [A loss: 0.773140, acc: 0.378906]\n",
      "1237: [D loss: 0.693779, acc: 0.542969]  [A loss: 1.042615, acc: 0.070312]\n",
      "1238: [D loss: 0.683836, acc: 0.552734]  [A loss: 0.736522, acc: 0.410156]\n",
      "1239: [D loss: 0.690208, acc: 0.537109]  [A loss: 1.055435, acc: 0.046875]\n",
      "1240: [D loss: 0.671479, acc: 0.583984]  [A loss: 0.783033, acc: 0.316406]\n",
      "1241: [D loss: 0.694658, acc: 0.548828]  [A loss: 0.990119, acc: 0.097656]\n",
      "1242: [D loss: 0.690304, acc: 0.556641]  [A loss: 0.770872, acc: 0.355469]\n",
      "1243: [D loss: 0.709983, acc: 0.531250]  [A loss: 0.949554, acc: 0.121094]\n",
      "1244: [D loss: 0.675850, acc: 0.578125]  [A loss: 0.761875, acc: 0.398438]\n",
      "1245: [D loss: 0.693762, acc: 0.542969]  [A loss: 0.940689, acc: 0.164062]\n",
      "1246: [D loss: 0.684038, acc: 0.552734]  [A loss: 0.809815, acc: 0.285156]\n",
      "1247: [D loss: 0.673991, acc: 0.576172]  [A loss: 0.833328, acc: 0.261719]\n",
      "1248: [D loss: 0.691288, acc: 0.578125]  [A loss: 0.918381, acc: 0.160156]\n",
      "1249: [D loss: 0.695153, acc: 0.558594]  [A loss: 0.791135, acc: 0.343750]\n",
      "1250: [D loss: 0.683078, acc: 0.542969]  [A loss: 0.889074, acc: 0.164062]\n",
      "1251: [D loss: 0.687467, acc: 0.558594]  [A loss: 0.789257, acc: 0.339844]\n",
      "1252: [D loss: 0.671490, acc: 0.585938]  [A loss: 0.949524, acc: 0.121094]\n",
      "1253: [D loss: 0.674433, acc: 0.548828]  [A loss: 0.863990, acc: 0.214844]\n",
      "1254: [D loss: 0.679855, acc: 0.558594]  [A loss: 0.947111, acc: 0.113281]\n",
      "1255: [D loss: 0.661127, acc: 0.591797]  [A loss: 0.822115, acc: 0.273438]\n",
      "1256: [D loss: 0.683651, acc: 0.568359]  [A loss: 0.939553, acc: 0.164062]\n",
      "1257: [D loss: 0.661970, acc: 0.613281]  [A loss: 0.947122, acc: 0.128906]\n",
      "1258: [D loss: 0.671516, acc: 0.566406]  [A loss: 0.891952, acc: 0.195312]\n",
      "1259: [D loss: 0.681215, acc: 0.578125]  [A loss: 0.900754, acc: 0.152344]\n",
      "1260: [D loss: 0.679037, acc: 0.564453]  [A loss: 0.836273, acc: 0.257812]\n",
      "1261: [D loss: 0.677127, acc: 0.568359]  [A loss: 0.891458, acc: 0.136719]\n",
      "1262: [D loss: 0.669855, acc: 0.589844]  [A loss: 0.896864, acc: 0.218750]\n",
      "1263: [D loss: 0.673326, acc: 0.564453]  [A loss: 0.842849, acc: 0.238281]\n",
      "1264: [D loss: 0.670285, acc: 0.601562]  [A loss: 0.985297, acc: 0.101562]\n",
      "1265: [D loss: 0.670399, acc: 0.556641]  [A loss: 0.787172, acc: 0.378906]\n",
      "1266: [D loss: 0.721982, acc: 0.513672]  [A loss: 1.144117, acc: 0.007812]\n",
      "1267: [D loss: 0.686638, acc: 0.574219]  [A loss: 0.690906, acc: 0.511719]\n",
      "1268: [D loss: 0.705100, acc: 0.527344]  [A loss: 1.036655, acc: 0.074219]\n",
      "1269: [D loss: 0.670134, acc: 0.601562]  [A loss: 0.772479, acc: 0.324219]\n",
      "1270: [D loss: 0.692474, acc: 0.544922]  [A loss: 0.928281, acc: 0.148438]\n",
      "1271: [D loss: 0.683083, acc: 0.556641]  [A loss: 0.865475, acc: 0.230469]\n",
      "1272: [D loss: 0.688583, acc: 0.548828]  [A loss: 0.844198, acc: 0.246094]\n",
      "1273: [D loss: 0.680614, acc: 0.562500]  [A loss: 0.899910, acc: 0.179688]\n",
      "1274: [D loss: 0.683860, acc: 0.564453]  [A loss: 0.864390, acc: 0.222656]\n",
      "1275: [D loss: 0.687028, acc: 0.556641]  [A loss: 0.828405, acc: 0.250000]\n",
      "1276: [D loss: 0.672966, acc: 0.595703]  [A loss: 0.853679, acc: 0.242188]\n",
      "1277: [D loss: 0.675780, acc: 0.544922]  [A loss: 0.936137, acc: 0.156250]\n",
      "1278: [D loss: 0.676218, acc: 0.570312]  [A loss: 0.838009, acc: 0.246094]\n",
      "1279: [D loss: 0.683015, acc: 0.580078]  [A loss: 0.830446, acc: 0.238281]\n",
      "1280: [D loss: 0.713144, acc: 0.535156]  [A loss: 0.946561, acc: 0.113281]\n",
      "1281: [D loss: 0.708642, acc: 0.507812]  [A loss: 0.863568, acc: 0.175781]\n",
      "1282: [D loss: 0.696960, acc: 0.529297]  [A loss: 0.910687, acc: 0.128906]\n",
      "1283: [D loss: 0.692203, acc: 0.492188]  [A loss: 0.788103, acc: 0.332031]\n",
      "1284: [D loss: 0.697555, acc: 0.541016]  [A loss: 0.991472, acc: 0.082031]\n",
      "1285: [D loss: 0.667532, acc: 0.576172]  [A loss: 0.836572, acc: 0.261719]\n",
      "1286: [D loss: 0.695774, acc: 0.519531]  [A loss: 0.934122, acc: 0.128906]\n",
      "1287: [D loss: 0.678364, acc: 0.568359]  [A loss: 0.788048, acc: 0.324219]\n",
      "1288: [D loss: 0.685780, acc: 0.562500]  [A loss: 0.900310, acc: 0.148438]\n",
      "1289: [D loss: 0.676191, acc: 0.570312]  [A loss: 0.837394, acc: 0.242188]\n",
      "1290: [D loss: 0.694039, acc: 0.556641]  [A loss: 0.911187, acc: 0.164062]\n",
      "1291: [D loss: 0.697235, acc: 0.529297]  [A loss: 0.848576, acc: 0.230469]\n",
      "1292: [D loss: 0.679052, acc: 0.568359]  [A loss: 0.928993, acc: 0.167969]\n",
      "1293: [D loss: 0.692157, acc: 0.544922]  [A loss: 0.754260, acc: 0.386719]\n",
      "1294: [D loss: 0.685451, acc: 0.562500]  [A loss: 0.975162, acc: 0.128906]\n",
      "1295: [D loss: 0.677868, acc: 0.568359]  [A loss: 0.752483, acc: 0.382812]\n",
      "1296: [D loss: 0.719189, acc: 0.533203]  [A loss: 1.039877, acc: 0.066406]\n",
      "1297: [D loss: 0.686041, acc: 0.541016]  [A loss: 0.773212, acc: 0.363281]\n",
      "1298: [D loss: 0.697186, acc: 0.544922]  [A loss: 0.985333, acc: 0.062500]\n",
      "1299: [D loss: 0.677236, acc: 0.558594]  [A loss: 0.777671, acc: 0.320312]\n",
      "1300: [D loss: 0.684284, acc: 0.576172]  [A loss: 0.956446, acc: 0.105469]\n",
      "1301: [D loss: 0.694672, acc: 0.570312]  [A loss: 0.796246, acc: 0.320312]\n",
      "1302: [D loss: 0.695248, acc: 0.537109]  [A loss: 0.915752, acc: 0.144531]\n",
      "1303: [D loss: 0.680434, acc: 0.556641]  [A loss: 0.780229, acc: 0.335938]\n",
      "1304: [D loss: 0.698153, acc: 0.544922]  [A loss: 0.954384, acc: 0.105469]\n",
      "1305: [D loss: 0.693447, acc: 0.519531]  [A loss: 0.798803, acc: 0.289062]\n",
      "1306: [D loss: 0.681869, acc: 0.578125]  [A loss: 0.960024, acc: 0.125000]\n",
      "1307: [D loss: 0.676813, acc: 0.570312]  [A loss: 0.819282, acc: 0.277344]\n",
      "1308: [D loss: 0.683936, acc: 0.558594]  [A loss: 0.946142, acc: 0.089844]\n",
      "1309: [D loss: 0.669296, acc: 0.603516]  [A loss: 0.760090, acc: 0.410156]\n",
      "1310: [D loss: 0.686466, acc: 0.539062]  [A loss: 1.027416, acc: 0.066406]\n",
      "1311: [D loss: 0.673107, acc: 0.554688]  [A loss: 0.758178, acc: 0.402344]\n",
      "1312: [D loss: 0.689973, acc: 0.542969]  [A loss: 0.950869, acc: 0.121094]\n",
      "1313: [D loss: 0.677168, acc: 0.570312]  [A loss: 0.764542, acc: 0.328125]\n",
      "1314: [D loss: 0.679557, acc: 0.580078]  [A loss: 0.916180, acc: 0.148438]\n",
      "1315: [D loss: 0.690174, acc: 0.556641]  [A loss: 0.860699, acc: 0.187500]\n",
      "1316: [D loss: 0.686087, acc: 0.527344]  [A loss: 0.896395, acc: 0.175781]\n",
      "1317: [D loss: 0.666888, acc: 0.601562]  [A loss: 0.837658, acc: 0.269531]\n",
      "1318: [D loss: 0.673582, acc: 0.582031]  [A loss: 0.851086, acc: 0.257812]\n",
      "1319: [D loss: 0.686758, acc: 0.529297]  [A loss: 0.934376, acc: 0.144531]\n",
      "1320: [D loss: 0.672139, acc: 0.572266]  [A loss: 0.887996, acc: 0.152344]\n",
      "1321: [D loss: 0.678371, acc: 0.607422]  [A loss: 0.918731, acc: 0.148438]\n",
      "1322: [D loss: 0.684673, acc: 0.541016]  [A loss: 0.918784, acc: 0.132812]\n",
      "1323: [D loss: 0.668560, acc: 0.593750]  [A loss: 0.796220, acc: 0.304688]\n",
      "1324: [D loss: 0.686272, acc: 0.558594]  [A loss: 0.947228, acc: 0.093750]\n",
      "1325: [D loss: 0.685058, acc: 0.560547]  [A loss: 0.852665, acc: 0.226562]\n",
      "1326: [D loss: 0.696431, acc: 0.513672]  [A loss: 1.009050, acc: 0.082031]\n",
      "1327: [D loss: 0.667458, acc: 0.597656]  [A loss: 0.739931, acc: 0.433594]\n",
      "1328: [D loss: 0.705583, acc: 0.537109]  [A loss: 1.046254, acc: 0.042969]\n",
      "1329: [D loss: 0.679855, acc: 0.541016]  [A loss: 0.688846, acc: 0.511719]\n",
      "1330: [D loss: 0.727788, acc: 0.529297]  [A loss: 1.054672, acc: 0.035156]\n",
      "1331: [D loss: 0.675024, acc: 0.587891]  [A loss: 0.698468, acc: 0.507812]\n",
      "1332: [D loss: 0.697977, acc: 0.554688]  [A loss: 0.954713, acc: 0.128906]\n",
      "1333: [D loss: 0.675299, acc: 0.597656]  [A loss: 0.764900, acc: 0.339844]\n",
      "1334: [D loss: 0.701121, acc: 0.537109]  [A loss: 0.892255, acc: 0.160156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1335: [D loss: 0.670589, acc: 0.613281]  [A loss: 0.815419, acc: 0.300781]\n",
      "1336: [D loss: 0.677126, acc: 0.583984]  [A loss: 0.906730, acc: 0.171875]\n",
      "1337: [D loss: 0.679404, acc: 0.570312]  [A loss: 0.828450, acc: 0.269531]\n",
      "1338: [D loss: 0.684878, acc: 0.578125]  [A loss: 0.910521, acc: 0.199219]\n",
      "1339: [D loss: 0.671132, acc: 0.568359]  [A loss: 0.880848, acc: 0.179688]\n",
      "1340: [D loss: 0.685046, acc: 0.556641]  [A loss: 0.913218, acc: 0.152344]\n",
      "1341: [D loss: 0.679125, acc: 0.574219]  [A loss: 0.898885, acc: 0.156250]\n",
      "1342: [D loss: 0.674678, acc: 0.583984]  [A loss: 0.945899, acc: 0.117188]\n",
      "1343: [D loss: 0.682929, acc: 0.576172]  [A loss: 0.766496, acc: 0.363281]\n",
      "1344: [D loss: 0.704774, acc: 0.552734]  [A loss: 0.993266, acc: 0.089844]\n",
      "1345: [D loss: 0.680700, acc: 0.611328]  [A loss: 0.744298, acc: 0.394531]\n",
      "1346: [D loss: 0.694825, acc: 0.539062]  [A loss: 1.034510, acc: 0.058594]\n",
      "1347: [D loss: 0.686143, acc: 0.546875]  [A loss: 0.692173, acc: 0.542969]\n",
      "1348: [D loss: 0.704148, acc: 0.542969]  [A loss: 0.992423, acc: 0.085938]\n",
      "1349: [D loss: 0.678644, acc: 0.587891]  [A loss: 0.746776, acc: 0.378906]\n",
      "1350: [D loss: 0.695045, acc: 0.556641]  [A loss: 0.944068, acc: 0.105469]\n",
      "1351: [D loss: 0.671426, acc: 0.580078]  [A loss: 0.788573, acc: 0.343750]\n",
      "1352: [D loss: 0.688636, acc: 0.572266]  [A loss: 0.922136, acc: 0.109375]\n",
      "1353: [D loss: 0.664687, acc: 0.591797]  [A loss: 0.802347, acc: 0.285156]\n",
      "1354: [D loss: 0.687622, acc: 0.548828]  [A loss: 0.913909, acc: 0.167969]\n",
      "1355: [D loss: 0.678009, acc: 0.556641]  [A loss: 0.825581, acc: 0.253906]\n",
      "1356: [D loss: 0.691071, acc: 0.560547]  [A loss: 0.949838, acc: 0.117188]\n",
      "1357: [D loss: 0.664551, acc: 0.582031]  [A loss: 0.802551, acc: 0.292969]\n",
      "1358: [D loss: 0.677549, acc: 0.593750]  [A loss: 0.922979, acc: 0.140625]\n",
      "1359: [D loss: 0.695747, acc: 0.539062]  [A loss: 0.885775, acc: 0.195312]\n",
      "1360: [D loss: 0.675025, acc: 0.597656]  [A loss: 0.947886, acc: 0.117188]\n",
      "1361: [D loss: 0.669780, acc: 0.560547]  [A loss: 0.818477, acc: 0.269531]\n",
      "1362: [D loss: 0.678560, acc: 0.597656]  [A loss: 0.973617, acc: 0.117188]\n",
      "1363: [D loss: 0.692647, acc: 0.574219]  [A loss: 0.715624, acc: 0.472656]\n",
      "1364: [D loss: 0.706350, acc: 0.533203]  [A loss: 1.023942, acc: 0.062500]\n",
      "1365: [D loss: 0.681714, acc: 0.548828]  [A loss: 0.736022, acc: 0.457031]\n",
      "1366: [D loss: 0.723327, acc: 0.494141]  [A loss: 1.014995, acc: 0.050781]\n",
      "1367: [D loss: 0.691315, acc: 0.558594]  [A loss: 0.772464, acc: 0.351562]\n",
      "1368: [D loss: 0.698934, acc: 0.539062]  [A loss: 0.911132, acc: 0.128906]\n",
      "1369: [D loss: 0.673643, acc: 0.595703]  [A loss: 0.804657, acc: 0.308594]\n",
      "1370: [D loss: 0.672003, acc: 0.585938]  [A loss: 0.882441, acc: 0.191406]\n",
      "1371: [D loss: 0.688929, acc: 0.554688]  [A loss: 0.867549, acc: 0.242188]\n",
      "1372: [D loss: 0.687260, acc: 0.544922]  [A loss: 0.870744, acc: 0.179688]\n",
      "1373: [D loss: 0.687674, acc: 0.533203]  [A loss: 0.821750, acc: 0.320312]\n",
      "1374: [D loss: 0.688012, acc: 0.537109]  [A loss: 0.907467, acc: 0.148438]\n",
      "1375: [D loss: 0.688333, acc: 0.537109]  [A loss: 0.783375, acc: 0.367188]\n",
      "1376: [D loss: 0.693709, acc: 0.533203]  [A loss: 1.001840, acc: 0.105469]\n",
      "1377: [D loss: 0.682960, acc: 0.560547]  [A loss: 0.729730, acc: 0.464844]\n",
      "1378: [D loss: 0.702468, acc: 0.537109]  [A loss: 0.989675, acc: 0.062500]\n",
      "1379: [D loss: 0.668143, acc: 0.595703]  [A loss: 0.771514, acc: 0.324219]\n",
      "1380: [D loss: 0.688403, acc: 0.556641]  [A loss: 0.979552, acc: 0.093750]\n",
      "1381: [D loss: 0.690137, acc: 0.552734]  [A loss: 0.759978, acc: 0.382812]\n",
      "1382: [D loss: 0.690570, acc: 0.548828]  [A loss: 0.937175, acc: 0.117188]\n",
      "1383: [D loss: 0.680268, acc: 0.574219]  [A loss: 0.832730, acc: 0.207031]\n",
      "1384: [D loss: 0.694703, acc: 0.576172]  [A loss: 0.960258, acc: 0.105469]\n",
      "1385: [D loss: 0.690308, acc: 0.562500]  [A loss: 0.837687, acc: 0.261719]\n",
      "1386: [D loss: 0.678744, acc: 0.580078]  [A loss: 0.937989, acc: 0.148438]\n",
      "1387: [D loss: 0.672303, acc: 0.582031]  [A loss: 0.778068, acc: 0.335938]\n",
      "1388: [D loss: 0.692287, acc: 0.513672]  [A loss: 0.878842, acc: 0.199219]\n",
      "1389: [D loss: 0.671652, acc: 0.595703]  [A loss: 0.856219, acc: 0.218750]\n",
      "1390: [D loss: 0.693848, acc: 0.556641]  [A loss: 0.964483, acc: 0.121094]\n",
      "1391: [D loss: 0.667241, acc: 0.587891]  [A loss: 0.785435, acc: 0.355469]\n",
      "1392: [D loss: 0.696869, acc: 0.544922]  [A loss: 0.978510, acc: 0.082031]\n",
      "1393: [D loss: 0.688605, acc: 0.527344]  [A loss: 0.823170, acc: 0.320312]\n",
      "1394: [D loss: 0.692303, acc: 0.531250]  [A loss: 0.989531, acc: 0.097656]\n",
      "1395: [D loss: 0.685085, acc: 0.548828]  [A loss: 0.765084, acc: 0.390625]\n",
      "1396: [D loss: 0.702202, acc: 0.541016]  [A loss: 0.952892, acc: 0.140625]\n",
      "1397: [D loss: 0.680802, acc: 0.541016]  [A loss: 0.757227, acc: 0.394531]\n",
      "1398: [D loss: 0.687651, acc: 0.568359]  [A loss: 1.005724, acc: 0.101562]\n",
      "1399: [D loss: 0.674407, acc: 0.585938]  [A loss: 0.732062, acc: 0.464844]\n",
      "1400: [D loss: 0.684375, acc: 0.566406]  [A loss: 1.007531, acc: 0.097656]\n",
      "1401: [D loss: 0.676191, acc: 0.572266]  [A loss: 0.842479, acc: 0.234375]\n",
      "1402: [D loss: 0.701935, acc: 0.552734]  [A loss: 0.948955, acc: 0.128906]\n",
      "1403: [D loss: 0.676085, acc: 0.548828]  [A loss: 0.809079, acc: 0.277344]\n",
      "1404: [D loss: 0.672263, acc: 0.583984]  [A loss: 0.897629, acc: 0.187500]\n",
      "1405: [D loss: 0.711053, acc: 0.507812]  [A loss: 0.901916, acc: 0.128906]\n",
      "1406: [D loss: 0.688111, acc: 0.546875]  [A loss: 0.863518, acc: 0.191406]\n",
      "1407: [D loss: 0.678695, acc: 0.568359]  [A loss: 0.895438, acc: 0.164062]\n",
      "1408: [D loss: 0.688633, acc: 0.564453]  [A loss: 0.834144, acc: 0.253906]\n",
      "1409: [D loss: 0.677315, acc: 0.558594]  [A loss: 0.839010, acc: 0.257812]\n",
      "1410: [D loss: 0.697173, acc: 0.539062]  [A loss: 0.938379, acc: 0.105469]\n",
      "1411: [D loss: 0.690243, acc: 0.552734]  [A loss: 0.845333, acc: 0.226562]\n",
      "1412: [D loss: 0.694467, acc: 0.537109]  [A loss: 1.026659, acc: 0.058594]\n",
      "1413: [D loss: 0.682257, acc: 0.554688]  [A loss: 0.768294, acc: 0.343750]\n",
      "1414: [D loss: 0.684736, acc: 0.562500]  [A loss: 0.996249, acc: 0.089844]\n",
      "1415: [D loss: 0.670928, acc: 0.585938]  [A loss: 0.729729, acc: 0.433594]\n",
      "1416: [D loss: 0.690355, acc: 0.533203]  [A loss: 1.013057, acc: 0.078125]\n",
      "1417: [D loss: 0.673423, acc: 0.556641]  [A loss: 0.769139, acc: 0.410156]\n",
      "1418: [D loss: 0.688375, acc: 0.556641]  [A loss: 0.935430, acc: 0.121094]\n",
      "1419: [D loss: 0.683049, acc: 0.570312]  [A loss: 0.805852, acc: 0.285156]\n",
      "1420: [D loss: 0.674801, acc: 0.572266]  [A loss: 0.873980, acc: 0.214844]\n",
      "1421: [D loss: 0.685697, acc: 0.537109]  [A loss: 0.853250, acc: 0.207031]\n",
      "1422: [D loss: 0.695253, acc: 0.533203]  [A loss: 0.914892, acc: 0.148438]\n",
      "1423: [D loss: 0.684450, acc: 0.564453]  [A loss: 0.806341, acc: 0.324219]\n",
      "1424: [D loss: 0.686864, acc: 0.566406]  [A loss: 0.941897, acc: 0.117188]\n",
      "1425: [D loss: 0.671931, acc: 0.597656]  [A loss: 0.847700, acc: 0.210938]\n",
      "1426: [D loss: 0.690588, acc: 0.531250]  [A loss: 0.906531, acc: 0.140625]\n",
      "1427: [D loss: 0.681192, acc: 0.556641]  [A loss: 0.847232, acc: 0.226562]\n",
      "1428: [D loss: 0.686022, acc: 0.542969]  [A loss: 0.951221, acc: 0.128906]\n",
      "1429: [D loss: 0.691581, acc: 0.517578]  [A loss: 0.878021, acc: 0.175781]\n",
      "1430: [D loss: 0.674125, acc: 0.570312]  [A loss: 0.937188, acc: 0.132812]\n",
      "1431: [D loss: 0.677060, acc: 0.566406]  [A loss: 0.912041, acc: 0.195312]\n",
      "1432: [D loss: 0.697459, acc: 0.527344]  [A loss: 0.901108, acc: 0.136719]\n",
      "1433: [D loss: 0.676063, acc: 0.568359]  [A loss: 0.860064, acc: 0.222656]\n",
      "1434: [D loss: 0.687036, acc: 0.556641]  [A loss: 0.972868, acc: 0.093750]\n",
      "1435: [D loss: 0.677336, acc: 0.572266]  [A loss: 0.791516, acc: 0.312500]\n",
      "1436: [D loss: 0.683323, acc: 0.558594]  [A loss: 0.998248, acc: 0.089844]\n",
      "1437: [D loss: 0.670910, acc: 0.585938]  [A loss: 0.760095, acc: 0.402344]\n",
      "1438: [D loss: 0.718798, acc: 0.515625]  [A loss: 1.100775, acc: 0.042969]\n",
      "1439: [D loss: 0.689265, acc: 0.552734]  [A loss: 0.726017, acc: 0.476562]\n",
      "1440: [D loss: 0.691303, acc: 0.550781]  [A loss: 1.006723, acc: 0.062500]\n",
      "1441: [D loss: 0.681986, acc: 0.560547]  [A loss: 0.713338, acc: 0.488281]\n",
      "1442: [D loss: 0.701060, acc: 0.537109]  [A loss: 0.980521, acc: 0.109375]\n",
      "1443: [D loss: 0.700018, acc: 0.517578]  [A loss: 0.809804, acc: 0.292969]\n",
      "1444: [D loss: 0.680705, acc: 0.574219]  [A loss: 0.883173, acc: 0.160156]\n",
      "1445: [D loss: 0.683007, acc: 0.562500]  [A loss: 0.817063, acc: 0.261719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446: [D loss: 0.677430, acc: 0.576172]  [A loss: 0.879954, acc: 0.179688]\n",
      "1447: [D loss: 0.694714, acc: 0.523438]  [A loss: 0.843464, acc: 0.230469]\n",
      "1448: [D loss: 0.689571, acc: 0.537109]  [A loss: 0.925799, acc: 0.144531]\n",
      "1449: [D loss: 0.682817, acc: 0.541016]  [A loss: 0.854862, acc: 0.253906]\n",
      "1450: [D loss: 0.697332, acc: 0.537109]  [A loss: 0.916327, acc: 0.121094]\n",
      "1451: [D loss: 0.674875, acc: 0.595703]  [A loss: 0.816156, acc: 0.289062]\n",
      "1452: [D loss: 0.686505, acc: 0.576172]  [A loss: 0.893652, acc: 0.167969]\n",
      "1453: [D loss: 0.679141, acc: 0.576172]  [A loss: 0.858643, acc: 0.214844]\n",
      "1454: [D loss: 0.676621, acc: 0.556641]  [A loss: 0.872283, acc: 0.195312]\n",
      "1455: [D loss: 0.668277, acc: 0.572266]  [A loss: 0.804056, acc: 0.308594]\n",
      "1456: [D loss: 0.703430, acc: 0.560547]  [A loss: 0.941838, acc: 0.128906]\n",
      "1457: [D loss: 0.682525, acc: 0.542969]  [A loss: 0.748304, acc: 0.414062]\n",
      "1458: [D loss: 0.686142, acc: 0.533203]  [A loss: 1.001927, acc: 0.066406]\n",
      "1459: [D loss: 0.683476, acc: 0.554688]  [A loss: 0.789187, acc: 0.363281]\n",
      "1460: [D loss: 0.691515, acc: 0.558594]  [A loss: 0.921167, acc: 0.179688]\n",
      "1461: [D loss: 0.676717, acc: 0.578125]  [A loss: 0.823438, acc: 0.222656]\n",
      "1462: [D loss: 0.696900, acc: 0.546875]  [A loss: 0.897693, acc: 0.160156]\n",
      "1463: [D loss: 0.677848, acc: 0.572266]  [A loss: 0.849943, acc: 0.218750]\n",
      "1464: [D loss: 0.697160, acc: 0.550781]  [A loss: 0.914911, acc: 0.132812]\n",
      "1465: [D loss: 0.679684, acc: 0.576172]  [A loss: 0.782672, acc: 0.343750]\n",
      "1466: [D loss: 0.692648, acc: 0.558594]  [A loss: 1.032941, acc: 0.054688]\n",
      "1467: [D loss: 0.685755, acc: 0.556641]  [A loss: 0.758218, acc: 0.351562]\n",
      "1468: [D loss: 0.689541, acc: 0.548828]  [A loss: 0.956000, acc: 0.093750]\n",
      "1469: [D loss: 0.672966, acc: 0.574219]  [A loss: 0.800740, acc: 0.296875]\n",
      "1470: [D loss: 0.693951, acc: 0.542969]  [A loss: 0.891260, acc: 0.179688]\n",
      "1471: [D loss: 0.678984, acc: 0.582031]  [A loss: 0.804543, acc: 0.281250]\n",
      "1472: [D loss: 0.700929, acc: 0.531250]  [A loss: 1.004433, acc: 0.089844]\n",
      "1473: [D loss: 0.674194, acc: 0.583984]  [A loss: 0.726304, acc: 0.421875]\n",
      "1474: [D loss: 0.709660, acc: 0.525391]  [A loss: 1.082244, acc: 0.035156]\n",
      "1475: [D loss: 0.679511, acc: 0.582031]  [A loss: 0.709485, acc: 0.453125]\n",
      "1476: [D loss: 0.689714, acc: 0.556641]  [A loss: 1.008463, acc: 0.101562]\n",
      "1477: [D loss: 0.677706, acc: 0.578125]  [A loss: 0.793241, acc: 0.308594]\n",
      "1478: [D loss: 0.690775, acc: 0.566406]  [A loss: 0.936240, acc: 0.136719]\n",
      "1479: [D loss: 0.679660, acc: 0.595703]  [A loss: 0.812533, acc: 0.285156]\n",
      "1480: [D loss: 0.697375, acc: 0.537109]  [A loss: 0.974023, acc: 0.117188]\n",
      "1481: [D loss: 0.689511, acc: 0.542969]  [A loss: 0.770140, acc: 0.367188]\n",
      "1482: [D loss: 0.690043, acc: 0.548828]  [A loss: 0.941347, acc: 0.128906]\n",
      "1483: [D loss: 0.667897, acc: 0.595703]  [A loss: 0.809194, acc: 0.292969]\n",
      "1484: [D loss: 0.687064, acc: 0.570312]  [A loss: 0.994036, acc: 0.101562]\n",
      "1485: [D loss: 0.678732, acc: 0.562500]  [A loss: 0.731703, acc: 0.402344]\n",
      "1486: [D loss: 0.713214, acc: 0.531250]  [A loss: 1.017520, acc: 0.031250]\n",
      "1487: [D loss: 0.687392, acc: 0.556641]  [A loss: 0.711483, acc: 0.527344]\n",
      "1488: [D loss: 0.702525, acc: 0.539062]  [A loss: 1.022030, acc: 0.058594]\n",
      "1489: [D loss: 0.688026, acc: 0.556641]  [A loss: 0.755630, acc: 0.421875]\n",
      "1490: [D loss: 0.695748, acc: 0.529297]  [A loss: 0.917306, acc: 0.121094]\n",
      "1491: [D loss: 0.675083, acc: 0.562500]  [A loss: 0.862020, acc: 0.226562]\n",
      "1492: [D loss: 0.678871, acc: 0.542969]  [A loss: 0.941912, acc: 0.164062]\n",
      "1493: [D loss: 0.676818, acc: 0.582031]  [A loss: 0.804217, acc: 0.285156]\n",
      "1494: [D loss: 0.689876, acc: 0.552734]  [A loss: 0.954367, acc: 0.125000]\n",
      "1495: [D loss: 0.667359, acc: 0.599609]  [A loss: 0.786325, acc: 0.339844]\n",
      "1496: [D loss: 0.691046, acc: 0.558594]  [A loss: 0.907232, acc: 0.140625]\n",
      "1497: [D loss: 0.680145, acc: 0.576172]  [A loss: 0.864521, acc: 0.214844]\n",
      "1498: [D loss: 0.684334, acc: 0.603516]  [A loss: 0.877903, acc: 0.199219]\n",
      "1499: [D loss: 0.688753, acc: 0.542969]  [A loss: 0.877942, acc: 0.148438]\n",
      "1500: [D loss: 0.695109, acc: 0.529297]  [A loss: 0.855028, acc: 0.199219]\n",
      "1501: [D loss: 0.692591, acc: 0.544922]  [A loss: 0.865147, acc: 0.214844]\n",
      "1502: [D loss: 0.682326, acc: 0.574219]  [A loss: 0.921939, acc: 0.144531]\n",
      "1503: [D loss: 0.680958, acc: 0.582031]  [A loss: 0.870928, acc: 0.179688]\n",
      "1504: [D loss: 0.689468, acc: 0.585938]  [A loss: 0.945519, acc: 0.105469]\n",
      "1505: [D loss: 0.688434, acc: 0.539062]  [A loss: 0.815453, acc: 0.277344]\n",
      "1506: [D loss: 0.684757, acc: 0.578125]  [A loss: 0.973232, acc: 0.089844]\n",
      "1507: [D loss: 0.670176, acc: 0.572266]  [A loss: 0.771054, acc: 0.394531]\n",
      "1508: [D loss: 0.692940, acc: 0.542969]  [A loss: 1.020291, acc: 0.070312]\n",
      "1509: [D loss: 0.684320, acc: 0.564453]  [A loss: 0.749325, acc: 0.402344]\n",
      "1510: [D loss: 0.690938, acc: 0.546875]  [A loss: 1.014101, acc: 0.105469]\n",
      "1511: [D loss: 0.667965, acc: 0.570312]  [A loss: 0.754090, acc: 0.429688]\n",
      "1512: [D loss: 0.690934, acc: 0.539062]  [A loss: 0.973225, acc: 0.121094]\n",
      "1513: [D loss: 0.685417, acc: 0.544922]  [A loss: 0.740192, acc: 0.429688]\n",
      "1514: [D loss: 0.691366, acc: 0.566406]  [A loss: 0.966901, acc: 0.105469]\n",
      "1515: [D loss: 0.687023, acc: 0.562500]  [A loss: 0.710029, acc: 0.480469]\n",
      "1516: [D loss: 0.699125, acc: 0.533203]  [A loss: 1.016880, acc: 0.082031]\n",
      "1517: [D loss: 0.685256, acc: 0.568359]  [A loss: 0.719491, acc: 0.468750]\n",
      "1518: [D loss: 0.702998, acc: 0.542969]  [A loss: 0.915918, acc: 0.175781]\n",
      "1519: [D loss: 0.683910, acc: 0.572266]  [A loss: 0.831005, acc: 0.289062]\n",
      "1520: [D loss: 0.695284, acc: 0.533203]  [A loss: 0.855580, acc: 0.242188]\n",
      "1521: [D loss: 0.690266, acc: 0.548828]  [A loss: 0.824163, acc: 0.261719]\n",
      "1522: [D loss: 0.683552, acc: 0.527344]  [A loss: 0.905853, acc: 0.167969]\n",
      "1523: [D loss: 0.702860, acc: 0.511719]  [A loss: 0.811005, acc: 0.269531]\n",
      "1524: [D loss: 0.697455, acc: 0.535156]  [A loss: 0.963249, acc: 0.097656]\n",
      "1525: [D loss: 0.676547, acc: 0.568359]  [A loss: 0.793421, acc: 0.316406]\n",
      "1526: [D loss: 0.708721, acc: 0.521484]  [A loss: 0.868657, acc: 0.195312]\n",
      "1527: [D loss: 0.681326, acc: 0.568359]  [A loss: 0.797941, acc: 0.308594]\n",
      "1528: [D loss: 0.697397, acc: 0.535156]  [A loss: 0.946317, acc: 0.121094]\n",
      "1529: [D loss: 0.689971, acc: 0.558594]  [A loss: 0.846025, acc: 0.238281]\n",
      "1530: [D loss: 0.688807, acc: 0.566406]  [A loss: 0.905938, acc: 0.140625]\n",
      "1531: [D loss: 0.674965, acc: 0.556641]  [A loss: 0.772482, acc: 0.367188]\n",
      "1532: [D loss: 0.682886, acc: 0.564453]  [A loss: 0.924522, acc: 0.152344]\n",
      "1533: [D loss: 0.682801, acc: 0.531250]  [A loss: 0.768035, acc: 0.410156]\n",
      "1534: [D loss: 0.710692, acc: 0.511719]  [A loss: 0.997003, acc: 0.117188]\n",
      "1535: [D loss: 0.678095, acc: 0.574219]  [A loss: 0.728965, acc: 0.441406]\n",
      "1536: [D loss: 0.716100, acc: 0.523438]  [A loss: 1.028310, acc: 0.046875]\n",
      "1537: [D loss: 0.667001, acc: 0.574219]  [A loss: 0.686013, acc: 0.535156]\n",
      "1538: [D loss: 0.698164, acc: 0.535156]  [A loss: 1.003150, acc: 0.070312]\n",
      "1539: [D loss: 0.690404, acc: 0.552734]  [A loss: 0.747908, acc: 0.406250]\n",
      "1540: [D loss: 0.708779, acc: 0.542969]  [A loss: 0.954361, acc: 0.093750]\n",
      "1541: [D loss: 0.674056, acc: 0.542969]  [A loss: 0.758750, acc: 0.394531]\n",
      "1542: [D loss: 0.691395, acc: 0.554688]  [A loss: 0.911102, acc: 0.171875]\n",
      "1543: [D loss: 0.687300, acc: 0.576172]  [A loss: 0.853663, acc: 0.207031]\n",
      "1544: [D loss: 0.696168, acc: 0.521484]  [A loss: 0.858448, acc: 0.203125]\n",
      "1545: [D loss: 0.689153, acc: 0.560547]  [A loss: 0.982434, acc: 0.066406]\n",
      "1546: [D loss: 0.686303, acc: 0.539062]  [A loss: 0.727354, acc: 0.445312]\n",
      "1547: [D loss: 0.698212, acc: 0.500000]  [A loss: 0.994036, acc: 0.078125]\n",
      "1548: [D loss: 0.693720, acc: 0.542969]  [A loss: 0.743904, acc: 0.421875]\n",
      "1549: [D loss: 0.692176, acc: 0.521484]  [A loss: 0.941622, acc: 0.082031]\n",
      "1550: [D loss: 0.665896, acc: 0.611328]  [A loss: 0.818990, acc: 0.277344]\n",
      "1551: [D loss: 0.680708, acc: 0.589844]  [A loss: 0.909378, acc: 0.136719]\n",
      "1552: [D loss: 0.679202, acc: 0.564453]  [A loss: 0.799141, acc: 0.332031]\n",
      "1553: [D loss: 0.695163, acc: 0.541016]  [A loss: 0.937898, acc: 0.125000]\n",
      "1554: [D loss: 0.682509, acc: 0.568359]  [A loss: 0.745467, acc: 0.406250]\n",
      "1555: [D loss: 0.685695, acc: 0.562500]  [A loss: 0.913430, acc: 0.140625]\n",
      "1556: [D loss: 0.700035, acc: 0.511719]  [A loss: 0.785672, acc: 0.347656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557: [D loss: 0.704076, acc: 0.515625]  [A loss: 0.989431, acc: 0.082031]\n",
      "1558: [D loss: 0.671928, acc: 0.609375]  [A loss: 0.753937, acc: 0.410156]\n",
      "1559: [D loss: 0.709161, acc: 0.509766]  [A loss: 0.933502, acc: 0.117188]\n",
      "1560: [D loss: 0.675668, acc: 0.583984]  [A loss: 0.716697, acc: 0.457031]\n",
      "1561: [D loss: 0.709270, acc: 0.507812]  [A loss: 0.987435, acc: 0.085938]\n",
      "1562: [D loss: 0.679402, acc: 0.587891]  [A loss: 0.767321, acc: 0.351562]\n",
      "1563: [D loss: 0.694827, acc: 0.548828]  [A loss: 0.911744, acc: 0.191406]\n",
      "1564: [D loss: 0.685579, acc: 0.550781]  [A loss: 0.821210, acc: 0.281250]\n",
      "1565: [D loss: 0.688139, acc: 0.548828]  [A loss: 0.917943, acc: 0.136719]\n",
      "1566: [D loss: 0.686848, acc: 0.552734]  [A loss: 0.792130, acc: 0.328125]\n",
      "1567: [D loss: 0.686418, acc: 0.578125]  [A loss: 0.921604, acc: 0.160156]\n",
      "1568: [D loss: 0.679306, acc: 0.585938]  [A loss: 0.783869, acc: 0.343750]\n",
      "1569: [D loss: 0.691942, acc: 0.541016]  [A loss: 0.914762, acc: 0.105469]\n",
      "1570: [D loss: 0.678223, acc: 0.554688]  [A loss: 0.787724, acc: 0.363281]\n",
      "1571: [D loss: 0.693281, acc: 0.529297]  [A loss: 0.890803, acc: 0.210938]\n",
      "1572: [D loss: 0.688660, acc: 0.544922]  [A loss: 0.844853, acc: 0.253906]\n",
      "1573: [D loss: 0.690298, acc: 0.533203]  [A loss: 0.851032, acc: 0.234375]\n",
      "1574: [D loss: 0.690921, acc: 0.527344]  [A loss: 0.812931, acc: 0.300781]\n",
      "1575: [D loss: 0.679076, acc: 0.554688]  [A loss: 0.900971, acc: 0.160156]\n",
      "1576: [D loss: 0.678127, acc: 0.554688]  [A loss: 0.824192, acc: 0.261719]\n",
      "1577: [D loss: 0.675767, acc: 0.574219]  [A loss: 0.922419, acc: 0.156250]\n",
      "1578: [D loss: 0.674073, acc: 0.574219]  [A loss: 0.757870, acc: 0.429688]\n",
      "1579: [D loss: 0.700646, acc: 0.537109]  [A loss: 0.933678, acc: 0.113281]\n",
      "1580: [D loss: 0.696220, acc: 0.533203]  [A loss: 0.766345, acc: 0.363281]\n",
      "1581: [D loss: 0.701523, acc: 0.564453]  [A loss: 0.939616, acc: 0.089844]\n",
      "1582: [D loss: 0.676173, acc: 0.578125]  [A loss: 0.761865, acc: 0.339844]\n",
      "1583: [D loss: 0.699088, acc: 0.525391]  [A loss: 0.957611, acc: 0.117188]\n",
      "1584: [D loss: 0.681031, acc: 0.552734]  [A loss: 0.784093, acc: 0.328125]\n",
      "1585: [D loss: 0.683105, acc: 0.552734]  [A loss: 0.947024, acc: 0.128906]\n",
      "1586: [D loss: 0.683207, acc: 0.580078]  [A loss: 0.767641, acc: 0.402344]\n",
      "1587: [D loss: 0.698178, acc: 0.546875]  [A loss: 0.947190, acc: 0.109375]\n",
      "1588: [D loss: 0.679574, acc: 0.556641]  [A loss: 0.742419, acc: 0.394531]\n",
      "1589: [D loss: 0.715356, acc: 0.537109]  [A loss: 0.985879, acc: 0.105469]\n",
      "1590: [D loss: 0.677809, acc: 0.566406]  [A loss: 0.716381, acc: 0.492188]\n",
      "1591: [D loss: 0.709318, acc: 0.515625]  [A loss: 0.985116, acc: 0.093750]\n",
      "1592: [D loss: 0.681285, acc: 0.572266]  [A loss: 0.732167, acc: 0.421875]\n",
      "1593: [D loss: 0.700045, acc: 0.531250]  [A loss: 0.960403, acc: 0.097656]\n",
      "1594: [D loss: 0.707862, acc: 0.509766]  [A loss: 0.788168, acc: 0.304688]\n",
      "1595: [D loss: 0.691627, acc: 0.560547]  [A loss: 0.898423, acc: 0.167969]\n",
      "1596: [D loss: 0.676695, acc: 0.587891]  [A loss: 0.839267, acc: 0.242188]\n",
      "1597: [D loss: 0.684654, acc: 0.539062]  [A loss: 0.825999, acc: 0.269531]\n",
      "1598: [D loss: 0.677736, acc: 0.560547]  [A loss: 0.829454, acc: 0.242188]\n",
      "1599: [D loss: 0.682405, acc: 0.570312]  [A loss: 0.822200, acc: 0.273438]\n",
      "1600: [D loss: 0.687686, acc: 0.566406]  [A loss: 0.861663, acc: 0.226562]\n",
      "1601: [D loss: 0.689113, acc: 0.558594]  [A loss: 0.814113, acc: 0.347656]\n",
      "1602: [D loss: 0.695675, acc: 0.552734]  [A loss: 0.881636, acc: 0.183594]\n",
      "1603: [D loss: 0.693766, acc: 0.541016]  [A loss: 0.795059, acc: 0.363281]\n",
      "1604: [D loss: 0.688158, acc: 0.564453]  [A loss: 0.901797, acc: 0.179688]\n",
      "1605: [D loss: 0.683219, acc: 0.570312]  [A loss: 0.843286, acc: 0.238281]\n",
      "1606: [D loss: 0.692695, acc: 0.556641]  [A loss: 0.884325, acc: 0.171875]\n",
      "1607: [D loss: 0.695003, acc: 0.539062]  [A loss: 0.887363, acc: 0.183594]\n",
      "1608: [D loss: 0.688284, acc: 0.582031]  [A loss: 0.853165, acc: 0.191406]\n",
      "1609: [D loss: 0.684245, acc: 0.558594]  [A loss: 0.863587, acc: 0.218750]\n",
      "1610: [D loss: 0.696453, acc: 0.548828]  [A loss: 0.936268, acc: 0.128906]\n",
      "1611: [D loss: 0.687615, acc: 0.537109]  [A loss: 0.830771, acc: 0.234375]\n",
      "1612: [D loss: 0.684729, acc: 0.542969]  [A loss: 0.937200, acc: 0.101562]\n",
      "1613: [D loss: 0.685000, acc: 0.562500]  [A loss: 0.738921, acc: 0.406250]\n",
      "1614: [D loss: 0.699173, acc: 0.535156]  [A loss: 0.972972, acc: 0.117188]\n",
      "1615: [D loss: 0.693189, acc: 0.533203]  [A loss: 0.781336, acc: 0.347656]\n",
      "1616: [D loss: 0.700565, acc: 0.542969]  [A loss: 0.942747, acc: 0.128906]\n",
      "1617: [D loss: 0.676053, acc: 0.560547]  [A loss: 0.762265, acc: 0.390625]\n",
      "1618: [D loss: 0.698056, acc: 0.531250]  [A loss: 0.904802, acc: 0.132812]\n",
      "1619: [D loss: 0.676979, acc: 0.593750]  [A loss: 0.774747, acc: 0.339844]\n",
      "1620: [D loss: 0.680597, acc: 0.544922]  [A loss: 0.880383, acc: 0.183594]\n",
      "1621: [D loss: 0.695118, acc: 0.533203]  [A loss: 0.846458, acc: 0.238281]\n",
      "1622: [D loss: 0.680465, acc: 0.541016]  [A loss: 0.908207, acc: 0.109375]\n",
      "1623: [D loss: 0.687877, acc: 0.570312]  [A loss: 0.825599, acc: 0.265625]\n",
      "1624: [D loss: 0.687477, acc: 0.568359]  [A loss: 0.872026, acc: 0.183594]\n",
      "1625: [D loss: 0.688269, acc: 0.525391]  [A loss: 0.818675, acc: 0.285156]\n",
      "1626: [D loss: 0.702574, acc: 0.533203]  [A loss: 0.950822, acc: 0.121094]\n",
      "1627: [D loss: 0.680804, acc: 0.562500]  [A loss: 0.746921, acc: 0.417969]\n",
      "1628: [D loss: 0.703229, acc: 0.531250]  [A loss: 1.000256, acc: 0.066406]\n",
      "1629: [D loss: 0.684130, acc: 0.564453]  [A loss: 0.754356, acc: 0.375000]\n",
      "1630: [D loss: 0.714935, acc: 0.500000]  [A loss: 0.965612, acc: 0.093750]\n",
      "1631: [D loss: 0.681566, acc: 0.564453]  [A loss: 0.761356, acc: 0.367188]\n",
      "1632: [D loss: 0.674643, acc: 0.548828]  [A loss: 0.941449, acc: 0.144531]\n",
      "1633: [D loss: 0.679800, acc: 0.562500]  [A loss: 0.783975, acc: 0.328125]\n",
      "1634: [D loss: 0.687042, acc: 0.537109]  [A loss: 0.946902, acc: 0.152344]\n",
      "1635: [D loss: 0.682747, acc: 0.542969]  [A loss: 0.733848, acc: 0.433594]\n",
      "1636: [D loss: 0.693335, acc: 0.550781]  [A loss: 0.919740, acc: 0.148438]\n",
      "1637: [D loss: 0.678448, acc: 0.566406]  [A loss: 0.812248, acc: 0.277344]\n",
      "1638: [D loss: 0.718164, acc: 0.494141]  [A loss: 0.953667, acc: 0.093750]\n",
      "1639: [D loss: 0.675256, acc: 0.582031]  [A loss: 0.845354, acc: 0.250000]\n",
      "1640: [D loss: 0.690411, acc: 0.541016]  [A loss: 0.932774, acc: 0.164062]\n",
      "1641: [D loss: 0.686095, acc: 0.580078]  [A loss: 0.818648, acc: 0.277344]\n",
      "1642: [D loss: 0.696336, acc: 0.539062]  [A loss: 0.916064, acc: 0.152344]\n",
      "1643: [D loss: 0.676838, acc: 0.562500]  [A loss: 0.817662, acc: 0.261719]\n",
      "1644: [D loss: 0.687981, acc: 0.576172]  [A loss: 0.922143, acc: 0.144531]\n",
      "1645: [D loss: 0.699236, acc: 0.519531]  [A loss: 0.814011, acc: 0.257812]\n",
      "1646: [D loss: 0.699334, acc: 0.527344]  [A loss: 0.942393, acc: 0.136719]\n",
      "1647: [D loss: 0.684117, acc: 0.552734]  [A loss: 0.766004, acc: 0.394531]\n",
      "1648: [D loss: 0.697178, acc: 0.531250]  [A loss: 0.978787, acc: 0.089844]\n",
      "1649: [D loss: 0.662723, acc: 0.605469]  [A loss: 0.806505, acc: 0.296875]\n",
      "1650: [D loss: 0.690065, acc: 0.558594]  [A loss: 0.906034, acc: 0.144531]\n",
      "1651: [D loss: 0.681300, acc: 0.568359]  [A loss: 0.875543, acc: 0.214844]\n",
      "1652: [D loss: 0.693225, acc: 0.527344]  [A loss: 0.877537, acc: 0.210938]\n",
      "1653: [D loss: 0.673337, acc: 0.572266]  [A loss: 0.850015, acc: 0.218750]\n",
      "1654: [D loss: 0.687951, acc: 0.537109]  [A loss: 0.848278, acc: 0.250000]\n",
      "1655: [D loss: 0.691564, acc: 0.546875]  [A loss: 0.846939, acc: 0.222656]\n",
      "1656: [D loss: 0.675957, acc: 0.564453]  [A loss: 0.857242, acc: 0.187500]\n",
      "1657: [D loss: 0.682547, acc: 0.574219]  [A loss: 0.870576, acc: 0.203125]\n",
      "1658: [D loss: 0.702124, acc: 0.521484]  [A loss: 0.884606, acc: 0.191406]\n",
      "1659: [D loss: 0.699312, acc: 0.515625]  [A loss: 0.877696, acc: 0.203125]\n",
      "1660: [D loss: 0.695168, acc: 0.537109]  [A loss: 0.854991, acc: 0.226562]\n",
      "1661: [D loss: 0.705033, acc: 0.527344]  [A loss: 0.995258, acc: 0.140625]\n",
      "1662: [D loss: 0.700939, acc: 0.527344]  [A loss: 0.781640, acc: 0.359375]\n",
      "1663: [D loss: 0.689565, acc: 0.544922]  [A loss: 0.918512, acc: 0.156250]\n",
      "1664: [D loss: 0.683022, acc: 0.583984]  [A loss: 0.803829, acc: 0.300781]\n",
      "1665: [D loss: 0.698191, acc: 0.552734]  [A loss: 0.949220, acc: 0.109375]\n",
      "1666: [D loss: 0.685935, acc: 0.523438]  [A loss: 0.752912, acc: 0.394531]\n",
      "1667: [D loss: 0.694811, acc: 0.533203]  [A loss: 0.958729, acc: 0.121094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668: [D loss: 0.687210, acc: 0.535156]  [A loss: 0.746120, acc: 0.425781]\n",
      "1669: [D loss: 0.696228, acc: 0.537109]  [A loss: 1.003473, acc: 0.093750]\n",
      "1670: [D loss: 0.696481, acc: 0.564453]  [A loss: 0.706786, acc: 0.507812]\n",
      "1671: [D loss: 0.714446, acc: 0.529297]  [A loss: 0.976400, acc: 0.062500]\n",
      "1672: [D loss: 0.688865, acc: 0.539062]  [A loss: 0.711779, acc: 0.507812]\n",
      "1673: [D loss: 0.696474, acc: 0.527344]  [A loss: 0.946872, acc: 0.109375]\n",
      "1674: [D loss: 0.689790, acc: 0.560547]  [A loss: 0.757815, acc: 0.410156]\n",
      "1675: [D loss: 0.688891, acc: 0.587891]  [A loss: 0.871601, acc: 0.203125]\n",
      "1676: [D loss: 0.687969, acc: 0.539062]  [A loss: 0.804297, acc: 0.281250]\n",
      "1677: [D loss: 0.689564, acc: 0.544922]  [A loss: 0.818566, acc: 0.257812]\n",
      "1678: [D loss: 0.675665, acc: 0.564453]  [A loss: 0.853838, acc: 0.207031]\n",
      "1679: [D loss: 0.674394, acc: 0.578125]  [A loss: 0.793457, acc: 0.300781]\n",
      "1680: [D loss: 0.682813, acc: 0.554688]  [A loss: 0.885586, acc: 0.195312]\n",
      "1681: [D loss: 0.689964, acc: 0.574219]  [A loss: 0.815988, acc: 0.273438]\n",
      "1682: [D loss: 0.696695, acc: 0.539062]  [A loss: 0.887893, acc: 0.144531]\n",
      "1683: [D loss: 0.681070, acc: 0.564453]  [A loss: 0.808419, acc: 0.312500]\n",
      "1684: [D loss: 0.688276, acc: 0.583984]  [A loss: 0.890907, acc: 0.167969]\n",
      "1685: [D loss: 0.683415, acc: 0.562500]  [A loss: 0.800763, acc: 0.304688]\n",
      "1686: [D loss: 0.682078, acc: 0.560547]  [A loss: 0.922258, acc: 0.128906]\n",
      "1687: [D loss: 0.684062, acc: 0.574219]  [A loss: 0.765435, acc: 0.371094]\n",
      "1688: [D loss: 0.711098, acc: 0.494141]  [A loss: 0.914511, acc: 0.132812]\n",
      "1689: [D loss: 0.679118, acc: 0.556641]  [A loss: 0.778364, acc: 0.355469]\n",
      "1690: [D loss: 0.696778, acc: 0.541016]  [A loss: 0.979209, acc: 0.117188]\n",
      "1691: [D loss: 0.695125, acc: 0.550781]  [A loss: 0.796479, acc: 0.300781]\n",
      "1692: [D loss: 0.689833, acc: 0.544922]  [A loss: 0.938714, acc: 0.105469]\n",
      "1693: [D loss: 0.680428, acc: 0.542969]  [A loss: 0.773469, acc: 0.332031]\n",
      "1694: [D loss: 0.693182, acc: 0.541016]  [A loss: 0.950053, acc: 0.117188]\n",
      "1695: [D loss: 0.684297, acc: 0.583984]  [A loss: 0.813781, acc: 0.296875]\n",
      "1696: [D loss: 0.685929, acc: 0.554688]  [A loss: 0.879636, acc: 0.175781]\n",
      "1697: [D loss: 0.673808, acc: 0.599609]  [A loss: 0.843586, acc: 0.285156]\n",
      "1698: [D loss: 0.685162, acc: 0.568359]  [A loss: 0.908174, acc: 0.171875]\n",
      "1699: [D loss: 0.679200, acc: 0.587891]  [A loss: 0.802900, acc: 0.312500]\n",
      "1700: [D loss: 0.695588, acc: 0.519531]  [A loss: 0.894318, acc: 0.152344]\n",
      "1701: [D loss: 0.677031, acc: 0.583984]  [A loss: 0.767288, acc: 0.363281]\n",
      "1702: [D loss: 0.687018, acc: 0.556641]  [A loss: 0.940607, acc: 0.152344]\n",
      "1703: [D loss: 0.682892, acc: 0.591797]  [A loss: 0.822630, acc: 0.257812]\n",
      "1704: [D loss: 0.670625, acc: 0.599609]  [A loss: 0.953588, acc: 0.140625]\n",
      "1705: [D loss: 0.681610, acc: 0.576172]  [A loss: 0.762339, acc: 0.371094]\n",
      "1706: [D loss: 0.704189, acc: 0.523438]  [A loss: 1.044355, acc: 0.054688]\n",
      "1707: [D loss: 0.673442, acc: 0.574219]  [A loss: 0.671516, acc: 0.554688]\n",
      "1708: [D loss: 0.722744, acc: 0.521484]  [A loss: 1.022101, acc: 0.066406]\n",
      "1709: [D loss: 0.670792, acc: 0.562500]  [A loss: 0.674950, acc: 0.542969]\n",
      "1710: [D loss: 0.724287, acc: 0.517578]  [A loss: 0.983382, acc: 0.121094]\n",
      "1711: [D loss: 0.689230, acc: 0.529297]  [A loss: 0.760109, acc: 0.367188]\n",
      "1712: [D loss: 0.691535, acc: 0.541016]  [A loss: 0.829645, acc: 0.273438]\n",
      "1713: [D loss: 0.689514, acc: 0.541016]  [A loss: 0.819707, acc: 0.273438]\n",
      "1714: [D loss: 0.689826, acc: 0.544922]  [A loss: 0.873745, acc: 0.167969]\n",
      "1715: [D loss: 0.680580, acc: 0.568359]  [A loss: 0.729998, acc: 0.421875]\n",
      "1716: [D loss: 0.701405, acc: 0.527344]  [A loss: 0.903820, acc: 0.171875]\n",
      "1717: [D loss: 0.688780, acc: 0.535156]  [A loss: 0.836747, acc: 0.265625]\n",
      "1718: [D loss: 0.686580, acc: 0.566406]  [A loss: 0.848418, acc: 0.250000]\n",
      "1719: [D loss: 0.678365, acc: 0.570312]  [A loss: 0.874563, acc: 0.199219]\n",
      "1720: [D loss: 0.687953, acc: 0.582031]  [A loss: 0.843872, acc: 0.226562]\n",
      "1721: [D loss: 0.683289, acc: 0.554688]  [A loss: 0.841812, acc: 0.277344]\n",
      "1722: [D loss: 0.698634, acc: 0.527344]  [A loss: 0.883601, acc: 0.132812]\n",
      "1723: [D loss: 0.693523, acc: 0.537109]  [A loss: 0.858591, acc: 0.246094]\n",
      "1724: [D loss: 0.675626, acc: 0.595703]  [A loss: 0.837504, acc: 0.250000]\n",
      "1725: [D loss: 0.690708, acc: 0.568359]  [A loss: 0.867447, acc: 0.218750]\n",
      "1726: [D loss: 0.680869, acc: 0.554688]  [A loss: 0.839114, acc: 0.222656]\n",
      "1727: [D loss: 0.687746, acc: 0.576172]  [A loss: 0.892155, acc: 0.160156]\n",
      "1728: [D loss: 0.699894, acc: 0.513672]  [A loss: 0.909967, acc: 0.148438]\n",
      "1729: [D loss: 0.677095, acc: 0.533203]  [A loss: 0.855143, acc: 0.226562]\n",
      "1730: [D loss: 0.686442, acc: 0.570312]  [A loss: 0.890430, acc: 0.183594]\n",
      "1731: [D loss: 0.685374, acc: 0.568359]  [A loss: 0.850726, acc: 0.234375]\n",
      "1732: [D loss: 0.678845, acc: 0.568359]  [A loss: 0.916573, acc: 0.152344]\n",
      "1733: [D loss: 0.703117, acc: 0.531250]  [A loss: 0.893686, acc: 0.207031]\n",
      "1734: [D loss: 0.696675, acc: 0.564453]  [A loss: 0.814742, acc: 0.285156]\n",
      "1735: [D loss: 0.681244, acc: 0.562500]  [A loss: 0.934194, acc: 0.140625]\n",
      "1736: [D loss: 0.671972, acc: 0.583984]  [A loss: 0.896072, acc: 0.179688]\n",
      "1737: [D loss: 0.691791, acc: 0.560547]  [A loss: 0.889948, acc: 0.179688]\n",
      "1738: [D loss: 0.695517, acc: 0.533203]  [A loss: 0.869636, acc: 0.195312]\n",
      "1739: [D loss: 0.688048, acc: 0.525391]  [A loss: 0.859063, acc: 0.234375]\n",
      "1740: [D loss: 0.689226, acc: 0.535156]  [A loss: 0.977440, acc: 0.125000]\n",
      "1741: [D loss: 0.694353, acc: 0.537109]  [A loss: 0.717912, acc: 0.433594]\n",
      "1742: [D loss: 0.689902, acc: 0.533203]  [A loss: 1.089417, acc: 0.093750]\n",
      "1743: [D loss: 0.695514, acc: 0.544922]  [A loss: 0.720244, acc: 0.476562]\n",
      "1744: [D loss: 0.719440, acc: 0.529297]  [A loss: 0.959365, acc: 0.082031]\n",
      "1745: [D loss: 0.690318, acc: 0.544922]  [A loss: 0.698921, acc: 0.554688]\n",
      "1746: [D loss: 0.730489, acc: 0.517578]  [A loss: 1.044188, acc: 0.042969]\n",
      "1747: [D loss: 0.680384, acc: 0.560547]  [A loss: 0.701111, acc: 0.496094]\n",
      "1748: [D loss: 0.710444, acc: 0.525391]  [A loss: 0.950259, acc: 0.121094]\n",
      "1749: [D loss: 0.692331, acc: 0.533203]  [A loss: 0.777344, acc: 0.320312]\n",
      "1750: [D loss: 0.708571, acc: 0.509766]  [A loss: 0.901871, acc: 0.160156]\n",
      "1751: [D loss: 0.675495, acc: 0.564453]  [A loss: 0.789527, acc: 0.281250]\n",
      "1752: [D loss: 0.690326, acc: 0.560547]  [A loss: 0.875265, acc: 0.179688]\n",
      "1753: [D loss: 0.689995, acc: 0.562500]  [A loss: 0.841243, acc: 0.234375]\n",
      "1754: [D loss: 0.680960, acc: 0.591797]  [A loss: 0.887818, acc: 0.167969]\n",
      "1755: [D loss: 0.675639, acc: 0.572266]  [A loss: 0.843463, acc: 0.226562]\n",
      "1756: [D loss: 0.689549, acc: 0.542969]  [A loss: 0.872485, acc: 0.214844]\n",
      "1757: [D loss: 0.690844, acc: 0.568359]  [A loss: 0.850330, acc: 0.222656]\n",
      "1758: [D loss: 0.699620, acc: 0.498047]  [A loss: 0.850460, acc: 0.214844]\n",
      "1759: [D loss: 0.713820, acc: 0.498047]  [A loss: 0.969686, acc: 0.085938]\n",
      "1760: [D loss: 0.677974, acc: 0.560547]  [A loss: 0.856335, acc: 0.238281]\n",
      "1761: [D loss: 0.706693, acc: 0.537109]  [A loss: 0.887236, acc: 0.199219]\n",
      "1762: [D loss: 0.686001, acc: 0.544922]  [A loss: 0.816476, acc: 0.277344]\n",
      "1763: [D loss: 0.701197, acc: 0.523438]  [A loss: 0.879165, acc: 0.167969]\n",
      "1764: [D loss: 0.691908, acc: 0.546875]  [A loss: 0.931473, acc: 0.148438]\n",
      "1765: [D loss: 0.693434, acc: 0.554688]  [A loss: 0.856217, acc: 0.207031]\n",
      "1766: [D loss: 0.694338, acc: 0.556641]  [A loss: 0.870496, acc: 0.183594]\n",
      "1767: [D loss: 0.685233, acc: 0.562500]  [A loss: 0.943943, acc: 0.140625]\n",
      "1768: [D loss: 0.688169, acc: 0.550781]  [A loss: 0.801778, acc: 0.328125]\n",
      "1769: [D loss: 0.690569, acc: 0.568359]  [A loss: 1.006575, acc: 0.074219]\n",
      "1770: [D loss: 0.683712, acc: 0.568359]  [A loss: 0.745708, acc: 0.398438]\n",
      "1771: [D loss: 0.713262, acc: 0.500000]  [A loss: 0.930517, acc: 0.121094]\n",
      "1772: [D loss: 0.701486, acc: 0.513672]  [A loss: 0.836511, acc: 0.238281]\n",
      "1773: [D loss: 0.689854, acc: 0.525391]  [A loss: 0.952253, acc: 0.125000]\n",
      "1774: [D loss: 0.680216, acc: 0.566406]  [A loss: 0.770096, acc: 0.343750]\n",
      "1775: [D loss: 0.695797, acc: 0.550781]  [A loss: 0.935521, acc: 0.105469]\n",
      "1776: [D loss: 0.690989, acc: 0.544922]  [A loss: 0.787750, acc: 0.316406]\n",
      "1777: [D loss: 0.691636, acc: 0.560547]  [A loss: 0.893942, acc: 0.171875]\n",
      "1778: [D loss: 0.706362, acc: 0.515625]  [A loss: 0.913624, acc: 0.167969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1779: [D loss: 0.696246, acc: 0.539062]  [A loss: 0.820793, acc: 0.281250]\n",
      "1780: [D loss: 0.703716, acc: 0.531250]  [A loss: 0.846251, acc: 0.234375]\n",
      "1781: [D loss: 0.688310, acc: 0.568359]  [A loss: 0.957434, acc: 0.121094]\n",
      "1782: [D loss: 0.689357, acc: 0.519531]  [A loss: 0.778299, acc: 0.324219]\n",
      "1783: [D loss: 0.694052, acc: 0.560547]  [A loss: 0.922227, acc: 0.167969]\n",
      "1784: [D loss: 0.693688, acc: 0.527344]  [A loss: 0.810543, acc: 0.300781]\n",
      "1785: [D loss: 0.688800, acc: 0.541016]  [A loss: 0.900660, acc: 0.195312]\n",
      "1786: [D loss: 0.677842, acc: 0.578125]  [A loss: 0.760650, acc: 0.406250]\n",
      "1787: [D loss: 0.696452, acc: 0.529297]  [A loss: 0.960737, acc: 0.117188]\n",
      "1788: [D loss: 0.705940, acc: 0.519531]  [A loss: 0.774141, acc: 0.363281]\n",
      "1789: [D loss: 0.695086, acc: 0.542969]  [A loss: 0.954032, acc: 0.105469]\n",
      "1790: [D loss: 0.683456, acc: 0.564453]  [A loss: 0.746494, acc: 0.414062]\n",
      "1791: [D loss: 0.700196, acc: 0.552734]  [A loss: 1.012647, acc: 0.066406]\n",
      "1792: [D loss: 0.677636, acc: 0.583984]  [A loss: 0.743969, acc: 0.421875]\n",
      "1793: [D loss: 0.704090, acc: 0.525391]  [A loss: 0.931107, acc: 0.113281]\n",
      "1794: [D loss: 0.676041, acc: 0.568359]  [A loss: 0.681888, acc: 0.546875]\n",
      "1795: [D loss: 0.724397, acc: 0.500000]  [A loss: 1.010637, acc: 0.062500]\n",
      "1796: [D loss: 0.688811, acc: 0.546875]  [A loss: 0.737973, acc: 0.398438]\n",
      "1797: [D loss: 0.712982, acc: 0.513672]  [A loss: 0.918867, acc: 0.140625]\n",
      "1798: [D loss: 0.675301, acc: 0.580078]  [A loss: 0.784474, acc: 0.316406]\n",
      "1799: [D loss: 0.694224, acc: 0.556641]  [A loss: 0.917080, acc: 0.117188]\n",
      "1800: [D loss: 0.675618, acc: 0.570312]  [A loss: 0.770961, acc: 0.339844]\n",
      "1801: [D loss: 0.685123, acc: 0.548828]  [A loss: 0.869051, acc: 0.230469]\n",
      "1802: [D loss: 0.682892, acc: 0.550781]  [A loss: 0.862297, acc: 0.171875]\n",
      "1803: [D loss: 0.675693, acc: 0.580078]  [A loss: 0.809322, acc: 0.304688]\n",
      "1804: [D loss: 0.682492, acc: 0.566406]  [A loss: 0.875013, acc: 0.191406]\n",
      "1805: [D loss: 0.697786, acc: 0.513672]  [A loss: 0.829919, acc: 0.250000]\n",
      "1806: [D loss: 0.691657, acc: 0.550781]  [A loss: 0.883579, acc: 0.140625]\n",
      "1807: [D loss: 0.682830, acc: 0.562500]  [A loss: 0.852518, acc: 0.191406]\n",
      "1808: [D loss: 0.683886, acc: 0.558594]  [A loss: 0.912994, acc: 0.164062]\n",
      "1809: [D loss: 0.684493, acc: 0.539062]  [A loss: 0.818177, acc: 0.273438]\n",
      "1810: [D loss: 0.680320, acc: 0.537109]  [A loss: 0.846107, acc: 0.222656]\n",
      "1811: [D loss: 0.687083, acc: 0.523438]  [A loss: 0.907700, acc: 0.144531]\n",
      "1812: [D loss: 0.672110, acc: 0.578125]  [A loss: 0.840202, acc: 0.265625]\n",
      "1813: [D loss: 0.685369, acc: 0.527344]  [A loss: 0.855046, acc: 0.226562]\n",
      "1814: [D loss: 0.681972, acc: 0.541016]  [A loss: 0.847015, acc: 0.230469]\n",
      "1815: [D loss: 0.693878, acc: 0.529297]  [A loss: 0.903267, acc: 0.136719]\n",
      "1816: [D loss: 0.690745, acc: 0.558594]  [A loss: 0.798534, acc: 0.316406]\n",
      "1817: [D loss: 0.698034, acc: 0.542969]  [A loss: 1.060627, acc: 0.050781]\n",
      "1818: [D loss: 0.680729, acc: 0.589844]  [A loss: 0.717667, acc: 0.457031]\n",
      "1819: [D loss: 0.701755, acc: 0.529297]  [A loss: 1.029089, acc: 0.093750]\n",
      "1820: [D loss: 0.679091, acc: 0.548828]  [A loss: 0.707633, acc: 0.480469]\n",
      "1821: [D loss: 0.719499, acc: 0.537109]  [A loss: 0.913022, acc: 0.152344]\n",
      "1822: [D loss: 0.701562, acc: 0.544922]  [A loss: 0.796279, acc: 0.312500]\n",
      "1823: [D loss: 0.686411, acc: 0.566406]  [A loss: 0.901475, acc: 0.195312]\n",
      "1824: [D loss: 0.685100, acc: 0.548828]  [A loss: 0.811218, acc: 0.304688]\n",
      "1825: [D loss: 0.674830, acc: 0.591797]  [A loss: 0.822933, acc: 0.273438]\n",
      "1826: [D loss: 0.696722, acc: 0.531250]  [A loss: 0.911038, acc: 0.187500]\n",
      "1827: [D loss: 0.680892, acc: 0.552734]  [A loss: 0.828367, acc: 0.246094]\n",
      "1828: [D loss: 0.708665, acc: 0.500000]  [A loss: 0.919188, acc: 0.156250]\n",
      "1829: [D loss: 0.685906, acc: 0.568359]  [A loss: 0.772948, acc: 0.386719]\n",
      "1830: [D loss: 0.684584, acc: 0.550781]  [A loss: 0.964373, acc: 0.101562]\n",
      "1831: [D loss: 0.704548, acc: 0.521484]  [A loss: 0.746576, acc: 0.398438]\n",
      "1832: [D loss: 0.720824, acc: 0.552734]  [A loss: 0.970178, acc: 0.109375]\n",
      "1833: [D loss: 0.669153, acc: 0.576172]  [A loss: 0.768030, acc: 0.394531]\n",
      "1834: [D loss: 0.695949, acc: 0.558594]  [A loss: 0.888614, acc: 0.160156]\n",
      "1835: [D loss: 0.681930, acc: 0.564453]  [A loss: 0.776147, acc: 0.355469]\n",
      "1836: [D loss: 0.711194, acc: 0.525391]  [A loss: 0.978134, acc: 0.074219]\n",
      "1837: [D loss: 0.693541, acc: 0.519531]  [A loss: 0.728403, acc: 0.406250]\n",
      "1838: [D loss: 0.683109, acc: 0.570312]  [A loss: 0.901577, acc: 0.187500]\n",
      "1839: [D loss: 0.698752, acc: 0.525391]  [A loss: 0.869164, acc: 0.179688]\n",
      "1840: [D loss: 0.681355, acc: 0.548828]  [A loss: 0.888074, acc: 0.179688]\n",
      "1841: [D loss: 0.696684, acc: 0.546875]  [A loss: 0.770562, acc: 0.332031]\n",
      "1842: [D loss: 0.704187, acc: 0.525391]  [A loss: 0.949696, acc: 0.132812]\n",
      "1843: [D loss: 0.685180, acc: 0.578125]  [A loss: 0.791316, acc: 0.343750]\n",
      "1844: [D loss: 0.677441, acc: 0.576172]  [A loss: 0.927364, acc: 0.121094]\n",
      "1845: [D loss: 0.671353, acc: 0.603516]  [A loss: 0.770934, acc: 0.371094]\n",
      "1846: [D loss: 0.691467, acc: 0.566406]  [A loss: 1.006931, acc: 0.066406]\n",
      "1847: [D loss: 0.684247, acc: 0.548828]  [A loss: 0.745962, acc: 0.410156]\n",
      "1848: [D loss: 0.693241, acc: 0.558594]  [A loss: 1.048149, acc: 0.070312]\n",
      "1849: [D loss: 0.674836, acc: 0.583984]  [A loss: 0.680483, acc: 0.554688]\n",
      "1850: [D loss: 0.700682, acc: 0.537109]  [A loss: 0.890002, acc: 0.175781]\n",
      "1851: [D loss: 0.682711, acc: 0.582031]  [A loss: 0.791945, acc: 0.367188]\n",
      "1852: [D loss: 0.691830, acc: 0.550781]  [A loss: 0.913510, acc: 0.207031]\n",
      "1853: [D loss: 0.687804, acc: 0.535156]  [A loss: 0.798118, acc: 0.273438]\n",
      "1854: [D loss: 0.697818, acc: 0.535156]  [A loss: 0.905363, acc: 0.187500]\n",
      "1855: [D loss: 0.681116, acc: 0.578125]  [A loss: 0.811339, acc: 0.335938]\n",
      "1856: [D loss: 0.707896, acc: 0.525391]  [A loss: 0.883493, acc: 0.199219]\n",
      "1857: [D loss: 0.680462, acc: 0.566406]  [A loss: 0.816405, acc: 0.320312]\n",
      "1858: [D loss: 0.690967, acc: 0.533203]  [A loss: 0.928532, acc: 0.136719]\n",
      "1859: [D loss: 0.690017, acc: 0.542969]  [A loss: 0.822450, acc: 0.269531]\n",
      "1860: [D loss: 0.696277, acc: 0.519531]  [A loss: 0.875416, acc: 0.222656]\n",
      "1861: [D loss: 0.680732, acc: 0.576172]  [A loss: 0.857795, acc: 0.214844]\n",
      "1862: [D loss: 0.686621, acc: 0.558594]  [A loss: 0.845645, acc: 0.250000]\n",
      "1863: [D loss: 0.680848, acc: 0.582031]  [A loss: 0.937364, acc: 0.121094]\n",
      "1864: [D loss: 0.672074, acc: 0.580078]  [A loss: 0.781108, acc: 0.339844]\n",
      "1865: [D loss: 0.696753, acc: 0.521484]  [A loss: 0.971962, acc: 0.121094]\n",
      "1866: [D loss: 0.684858, acc: 0.566406]  [A loss: 0.771777, acc: 0.378906]\n",
      "1867: [D loss: 0.703966, acc: 0.535156]  [A loss: 0.886193, acc: 0.210938]\n",
      "1868: [D loss: 0.683657, acc: 0.568359]  [A loss: 0.812280, acc: 0.324219]\n",
      "1869: [D loss: 0.697410, acc: 0.548828]  [A loss: 1.041149, acc: 0.054688]\n",
      "1870: [D loss: 0.697736, acc: 0.537109]  [A loss: 0.777365, acc: 0.343750]\n",
      "1871: [D loss: 0.693184, acc: 0.552734]  [A loss: 1.032537, acc: 0.066406]\n",
      "1872: [D loss: 0.704990, acc: 0.503906]  [A loss: 0.703889, acc: 0.480469]\n",
      "1873: [D loss: 0.711770, acc: 0.533203]  [A loss: 0.983482, acc: 0.113281]\n",
      "1874: [D loss: 0.683927, acc: 0.544922]  [A loss: 0.730797, acc: 0.464844]\n",
      "1875: [D loss: 0.688416, acc: 0.570312]  [A loss: 0.878452, acc: 0.195312]\n",
      "1876: [D loss: 0.684567, acc: 0.544922]  [A loss: 0.805434, acc: 0.285156]\n",
      "1877: [D loss: 0.695092, acc: 0.552734]  [A loss: 0.927364, acc: 0.109375]\n",
      "1878: [D loss: 0.692417, acc: 0.546875]  [A loss: 0.806262, acc: 0.273438]\n",
      "1879: [D loss: 0.682168, acc: 0.554688]  [A loss: 0.901317, acc: 0.152344]\n",
      "1880: [D loss: 0.672859, acc: 0.593750]  [A loss: 0.843668, acc: 0.261719]\n",
      "1881: [D loss: 0.685667, acc: 0.568359]  [A loss: 0.926844, acc: 0.160156]\n",
      "1882: [D loss: 0.697944, acc: 0.572266]  [A loss: 0.813873, acc: 0.261719]\n",
      "1883: [D loss: 0.691049, acc: 0.542969]  [A loss: 0.828434, acc: 0.261719]\n",
      "1884: [D loss: 0.681385, acc: 0.572266]  [A loss: 0.864734, acc: 0.187500]\n",
      "1885: [D loss: 0.679236, acc: 0.554688]  [A loss: 0.847503, acc: 0.210938]\n",
      "1886: [D loss: 0.693125, acc: 0.531250]  [A loss: 0.885890, acc: 0.222656]\n",
      "1887: [D loss: 0.692846, acc: 0.537109]  [A loss: 0.843876, acc: 0.226562]\n",
      "1888: [D loss: 0.689429, acc: 0.541016]  [A loss: 0.884095, acc: 0.203125]\n",
      "1889: [D loss: 0.672604, acc: 0.580078]  [A loss: 0.709258, acc: 0.488281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1890: [D loss: 0.704540, acc: 0.535156]  [A loss: 1.040315, acc: 0.042969]\n",
      "1891: [D loss: 0.685454, acc: 0.558594]  [A loss: 0.645125, acc: 0.644531]\n",
      "1892: [D loss: 0.730357, acc: 0.517578]  [A loss: 1.014222, acc: 0.093750]\n",
      "1893: [D loss: 0.704869, acc: 0.521484]  [A loss: 0.751087, acc: 0.433594]\n",
      "1894: [D loss: 0.697337, acc: 0.511719]  [A loss: 0.947355, acc: 0.105469]\n",
      "1895: [D loss: 0.685187, acc: 0.550781]  [A loss: 0.839423, acc: 0.234375]\n",
      "1896: [D loss: 0.687570, acc: 0.556641]  [A loss: 0.814458, acc: 0.300781]\n",
      "1897: [D loss: 0.686949, acc: 0.542969]  [A loss: 0.889548, acc: 0.222656]\n",
      "1898: [D loss: 0.681531, acc: 0.554688]  [A loss: 0.827852, acc: 0.316406]\n",
      "1899: [D loss: 0.689716, acc: 0.560547]  [A loss: 0.905606, acc: 0.164062]\n",
      "1900: [D loss: 0.673032, acc: 0.574219]  [A loss: 0.824635, acc: 0.253906]\n",
      "1901: [D loss: 0.697534, acc: 0.546875]  [A loss: 0.896554, acc: 0.152344]\n",
      "1902: [D loss: 0.683116, acc: 0.556641]  [A loss: 0.842644, acc: 0.273438]\n",
      "1903: [D loss: 0.690019, acc: 0.564453]  [A loss: 0.992384, acc: 0.101562]\n",
      "1904: [D loss: 0.694412, acc: 0.521484]  [A loss: 0.778746, acc: 0.355469]\n",
      "1905: [D loss: 0.703373, acc: 0.529297]  [A loss: 0.959172, acc: 0.152344]\n",
      "1906: [D loss: 0.675615, acc: 0.576172]  [A loss: 0.740432, acc: 0.402344]\n",
      "1907: [D loss: 0.704460, acc: 0.539062]  [A loss: 1.011322, acc: 0.070312]\n",
      "1908: [D loss: 0.672742, acc: 0.572266]  [A loss: 0.739245, acc: 0.433594]\n",
      "1909: [D loss: 0.700498, acc: 0.529297]  [A loss: 0.929670, acc: 0.148438]\n",
      "1910: [D loss: 0.693149, acc: 0.521484]  [A loss: 0.840163, acc: 0.250000]\n",
      "1911: [D loss: 0.689677, acc: 0.552734]  [A loss: 0.947436, acc: 0.113281]\n",
      "1912: [D loss: 0.681298, acc: 0.576172]  [A loss: 0.803827, acc: 0.296875]\n",
      "1913: [D loss: 0.691298, acc: 0.546875]  [A loss: 0.965848, acc: 0.105469]\n",
      "1914: [D loss: 0.672501, acc: 0.597656]  [A loss: 0.782142, acc: 0.332031]\n",
      "1915: [D loss: 0.703794, acc: 0.529297]  [A loss: 0.993141, acc: 0.074219]\n",
      "1916: [D loss: 0.680228, acc: 0.564453]  [A loss: 0.770730, acc: 0.375000]\n",
      "1917: [D loss: 0.707432, acc: 0.531250]  [A loss: 1.018092, acc: 0.085938]\n",
      "1918: [D loss: 0.677361, acc: 0.574219]  [A loss: 0.764255, acc: 0.378906]\n",
      "1919: [D loss: 0.696022, acc: 0.521484]  [A loss: 0.906326, acc: 0.160156]\n",
      "1920: [D loss: 0.675630, acc: 0.568359]  [A loss: 0.837460, acc: 0.265625]\n",
      "1921: [D loss: 0.692692, acc: 0.552734]  [A loss: 0.849868, acc: 0.257812]\n",
      "1922: [D loss: 0.697703, acc: 0.513672]  [A loss: 0.870187, acc: 0.218750]\n",
      "1923: [D loss: 0.671094, acc: 0.580078]  [A loss: 0.852352, acc: 0.226562]\n",
      "1924: [D loss: 0.690208, acc: 0.541016]  [A loss: 0.860334, acc: 0.187500]\n",
      "1925: [D loss: 0.695544, acc: 0.544922]  [A loss: 0.916345, acc: 0.152344]\n",
      "1926: [D loss: 0.676755, acc: 0.576172]  [A loss: 0.907819, acc: 0.183594]\n",
      "1927: [D loss: 0.703629, acc: 0.521484]  [A loss: 0.896809, acc: 0.199219]\n",
      "1928: [D loss: 0.696772, acc: 0.533203]  [A loss: 0.985086, acc: 0.117188]\n",
      "1929: [D loss: 0.697759, acc: 0.515625]  [A loss: 0.790141, acc: 0.351562]\n",
      "1930: [D loss: 0.705389, acc: 0.533203]  [A loss: 1.063420, acc: 0.066406]\n",
      "1931: [D loss: 0.690078, acc: 0.531250]  [A loss: 0.709804, acc: 0.503906]\n",
      "1932: [D loss: 0.693569, acc: 0.541016]  [A loss: 1.015837, acc: 0.109375]\n",
      "1933: [D loss: 0.695533, acc: 0.544922]  [A loss: 0.698726, acc: 0.531250]\n",
      "1934: [D loss: 0.722064, acc: 0.517578]  [A loss: 1.017768, acc: 0.082031]\n",
      "1935: [D loss: 0.682658, acc: 0.562500]  [A loss: 0.731165, acc: 0.437500]\n",
      "1936: [D loss: 0.710965, acc: 0.525391]  [A loss: 0.916192, acc: 0.140625]\n",
      "1937: [D loss: 0.678110, acc: 0.589844]  [A loss: 0.782655, acc: 0.351562]\n",
      "1938: [D loss: 0.681193, acc: 0.560547]  [A loss: 0.925095, acc: 0.175781]\n",
      "1939: [D loss: 0.691405, acc: 0.556641]  [A loss: 0.825713, acc: 0.269531]\n",
      "1940: [D loss: 0.689804, acc: 0.564453]  [A loss: 0.841556, acc: 0.246094]\n",
      "1941: [D loss: 0.698052, acc: 0.558594]  [A loss: 0.950052, acc: 0.156250]\n",
      "1942: [D loss: 0.694071, acc: 0.525391]  [A loss: 0.794075, acc: 0.328125]\n",
      "1943: [D loss: 0.689875, acc: 0.562500]  [A loss: 0.903964, acc: 0.171875]\n",
      "1944: [D loss: 0.669328, acc: 0.613281]  [A loss: 0.802203, acc: 0.335938]\n",
      "1945: [D loss: 0.704830, acc: 0.517578]  [A loss: 0.932547, acc: 0.140625]\n",
      "1946: [D loss: 0.677351, acc: 0.568359]  [A loss: 0.765282, acc: 0.359375]\n",
      "1947: [D loss: 0.693678, acc: 0.541016]  [A loss: 0.954428, acc: 0.128906]\n",
      "1948: [D loss: 0.690509, acc: 0.531250]  [A loss: 0.826906, acc: 0.269531]\n",
      "1949: [D loss: 0.697645, acc: 0.533203]  [A loss: 0.868734, acc: 0.210938]\n",
      "1950: [D loss: 0.681717, acc: 0.564453]  [A loss: 0.807966, acc: 0.312500]\n",
      "1951: [D loss: 0.705546, acc: 0.533203]  [A loss: 0.942761, acc: 0.125000]\n",
      "1952: [D loss: 0.692634, acc: 0.554688]  [A loss: 0.756159, acc: 0.406250]\n",
      "1953: [D loss: 0.693455, acc: 0.544922]  [A loss: 0.915634, acc: 0.164062]\n",
      "1954: [D loss: 0.684821, acc: 0.566406]  [A loss: 0.821009, acc: 0.285156]\n",
      "1955: [D loss: 0.687089, acc: 0.548828]  [A loss: 0.964405, acc: 0.105469]\n",
      "1956: [D loss: 0.687482, acc: 0.539062]  [A loss: 0.737665, acc: 0.414062]\n",
      "1957: [D loss: 0.707810, acc: 0.527344]  [A loss: 0.929497, acc: 0.113281]\n",
      "1958: [D loss: 0.680541, acc: 0.537109]  [A loss: 0.770635, acc: 0.343750]\n",
      "1959: [D loss: 0.690292, acc: 0.550781]  [A loss: 0.993806, acc: 0.121094]\n",
      "1960: [D loss: 0.687386, acc: 0.560547]  [A loss: 0.754834, acc: 0.398438]\n",
      "1961: [D loss: 0.708857, acc: 0.529297]  [A loss: 0.953159, acc: 0.171875]\n",
      "1962: [D loss: 0.691370, acc: 0.529297]  [A loss: 0.734465, acc: 0.414062]\n",
      "1963: [D loss: 0.694323, acc: 0.541016]  [A loss: 0.956096, acc: 0.132812]\n",
      "1964: [D loss: 0.666517, acc: 0.583984]  [A loss: 0.732446, acc: 0.464844]\n",
      "1965: [D loss: 0.696755, acc: 0.544922]  [A loss: 0.948346, acc: 0.125000]\n",
      "1966: [D loss: 0.690045, acc: 0.548828]  [A loss: 0.763991, acc: 0.343750]\n",
      "1967: [D loss: 0.689768, acc: 0.578125]  [A loss: 0.980293, acc: 0.109375]\n",
      "1968: [D loss: 0.689921, acc: 0.556641]  [A loss: 0.783493, acc: 0.355469]\n",
      "1969: [D loss: 0.695256, acc: 0.556641]  [A loss: 0.935045, acc: 0.136719]\n",
      "1970: [D loss: 0.674813, acc: 0.583984]  [A loss: 0.816823, acc: 0.339844]\n",
      "1971: [D loss: 0.692514, acc: 0.519531]  [A loss: 0.882250, acc: 0.210938]\n",
      "1972: [D loss: 0.703992, acc: 0.537109]  [A loss: 0.838353, acc: 0.296875]\n",
      "1973: [D loss: 0.705839, acc: 0.533203]  [A loss: 1.016049, acc: 0.070312]\n",
      "1974: [D loss: 0.693851, acc: 0.552734]  [A loss: 0.727822, acc: 0.460938]\n",
      "1975: [D loss: 0.704795, acc: 0.535156]  [A loss: 1.024298, acc: 0.074219]\n",
      "1976: [D loss: 0.686768, acc: 0.568359]  [A loss: 0.708560, acc: 0.496094]\n",
      "1977: [D loss: 0.717070, acc: 0.521484]  [A loss: 0.943263, acc: 0.125000]\n",
      "1978: [D loss: 0.701911, acc: 0.505859]  [A loss: 0.751886, acc: 0.390625]\n",
      "1979: [D loss: 0.694634, acc: 0.556641]  [A loss: 0.885975, acc: 0.183594]\n",
      "1980: [D loss: 0.691081, acc: 0.548828]  [A loss: 0.777895, acc: 0.359375]\n",
      "1981: [D loss: 0.694286, acc: 0.564453]  [A loss: 0.870803, acc: 0.222656]\n",
      "1982: [D loss: 0.689129, acc: 0.537109]  [A loss: 0.759930, acc: 0.359375]\n",
      "1983: [D loss: 0.695768, acc: 0.558594]  [A loss: 0.921816, acc: 0.144531]\n",
      "1984: [D loss: 0.689564, acc: 0.544922]  [A loss: 0.810941, acc: 0.296875]\n",
      "1985: [D loss: 0.699889, acc: 0.537109]  [A loss: 0.898554, acc: 0.164062]\n",
      "1986: [D loss: 0.687802, acc: 0.562500]  [A loss: 0.865501, acc: 0.164062]\n",
      "1987: [D loss: 0.683028, acc: 0.566406]  [A loss: 0.873202, acc: 0.191406]\n",
      "1988: [D loss: 0.680109, acc: 0.591797]  [A loss: 0.827728, acc: 0.292969]\n",
      "1989: [D loss: 0.694319, acc: 0.539062]  [A loss: 0.892780, acc: 0.203125]\n",
      "1990: [D loss: 0.691764, acc: 0.539062]  [A loss: 0.828667, acc: 0.265625]\n",
      "1991: [D loss: 0.678653, acc: 0.562500]  [A loss: 0.942121, acc: 0.125000]\n",
      "1992: [D loss: 0.686853, acc: 0.576172]  [A loss: 0.760279, acc: 0.417969]\n",
      "1993: [D loss: 0.727332, acc: 0.492188]  [A loss: 1.048250, acc: 0.066406]\n",
      "1994: [D loss: 0.685501, acc: 0.554688]  [A loss: 0.711744, acc: 0.515625]\n",
      "1995: [D loss: 0.716622, acc: 0.505859]  [A loss: 0.982006, acc: 0.148438]\n",
      "1996: [D loss: 0.673148, acc: 0.570312]  [A loss: 0.732058, acc: 0.476562]\n",
      "1997: [D loss: 0.671373, acc: 0.603516]  [A loss: 0.930854, acc: 0.179688]\n",
      "1998: [D loss: 0.694795, acc: 0.539062]  [A loss: 0.816485, acc: 0.285156]\n",
      "1999: [D loss: 0.695518, acc: 0.544922]  [A loss: 0.896064, acc: 0.175781]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train for 5000 epochs and save at 500 intervals\n",
    "# print G images\n",
    "# save images\n",
    "mnist_dcgan.train(train_steps=2000, batch_size=256, save_interval=500)\n",
    "mnist_dcgan.plot_images(fake=True)\n",
    "mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
